<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="打怪升级日常">
<meta property="og:url" content="http://yoursite.com/page/71/index.html">
<meta property="og:site_name" content="打怪升级日常">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="打怪升级日常">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/71/">





  <title>打怪升级日常</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    <a href="https://github.com/T0UGH" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">打怪升级日常</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">两星炸弹人(╯‵□′)╯炸弹！•••</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/22/Tensorflow-9-自然语言处理/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/33653960?s=400&u=eb438b904ecb9d91f3aa9b777155a6488599493b&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/22/Tensorflow-9-自然语言处理/" itemprop="url">[Tensorflow][9]自然语言处理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-22T21:57:03+08:00">
                2019-05-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/22/Tensorflow-9-自然语言处理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/05/22/Tensorflow-9-自然语言处理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第9章-自然语言处理"><a href="#第9章-自然语言处理" class="headerlink" title="第9章 自然语言处理"></a>第9章 自然语言处理</h2><ul>
<li>发展历程<ol>
<li>在自然语言处理研究的早期，人们试图整理出关于自然语言的语法，并根据这些语法去理解和生成句子</li>
<li>从20世纪80年代起，随着硬件计算能力的增强和大型语料库的出现，使用统计方法对语言进行概率建模的方式开始变成主流</li>
<li>从2010年起，基于循环神经网络的方法在许多自然语言处理的问题上超越了传统的统计模型</li>
</ol>
</li>
</ul>
<hr>
<h3 id="9-1-语言模型的背景知识"><a href="#9-1-语言模型的背景知识" class="headerlink" title="9.1 语言模型的背景知识"></a>9.1 语言模型的背景知识</h3><h4 id="9-1-1-语言模型简介"><a href="#9-1-1-语言模型简介" class="headerlink" title="9.1.1 语言模型简介"></a>9.1.1 语言模型简介</h4><ul>
<li><p>语言模型简介</p>
<ul>
<li>假设一门语言中所有可能的句子服从某一个概率分布，每个句子出现的概率加起来为1，那么语言模型的任务就是预测每个句子在语言中出现的概率</li>
<li>把句子看成单词的序列，语言模型可以表示为一个计算<code>p(w1,w2,w3,...,wm)</code>的模型</li>
<li>语言模型仅仅对句子出现的概率进行建模，并不尝试“理解”句子的内容含义</li>
</ul>
</li>
<li><p>语言模型应用</p>
<ul>
<li>很多生成自然语言文本的应用都依赖语言模型来优化输出文本的流畅性<ul>
<li>生成的句子在语言模型中的概率越高，说明其越有可能是一个流畅、自然的句子</li>
</ul>
</li>
<li>神经网络机器翻译的Seq2Seq模型可以看作是一个条件语言模型(Conditional Language Model)，它相当于是在给定输入的情况下对目标语言的所有句子估算概率，并选择其中概率最大的句子作为输出</li>
</ul>
</li>
<li><p>如何计算一个句子的概率</p>
<ul>
<li>一个句子可以被看成一个单词序列<code>S = (w1, w2, w3, w4, ... wm)</code><ul>
<li>其中<code>m</code>代表句子的长度</li>
</ul>
</li>
<li>它的概率可以表示为：<code>p(S) = p(w1, w2, w3, w4, ... wm) = p(w1) * p(w2 | w1) * p(w3 | w1, w2)...p(wm | w1, w2, w3,..., wm-1)</code><ul>
<li>其中<code>p(wm | w1, w2, w3,..., wm-1)</code>表示，已知前m-1个单词时，第m个单词为wm的条件概率</li>
</ul>
</li>
</ul>
</li>
<li><p>然而</p>
<ul>
<li>任何一门语言的词汇量都很大，词汇的组合更是不计其数</li>
<li>假设一门语言的词汇量为V，如果要将<code>p(wm | w1, w2, w3,..., wm-1)</code>的所有参数保存在一个模型中，则需要<code>pow(v, m)</code>个参数，数量太多</li>
</ul>
</li>
<li><p>n-gram</p>
<ul>
<li>为了控制参数的数量，n-gram模型做了一个有限历史假设：当前单词的出现概率仅仅与前面的<code>n-1</code>个单词相关</li>
<li>公式如下<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/n-gram概率公式.jpg" alt=""></li>
<li>n-gram模型中的n指的是当前单词依赖它前面的单词的个数</li>
<li>通常n可以取1、2、3、4，其中n取1、2、3时分别称为unigram、bigram、trigram</li>
<li>假设某种语言的单词表大小为V，那么n-gram模型需要估计的不同参数数量为O(pow(v, n))量级</li>
<li>当n越大时，n-gram模型在理论上越准确，但也越复杂，需要的计算量和训练语料数据量也越大</li>
<li>n-gram模型的参数一般采用最大似然估计(Maximum Likelihood Estimation, MLE)方法计算<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/n-gramMLE方法.jpg" alt=""></li>
<li>训练语料的规模越大，参数估计的结果越可靠</li>
<li>但即使训练数据的规模非常大时，还是有许多单词序列在训练语料中不会出现，这就导致许多参数为0，为了避免因为乘以0而导致整个句子概率为0，使用最大似然估计的方法时需要加入平滑避免参数取值为0</li>
</ul>
</li>
</ul>
<hr>
<h4 id="9-1-2-语言模型的评价方法—–复杂度-perplexity"><a href="#9-1-2-语言模型的评价方法—–复杂度-perplexity" class="headerlink" title="9.1.2 语言模型的评价方法—–复杂度(perplexity)"></a>9.1.2 语言模型的评价方法—–复杂度(perplexity)</h4><ul>
<li><p>复杂度简介(perplexity)</p>
<ul>
<li>语言模型效果好坏的常用评价指标是复杂度(perplexity)</li>
<li>在一个测试集上得到的perplexity越低，说明建模的效果越好</li>
<li>复杂度计算公式</li>
<li>perplexity值刻画的是语言模型预测一个语言样本的能力<ul>
<li>比如，已知(w1, w2, w3,…, wm)这句话会出现在语料库中，那么通过语言模型计算得到的这句话的概率越高，说明语言模型对这个语料库拟合的越好</li>
</ul>
</li>
</ul>
</li>
<li><p>perplexity的理解</p>
<ul>
<li>perplexity实际上计算每一个单词得到的概率倒数的几何平均</li>
<li>perplexity可以理解为模型预测下一个词时的平均可选择数量</li>
<li>目前在PTB数据集上最好的语言模型perplexity为47.7，就是在平均情况下，该模型预测下一个词时，有47.7个词等可能地可以作为下一个词的合理选择</li>
</ul>
</li>
<li><p>perplexity与交叉熵</p>
<ul>
<li>交叉熵(Cross Entropy)用来描述两个概率分布之间的一种距离</li>
<li>在数学上log perplexity可以看作真实分布与预测分布之间的交叉熵</li>
<li>即每个位置上单词的真实分布与模型的预测分布之间的交叉熵</li>
</ul>
</li>
<li><p>TensorFlow提供了两个方便计算交叉熵的函数</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设词汇表的大小为3，语料包含两个单词"2 0"</span></span><br><span class="line">word_labels = tf.constant([<span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设模型对两个单词预测时，产生的logit分别为[2.0, -1.0, 3.0]和[1.0, 0.0, -0.5]</span></span><br><span class="line"><span class="comment"># 注意这里的logit不是概率，因此它们不是0.0-1.0之间的数字</span></span><br><span class="line"><span class="comment"># 如需要计算概率，则需要调用prob = tf.nn.softmax(logits)</span></span><br><span class="line"><span class="comment"># 但这里计算交叉熵的函数直接输入logits即可</span></span><br><span class="line">predict_logits = tf.constant([[<span class="number">2.0</span>, <span class="number">-1.0</span>, <span class="number">3.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sparse_softmax_cross_entropy_with_logits函数计算交叉熵，即log perplexity的值</span></span><br><span class="line">loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=word_labels, logits=predict_logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算perplexity损失的值</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(loss))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[0.32656264 0.4643688 ]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax_cross_entropy_with_logits与上面的函数类似，但是需要将预测目标以概率分布的形式给出</span></span><br><span class="line">word_prob_distribution = tf.constant([[<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits(labels=word_prob_distribution, logits=predict_logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算perplexity损失的值</span></span><br><span class="line">print(sess.run(loss))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[0.32656264 0.4643688 ]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于softmax_cross_entropy_with_logits允许提供一个概率分布，因此在使用时有更大的自由度</span></span><br><span class="line"><span class="comment"># 一种叫label smoothing的技巧是将正确数据的概率设为一个比1.0略小的值，将错误数据的概率设为比0.0略大的值</span></span><br><span class="line"><span class="comment"># 这样可以避免模型与数据过拟合，在某些时候可以提高训练效果</span></span><br><span class="line">word_prob_smooth = tf.constant([[<span class="number">0.01</span>, <span class="number">0.01</span>, <span class="number">0.98</span>], [<span class="number">0.98</span>, <span class="number">0.01</span>, <span class="number">0.01</span>]])</span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits(labels=word_prob_smooth, logits=predict_logits)</span><br><span class="line">print(sess.run(loss))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[0.37656265 0.48936883]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="9-2-神经语言模型"><a href="#9-2-神经语言模型" class="headerlink" title="9.2 神经语言模型"></a>9.2 神经语言模型</h3><hr>
<ul>
<li><p>循环神经网络可以将任意长度的上文信息存储在隐藏状态中，因此使用循环神经网络作为语言模型有着天然的优势</p>
</li>
<li><p>RNN自然语言模型DEMO</p>
<ul>
<li>如图<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/RNN自然语言建模.jpg" alt=""></li>
<li>工作原理<ul>
<li>每个时刻的输入为一个句子中的单词wi</li>
<li>每个时刻的输出为一个概率分布，表示句子中下一个位置为不同单词的概率p(wi+1 | w1, w2, w3,…, wi)</li>
<li>比如<ul>
<li>第一个时刻输入的单词为“大海”，而输出为p(x|”大海”)</li>
<li>即知道第一个词为大海后，其他不同单词出现在下一个位置的概率</li>
<li>若p(“的”|”大海”)=0.8，则“大海”之后的单词为“的”出现的概率为0.8</li>
</ul>
</li>
<li>类似的，可以得到p(“颜色”|”大海”,”的”)、p(“是”|”大海”,”的”,”颜色”)、p(“蓝色|”大海”,”的”,”颜色”,”是”)</li>
<li>然后将这些概率取对数再平均起来，就可以得到在这个句子上计算的log perplexity<h4 id="9-2-1-PTB数据集的预处理"><a href="#9-2-1-PTB数据集的预处理" class="headerlink" title="9.2.1 PTB数据集的预处理"></a>9.2.1 PTB数据集的预处理</h4></li>
</ul>
</li>
</ul>
</li>
<li><p>背景:自然语言文本数据无法直接被当成数值提供给神经网络，所以需要对它进行预处理</p>
</li>
<li><p>PTB(Penn Treebank Dataset)文本数据集是目前语言模型学习中使用最为广泛的数据集</p>
</li>
<li><p>数据集中共含有9998个词汇，加上稀有词语的特殊符号<code>&lt;unk&gt;</code>和语句结束换行符<code>&lt;eos&gt;</code>在内，一共10000个词汇</p>
</li>
<li><p>为了将文本转化为模型可以读入的单词序列，需要将这10000个不同的词汇分别映射到0-9999之间的整数编号</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集数据文件</span></span><br><span class="line">RAW_DATA = <span class="string">"/PycharmProjects/TFDemo/data/PTB/data/ptb.train.txt"</span></span><br><span class="line"><span class="comment"># 输出的词汇表文件</span></span><br><span class="line">VOCAB_OUTPUT = <span class="string">"/PycharmProjects/TFDemo/data/PTB/generate/ptb.vocab"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计单词出现的频率</span></span><br><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">with</span> codecs.open(RAW_DATA, <span class="string">'r'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> line.strip().split():</span><br><span class="line">            counter[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照词频顺序对单词进行排序</span></span><br><span class="line">sorted_word_to_cnt = sorted(counter.items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">sorted_words = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> sorted_word_to_cnt]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入句子结束符&lt;eos&gt;</span></span><br><span class="line">sorted_words = [<span class="string">"&lt;eos&gt;"</span>] + sorted_words</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将排序好的单词写入输出文件，其中行号就代表了单词的编号</span></span><br><span class="line"><span class="keyword">with</span> codecs.open(VOCAB_OUTPUT, <span class="string">'w'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> file_output:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sorted_words:</span><br><span class="line">        file_output.write(word + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在确定了词汇表之后，再将训练文件、测试文件等都根据词汇文件转化为单词编号，每个单词编号就是它在词汇文件中的行号</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line">RAW_DATA = <span class="string">"/PycharmProjects/TFDemo/data/PTB/data/ptb.train.txt"</span></span><br><span class="line">VOCAB = <span class="string">"/PycharmProjects/TFDemo/data/PTB/generate/ptb.vocab"</span></span><br><span class="line">OUTPUT_DATA = <span class="string">"/PycharmProjects/TFDemo/data/PTB/generate/ptb.train"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取词汇表，并建立词汇到单词编号的映射</span></span><br><span class="line"><span class="keyword">with</span> codecs.open(VOCAB, <span class="string">"r"</span>, <span class="string">"utf-8"</span>) <span class="keyword">as</span> f_vocab:</span><br><span class="line">    vocab = [w.strip() <span class="keyword">for</span> w <span class="keyword">in</span> f_vocab.readlines()]</span><br><span class="line">word_to_code = &#123;k: v <span class="keyword">for</span> (k, v) <span class="keyword">in</span> zip(vocab, range(len(vocab)))&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开输入与输出文件</span></span><br><span class="line">fin = codecs.open(RAW_DATA, <span class="string">"r"</span>, <span class="string">"utf-8"</span>)</span><br><span class="line">fout = codecs.open(OUTPUT_DATA, <span class="string">"w"</span>, <span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据词汇映射进行转码</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">    words = line.strip().split() + [<span class="string">"&lt;eos&gt;"</span>]</span><br><span class="line">    out_line = <span class="string">' '</span>.join([str(word_to_code[w]) <span class="keyword">for</span> w <span class="keyword">in</span> words]) + <span class="string">'\n'</span></span><br><span class="line">    fout.write(out_line)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭文件</span></span><br><span class="line">fin.close()</span><br><span class="line">fout.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p>在实际工程中，通常使用TFRecords格式来提高读写效率</p>
</li>
</ul>
<hr>
<h4 id="9-2-2-PTB数据的batching方法"><a href="#9-2-2-PTB数据的batching方法" class="headerlink" title="9.2.2 PTB数据的batching方法"></a>9.2.2 PTB数据的batching方法</h4><ul>
<li><p>背景</p>
<ul>
<li>在文本数据中，由于每个句子的长度不同，又无法像图像一样调整到固定维度，因此batching时需要一些特殊操作</li>
<li>最常见的方法是使用填充(padding)，将同一batch内的句子长度补齐</li>
<li>在PTB数据集中，每个句子并非随机抽取的文本，而是在上下文之间有关联的内容</li>
<li>语言模型为了利用上下文信息，必须将前面句子的消息传递到后面的句子</li>
</ul>
</li>
<li><p>解决方案1</p>
<ul>
<li>简介：如果模型大小不限，最理想的设计是将整个文档前后连接起来，当作一个句子来训练</li>
<li>如图<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/将整个文档前后连接示意图.jpg" alt=""></li>
<li>问题<ul>
<li>这样会导致计算图过大</li>
<li>另外序列过长还会导致梯度爆炸问题</li>
</ul>
</li>
</ul>
</li>
<li><p>解决方案2</p>
<ul>
<li>简介<ul>
<li>将长序列切割为固定长度的子序列</li>
<li>循环神经网络在处理完一个子序列后，它最终的隐藏状态将复制到下一个序列作为初始值</li>
<li>这样，在前向计算时，效果等同于一次性顺序读取了整个文档</li>
<li>而在反向传播时，梯度则只会在每个子序列内部传播</li>
</ul>
</li>
<li>如图<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/按固定长度切分文档示意图.jpg" alt=""></li>
</ul>
</li>
<li><p>解决方案3</p>
<ul>
<li>背景<ul>
<li>为了利用计算时的并行能力</li>
<li>我们希望每一次计算可以对多个句子进行并行处理</li>
<li>同时又要尽量暴增batch之间的上下文连续</li>
</ul>
</li>
<li>简介<ul>
<li>先将整个文档切分为若干个连续段落</li>
<li>再让batch中的每个位置负责其中一段</li>
</ul>
</li>
<li>如图<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/将长序列切分为batch示意图.jpg" alt=""></li>
</ul>
</li>
<li><p>下面的代码从文本文件中读取数据，并按照解决方案3将数据整理为batch</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">TRAIN_DATA = <span class="string">"/PycharmProjects/TFDemo/data/PTB/generate/ptb.train"</span></span><br><span class="line">TRAIN_BATCH_SIZE = <span class="number">20</span></span><br><span class="line">TRAIN_NUM_STEP = <span class="number">35</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从文件中读取数据，并返回包含单词编号的数组</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        <span class="comment"># 将整个文档读入一个长字符串</span></span><br><span class="line">        id_string = <span class="string">' '</span>.join([line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> fin.readlines()])</span><br><span class="line">    id_list = [int(w) <span class="keyword">for</span> w <span class="keyword">in</span> id_string.split()]</span><br><span class="line">    <span class="keyword">return</span> id_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_batches</span><span class="params">(id_list, batch_size, num_step)</span>:</span></span><br><span class="line">    <span class="comment"># 计算总的batch数量，每个batch包含的单词数量是batch_size * num_steps</span></span><br><span class="line">    num_batches = (len(id_list) <span class="number">-1</span>) // (batch_size * num_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据整理为一个维度为[batch_size, num_batches * num_step]的二维数组</span></span><br><span class="line">    data = np.array(id_list[:num_batches * batch_size * num_step])</span><br><span class="line">    data = np.reshape(data, [batch_size, num_batches * num_step])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 沿着第二个维度将数据切分为num_batches个batch，存入一个数组中</span></span><br><span class="line">    data_batches = np.split(data, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重复上述操作，但是每个位置向右移动一位</span></span><br><span class="line">    <span class="comment"># 作为输入数据的标记值，也就是RNN每一步输入所需要预测的下一个单词</span></span><br><span class="line">    label = np.array(id_list[<span class="number">1</span>: num_batches * batch_size * num_step + <span class="number">1</span>])</span><br><span class="line">    label = np.reshape(label, [batch_size, num_batches * num_step])</span><br><span class="line">    label_batches = np.split(label, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵</span></span><br><span class="line">    <span class="keyword">return</span> list(zip(data_batches, label_batches))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    train_batches = make_batches(read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line">    print(train_batches)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h4 id="9-2-3-基于循环神经网络的神经语言模型"><a href="#9-2-3-基于循环神经网络的神经语言模型" class="headerlink" title="9.2.3 基于循环神经网络的神经语言模型"></a>9.2.3 基于循环神经网络的神经语言模型</h4><ul>
<li><p>神经语言模型与第8章介绍的循环神经网络相比，多了两层：词向量层(embedding)和softmax层</p>
</li>
<li><p>词向量层</p>
<ul>
<li>词向量<ul>
<li>在输入层，每一个单词用一个实数向量表示，这个向量被称为“词向量”(word embedding)</li>
<li>词向量可以形象地理解为将词汇表嵌入到一个固定维度的实数空间中</li>
</ul>
</li>
<li>将单词编号转化为词向量的用处<ol>
<li>降低输入的维度<ul>
<li>如果不使用词向量层，而直接将单词以one-hot vector的形式输入循环神经网络，那么输入的维数大小将与词汇表大小相同</li>
<li>而词向量的维度通常小于词汇表大小</li>
<li>这将大大减少循环神经网络的参数数量与计算量</li>
</ul>
</li>
<li>增加语义信息<ul>
<li>简单的单词编号不包含任何语义信息<ul>
<li>两个单词之间编号相近，并不代表它们的含义有任何关联</li>
</ul>
</li>
<li>词向量层将稀疏的编号转化为稠密的向量表示，这使得词向量有可能包含更加丰富的信息</li>
<li>在自然语言应用中学习得到的词向量通常将含义相似的词赋予取值相近的词向量，使得上层的网络可以更容易地抓住相似单词之间的共性</li>
</ul>
</li>
</ol>
</li>
<li>词向量层的定义代码  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">embedding = tf.get_variable(<span class="string">"embedding"</span>, [VOCAB_SIZE, EMB_SIZE])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出的矩阵比输入矩阵多一个维度，新增维度的大小为EMB_SIZE</span></span><br><span class="line"><span class="comment"># 在语言模型中，一般input_data的维度为batch_size * num_steps</span></span><br><span class="line"><span class="comment"># 在经过词向量层之后, 维度为batch_size * num_steps * EMB_SIZE</span></span><br><span class="line">input_embedding = tf.nn.embedding_lookup(embedding, input_data)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Softmax层</p>
<ul>
<li>作用：将循环神经网络的输出转化为一个单词表中每个单词的输出概率</li>
<li>步骤<ol>
<li>使用一个线性映射将循环神经网络的输出映射为一个维度与词汇表大小相同的向量，这一步输出叫做logits</li>
<li>调用softmax方法将logits转化为加和为1的概率<ul>
<li>事实上，语言模型的每一步输出都可以看作一个分类问题，在VOCAB_SIZE个可能的类别中决定这一步最可能输出的单词</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>通过共享参数减少参数数量</p>
<ul>
<li>Softmax层和词向量层的参数数量都与词汇表大小VOCAB_SIZE成正比</li>
<li>由于VOCAB_SIZE的数值通常较大，而HIDDEN_SIZE相对较小，导致softmax和embdding在整个网络的参数数量中占有很大比例</li>
<li>有研究指出，如果共享词向量层和Softmax层的参数，不仅能大幅度减少参数数量，还能提高最终模型结果</li>
</ul>
</li>
<li><p>完整代码</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">TRAIN_DATA = <span class="string">'/PycharmProjects/TFDemo/data/PTB/generate/ptb.train'</span>  <span class="comment"># 训练数据路径</span></span><br><span class="line">EVAL_DATA = <span class="string">'/PycharmProjects/TFDemo/data/PTB/generate/ptb.valid'</span>   <span class="comment"># 验证数据路径</span></span><br><span class="line">TEST_DATA = <span class="string">'/PycharmProjects/TFDemo/data/PTB/generate/ptb.test'</span>    <span class="comment"># 测试数据路径</span></span><br><span class="line"></span><br><span class="line">HIDDEN_SIZE = <span class="number">300</span>                                                   <span class="comment"># 隐藏层规模</span></span><br><span class="line">NUM_LAYERS = <span class="number">2</span>                                                      <span class="comment"># 深层循环网络中LSTM结构的层数</span></span><br><span class="line">VOCAB_SIZE = <span class="number">10000</span>                                                  <span class="comment"># 词典规模</span></span><br><span class="line">TRAIN_BATCH_SIZE = <span class="number">20</span>                                               <span class="comment"># 训练数据batch的大小</span></span><br><span class="line">TRAIN_NUM_STEP = <span class="number">35</span>                                                 <span class="comment"># 训练数据截断长度,可以理解为单词序列的长度</span></span><br><span class="line">EVAL_BATCH_SIZE = <span class="number">1</span>                                                 <span class="comment"># 测试数据batch的大小</span></span><br><span class="line">EVAL_NUM_STEP = <span class="number">1</span>                                                   <span class="comment"># 测试数据截断长度</span></span><br><span class="line"></span><br><span class="line">NUM_EPOCH = <span class="number">5</span>                                                       <span class="comment"># 使用训练数据的轮数</span></span><br><span class="line">LSTM_KEEP_PROB = <span class="number">0.9</span>                                                <span class="comment"># LSTM节点不被dropout的概率</span></span><br><span class="line">EMBEDDING_KEEP_PROB = <span class="number">0.9</span>                                           <span class="comment"># 词向量不被dropout的概率</span></span><br><span class="line">MAX_GRAD_NORM = <span class="number">5</span>                                                   <span class="comment"># 用于控制梯度膨胀的梯度大小上限</span></span><br><span class="line">SHARE_EMB_AND_SOFTMAX = <span class="keyword">True</span>                                        <span class="comment"># 在Softmax层和词向量层之间共享参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用PTBModel类定义整个模型的结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PTBModel</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, is_training, batch_size, num_steps)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录使用的batch大小和截断长度</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义每一步的输入和预期输出</span></span><br><span class="line">        <span class="comment"># 两者的维度都为[batch_size, num_steps]</span></span><br><span class="line">        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])</span><br><span class="line">        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义使用LSTM结构为循环体结构且使用dropout的深层循环神经网络</span></span><br><span class="line">        drop_keep_prob = LSTM_KEEP_PROB <span class="keyword">if</span> is_training <span class="keyword">else</span> <span class="number">1.0</span></span><br><span class="line">        lstm_cells = [</span><br><span class="line">            tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">                tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),</span><br><span class="line">                output_keep_prob=drop_keep_prob</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)</span><br><span class="line">        ]</span><br><span class="line">        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化最初的状态，即全0的向量</span></span><br><span class="line">        <span class="comment"># 这个量只在每个epoch初始化第一个batch时使用</span></span><br><span class="line">        self.initial_state = cell.zero_state(batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义词向量矩阵</span></span><br><span class="line">        embedding = tf.get_variable(<span class="string">"embedding"</span>, [VOCAB_SIZE, HIDDEN_SIZE])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输入单词转化为词向量</span></span><br><span class="line">        inputs = tf.nn.embedding_lookup(embedding, self.input_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只在训练中使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义输出列表，这里先将不同时刻LSTM结构的输出收集起来，再一起提供给sotfmax层</span></span><br><span class="line">        outputs = []</span><br><span class="line">        state = self.initial_state</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"RNN"</span>):</span><br><span class="line">            <span class="keyword">for</span> time_step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">                <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">                    tf.get_variable_scope().reuse_variables()</span><br><span class="line">                cell_output, state = cell(inputs[:, time_step, :], state)</span><br><span class="line">                outputs.append(cell_output)</span><br><span class="line">        output = tf.reshape(tf.concat(outputs, <span class="number">1</span>), [<span class="number">-1</span>, HIDDEN_SIZE])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Softmax层:将RNN在每个位置上的输入转化为各个单词的logits</span></span><br><span class="line">        <span class="keyword">if</span> SHARE_EMB_AND_SOFTMAX:</span><br><span class="line">            weight = tf.transpose(embedding)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            weight = tf.get_variable(<span class="string">"weight"</span>, [HIDDEN_SIZE, VOCAB_SIZE])</span><br><span class="line">        bias = tf.get_variable(<span class="string">"bias"</span>, [VOCAB_SIZE])</span><br><span class="line">        logits = tf.matmul(output, weight) + bias</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义交叉熵损失函数和平均损失</span></span><br><span class="line">        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(self.targets, [<span class="number">-1</span>]), logits=logits)</span><br><span class="line">        self.cost = tf.reduce_sum(loss) / batch_size</span><br><span class="line">        self.final_state = state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只在训练模型时定义反向传播操作</span></span><br><span class="line">        trainable_variables = tf.trainable_variables()</span><br><span class="line">        <span class="comment"># 控制梯度大小不能超过MAX_GRAD_NORM</span></span><br><span class="line">        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)</span><br><span class="line">        <span class="comment"># 定义优化方法</span></span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义训练步骤</span></span><br><span class="line">        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用给定的模型model在数据data上运行train_op并返回在全部数据上的perplexity值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(sess, model, batches, train_op, output_log, step)</span>:</span></span><br><span class="line">    <span class="comment"># 计算平均perplexity的辅助变量</span></span><br><span class="line">    total_costs = <span class="number">0.0</span></span><br><span class="line">    iters = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 将state初始化为全0</span></span><br><span class="line">    state = sess.run(model.initial_state)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练一个epoch</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> batches:</span><br><span class="line">        <span class="comment"># 在当前batch上运行train_op并计算损失值</span></span><br><span class="line">        <span class="comment"># 交叉熵损失函数计算的就是下一个单词为给定单词的概率</span></span><br><span class="line">        cost, state, _ = sess.run(</span><br><span class="line">            [model.cost, model.final_state, train_op],</span><br><span class="line">            &#123;model.input_data: x, model.targets: y, model.initial_state: state&#125;</span><br><span class="line">        )</span><br><span class="line">        total_costs += cost</span><br><span class="line">        iters += model.num_steps</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_log <span class="keyword">and</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %d steps, perplexity is %.3f"</span> % (step, np.exp(cost / model.num_steps)))</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> step, np.exp(total_costs / iters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从文件中读取数据，并返回包含单词编号的数组</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        <span class="comment"># 将整个文档读入一个长字符串</span></span><br><span class="line">        id_string = <span class="string">' '</span>.join([line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> fin.readlines()])</span><br><span class="line">    id_list = [int(w) <span class="keyword">for</span> w <span class="keyword">in</span> id_string.split()]</span><br><span class="line">    <span class="keyword">return</span> id_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将包含单词编号的数组进行batching</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_batches</span><span class="params">(id_list, batch_size, num_step)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算总的batch数量，每个batch包含的单词数量是batch_size * num_steps</span></span><br><span class="line">    num_batches = (len(id_list) <span class="number">-1</span>) // (batch_size * num_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据整理为一个维度为[batch_size, num_batches * num_step]的二维数组</span></span><br><span class="line">    data = np.array(id_list[:num_batches * batch_size * num_step])</span><br><span class="line">    data = np.reshape(data, [batch_size, num_batches * num_step])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 沿着第二个维度将数据切分为num_batches个batch，存入一个数组中</span></span><br><span class="line">    data_batches = np.split(data, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重复上述操作，但是每个位置向右移动一位</span></span><br><span class="line">    <span class="comment"># 作为输入数据的标记值，也就是RNN每一步输入所需要预测的下一个单词</span></span><br><span class="line">    label = np.array(id_list[<span class="number">1</span>: num_batches * batch_size * num_step + <span class="number">1</span>])</span><br><span class="line">    label = np.reshape(label, [batch_size, num_batches * num_step])</span><br><span class="line">    label_batches = np.split(label, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵</span></span><br><span class="line">    <span class="keyword">return</span> list(zip(data_batches, label_batches))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义初始化函数</span></span><br><span class="line">    initializer = tf.random_uniform_initializer(<span class="number">-0.05</span>, <span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义训练用的循环神经网络模型</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"language_model"</span>, reuse=<span class="keyword">None</span>, initializer=initializer):</span><br><span class="line">        train_model = PTBModel(<span class="keyword">True</span>, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义测试用的循环神经网络模型，它与train_model共用参数，但是没有dropout</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"language_model"</span>, reuse=<span class="keyword">True</span>, initializer=initializer):</span><br><span class="line">        eval_model = PTBModel(<span class="keyword">False</span>, EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 初始化变量</span></span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="comment"># 初始化训练集、验证集和测试集数据</span></span><br><span class="line">        train_batches = make_batches(read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line">        eval_batches = make_batches(read_data(EVAL_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line">        test_batches = make_batches(read_data(TEST_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line"></span><br><span class="line">        step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 一共要用所有训练数据NUM_EPOCH次，以提高训练精度</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_EPOCH):</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"In iteration: %d"</span> % (i + <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># 先运行训练过程</span></span><br><span class="line">            step, train_pplx = run_epoch(sess, train_model, train_batches, train_model.train_op, <span class="keyword">True</span>, step)</span><br><span class="line">            print(<span class="string">"Epoch: %d Train Perplexity: %.3f"</span> % (i + <span class="number">1</span>, train_pplx))</span><br><span class="line">            <span class="comment"># 运行验证过程</span></span><br><span class="line">            _, eval_pplx = run_epoch(sess, eval_model, eval_batches, tf.no_op(), <span class="keyword">False</span>, <span class="number">0</span>)</span><br><span class="line">            print(<span class="string">"Epoch: %d Eval Perplexity: %.3f"</span> % (i + <span class="number">1</span>, eval_pplx))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化结束后，运行测试过程</span></span><br><span class="line">        _, test_pplx = run_epoch(sess, eval_model, test_batches, tf.no_op(), <span class="keyword">False</span>, <span class="number">0</span>)</span><br><span class="line">        print(<span class="string">"Test Perplexity: %.3f"</span> % test_pplx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">In iteration: 1</span></span><br><span class="line"><span class="string">After 0 steps, perplexity is 9971.553</span></span><br><span class="line"><span class="string">After 100 steps, perplexity is 781.005</span></span><br><span class="line"><span class="string">After 200 steps, perplexity is 590.538</span></span><br><span class="line"><span class="string">After 300 steps, perplexity is 348.273</span></span><br><span class="line"><span class="string">After 400 steps, perplexity is 400.334</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">After 6400 steps, perplexity is 66.869</span></span><br><span class="line"><span class="string">After 6500 steps, perplexity is 62.698</span></span><br><span class="line"><span class="string">After 6600 steps, perplexity is 72.749</span></span><br><span class="line"><span class="string">Epoch: 5 Train Perplexity: 72.026</span></span><br><span class="line"><span class="string">Epoch: 5 Eval Perplexity: 106.827</span></span><br><span class="line"><span class="string">Test Perplexity: 103.619</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/70/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/70/">70</a><span class="page-number current">71</span><a class="page-number" href="/page/72/">72</a><span class="space">&hellip;</span><a class="page-number" href="/page/92/">92</a><a class="extend next" rel="next" href="/page/72/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://avatars0.githubusercontent.com/u/33653960?s=400&u=eb438b904ecb9d91f3aa9b777155a6488599493b&v=4" alt="T0UGH(GuiPing Wang)">
            
              <p class="site-author-name" itemprop="name">T0UGH(GuiPing Wang)</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">92</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">52</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/T0UGH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="tough.neu.edu@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friends
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://neusoftware.top/solo" title="ChengYi" target="_blank">ChengYi</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://mike4ellis.github.io/" title="Mike" target="_blank">Mike</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">T0UGH(GuiPing Wang)</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'E1MH6h0YP3yhA0PJsohNBgiT-gzGzoHsz',
        appKey: 'YOiN6zLq3XGfKmlR0b8vyHtN',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
