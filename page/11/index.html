<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="打怪升级日常">
<meta property="og:url" content="http://yoursite.com/page/11/index.html">
<meta property="og:site_name" content="打怪升级日常">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="打怪升级日常">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/11/">





  <title>打怪升级日常</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    <a href="https://github.com/T0UGH" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">打怪升级日常</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">两星炸弹人(╯‵□′)╯炸弹！•••</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/" itemprop="url">[Tensorflow][6]图像识别与卷积神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-10T14:49:47+08:00">
                2019-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第6章-图像识别与卷积神经网络"><a href="#第6章-图像识别与卷积神经网络" class="headerlink" title="第6章 图像识别与卷积神经网络"></a>第6章 图像识别与卷积神经网络</h2><p>卷积神经网络(Convolutional Neural Network,CNN)</p>
<h3 id="6-1-图像识别问题简介及经典数据集"><a href="#6-1-图像识别问题简介及经典数据集" class="headerlink" title="6.1 图像识别问题简介及经典数据集"></a>6.1 图像识别问题简介及经典数据集</h3><ul>
<li><p>图像识别问题简介</p>
<ul>
<li>图像识别问题希望借助计算机程序来处理、分析和理解图片中的内容，使得计算机可以从图片中自动识别各种不同模式的目标和对象</li>
<li>卷积神经网络是图像识别问题取得突破性进展的最主要技术支持</li>
<li>在NNIST上，相比其他算法，卷积神经网络可以得到更低的错误率。而且通过卷积神经网络达到的错误率已经非常接近人工标注的错误率</li>
</ul>
</li>
<li><p>CIFAR数据集</p>
<ul>
<li>CIFAR数据集分为CIFAR-10和CIFAR-100两个问题，它们都是图像词典项目(Visual Dictionary)中800万张图片的一个子集</li>
<li>CIFAR数据集中的图片为32*32的彩色图片</li>
<li>CIFAR-10问题收集了来自10个不同种类的60000张图片</li>
<li><a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR官网</a>提供不同格式的数据集下载</li>
<li>CIFAR-10中的图片大小都是固定的且每一张图片中仅包含一个种类的实体</li>
<li>样例<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/cifar.jpg" alt=""></li>
</ul>
</li>
<li><p>MNIST和CIFAR相对于真实环境的2个问题</p>
<ol>
<li>现实生活中的图片分辨率要远高于32*32，而且图像的分辨率也不会是固定的</li>
<li>现实生活中的物体类别很多，而且一张图片中不会只出现一个种类的物体</li>
</ol>
</li>
<li><p>ImageNet图像数据库</p>
<ul>
<li>ImageNet是一个基于WordNet的大型图像数据库</li>
<li>在ImageNet中，将近1500万图片被关联到了WorkNet的大约20000个名词同义词集上</li>
<li><p>如图</p>
<ul>
<li>在这张图片上用几个矩形框出了不同实物的轮廓</li>
<li>在物体识别问题上，一般将用于框出实体的矩形称为bounding box</li>
</ul>
</li>
</ul>
</li>
<li><p>ILSVRC：ImageNet每年都举办图像识别相关的竞赛(ImageNet Large Scale Visual Recognition Challenge)</p>
</li>
<li><p>top-N正确率：指的是图像识别算法给出前N个答案的正确率作为比较的方法，其中N的取值一般为3或5</p>
</li>
</ul>
<h3 id="6-2-卷积神经网络简介"><a href="#6-2-卷积神经网络简介" class="headerlink" title="6.2 卷积神经网络简介"></a>6.2 卷积神经网络简介</h3><ul>
<li><p>全连接神经网络：每两层之间的所有节点都是有边相连的</p>
</li>
<li><p>卷积神经网络简介</p>
<ul>
<li>卷积神经网络也是通过一层一层的节点组织起来的</li>
<li>卷积神经网络中的每个节点都是一个神经元</li>
<li>卷积神经网络中相邻两层之间只有部分节点相连</li>
<li>为了展示每一层神经元的维度，一般会将每一层卷积层的节点组织成一个三维矩阵</li>
<li>卷积神经网络的输入层就是图像的原始像素，而输出层中的每一个节点代表了不同类别的可信度，这和全连接神经网络的输入输出是一致的</li>
</ul>
</li>
<li><p>卷积与全连接的区别：神经网络中相邻两层的连接方式</p>
</li>
<li><p>为什么全连接神经网络无法很好地处理图像数据</p>
<ul>
<li>使用全连接神经网络处理图像的最大问题在于全连接的参数太多</li>
<li>参数增多除了导致计算速度减慢，还很容易导致过拟合问题</li>
</ul>
</li>
<li><p>在卷积神经网络的前几层，每一层的节点都被组织成一个三维矩阵</p>
<ul>
<li>比如处理CIFAR-10数据集中的图片时，可以将输入层组织为一个32<em>32</em>3的三维矩阵</li>
</ul>
</li>
<li><p>一个卷积神经网络主要由以下5种结构组成</p>
<ol>
<li>输入层<ul>
<li>输入层是整个神经网络的输入，在处理图像的卷积神经网络中，它一般代表了一张图片的像素矩阵</li>
<li>三维矩阵的长和宽代表了图像的大小，而三维矩阵的深度代表了图像的色彩通道<ul>
<li>比如黑白图片的深度为1，而在RGB色彩模式下，图像的深度为3</li>
</ul>
</li>
<li>卷积神经网络通过不同的神经网络结构将上一层的三维矩阵转化为下一层的三维矩阵</li>
</ul>
</li>
<li>卷积层<ul>
<li>卷积层中每个节点的输入只是上一层神经网络的一小块，这个小块常用的大小有3<em>3或者5</em>5</li>
<li>卷积层试图将神经网络中的每一小块进行更加深入地分析从而得到抽象程度更高的特征</li>
<li>一般来说，通过卷积层处理过的节点矩阵会变得更深</li>
</ul>
</li>
<li>池化层(Pooling)<ul>
<li>池化层神经网络不会改变三维矩阵的深度，但是它可以缩小矩阵的大小</li>
<li>池化操作可以认为是将一张分辨率较高的图片转化为分辨率较低的图片</li>
<li>通过池化层，可以进一步缩小最后全连接层中节点的个数，从而达到减少整个神经网络中参数的目的</li>
</ul>
</li>
<li>全连接层<ul>
<li>我们可以将卷积层和池化层看作自动图像特征提取的过程</li>
<li>在特征提取完成之后，仍然需要使用全连接层来完成分类任务</li>
</ul>
</li>
<li>Softmax层<ul>
<li>Softmax层主要用于分类问题</li>
<li>通过Softmax层，可以得到当前样例属于不同种类的概率分布情况</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="6-3-卷积神经网络常用结构"><a href="#6-3-卷积神经网络常用结构" class="headerlink" title="6.3 卷积神经网络常用结构"></a>6.3 卷积神经网络常用结构</h3><h4 id="6-3-1-卷积层"><a href="#6-3-1-卷积层" class="headerlink" title="6.3.1 卷积层"></a>6.3.1 卷积层</h4><ul>
<li><p>过滤器</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/过滤器结构示意图.jpg" alt=""></li>
<li>简介<ul>
<li>过滤器可以将当前层神经网络上的一个子节点矩阵转化为下一层神经网络上的一个单位节点矩阵</li>
<li>单位节点矩阵指的是一个长和宽都为1，但深度不限的节点矩阵</li>
</ul>
</li>
<li>过滤器的尺寸<ul>
<li>过滤器所处理的节点矩阵的长和宽都是由人工指定的，这个节点矩阵的尺寸也被称为过滤器的尺寸</li>
<li>过滤器处理的矩阵深度和当前层神经网络节点矩阵的深度是一致的，也就是深度上不做切割，有多深就用多深</li>
</ul>
</li>
<li>过滤器的深度<ul>
<li>过滤器另一个需要人工指定的设置是输出的单位节点矩阵的深度，这个设置称为过滤器的深度</li>
</ul>
</li>
</ul>
</li>
<li><p>过滤器的前向传播过程</p>
<ul>
<li>简介：就是通过左侧小矩阵中的节点计算出右侧单位矩阵中节点的过程</li>
<li>计算公式<ul>
<li>公式<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/过滤器计算公式.jpg" alt=""></li>
<li>解读<ul>
<li><code>ax,y,z</code>：过滤器中节点<code>(x,y,z)</code>的取值</li>
<li><code>f</code>：激活函数</li>
<li><code>wix,y,z</code>：输入节点<code>(x,y,z)</code>相对于第<code>i</code>个输出节点的权重参数</li>
<li><code>bi</code>：第<code>i</code>个输出节点的偏置值</li>
</ul>
</li>
<li>假设矩阵的size为2 <em> 2 </em> 3,过滤器的深度为5，则参数值为：2 <em> 2 </em> 3 * 5 + 5 = 65</li>
</ul>
</li>
</ul>
</li>
<li><p>卷积层结构的前向传播过程</p>
<ul>
<li>简介：卷积层结构的前向传播过程就是通过将一个过滤器从神经网络的左上角移动到右下角，并且在移动中计算每一个对应的单位矩阵得到的</li>
<li>过滤器每移动一次，可以计算得到一个单位矩阵（深度为k），将这些数值拼接成一个新的矩阵，就完成了卷积层前向传播的过程</li>
</ul>
</li>
<li><p>如何避免卷积层运算前后矩阵尺寸的变化</p>
<ul>
<li>背景：当过滤器的大小不为1*1时，卷积层前向传播得到的矩阵的尺寸要小于当前层矩阵的尺寸</li>
<li>解决：为了避免尺寸的变化，可以在当前层矩阵的边界上加入全0填充</li>
<li>如图<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/全0填充.jpg" alt=""></li>
</ul>
</li>
<li><p>通过设置过滤器移动的步长来调整结果矩阵的大小</p>
<ul>
<li>如图：显示了当移动步长为2且使用全0填充时，卷积层前向传播的过程<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/步长为2.jpg" alt=""></li>
</ul>
</li>
<li><p>卷积层中过滤器参数的设定</p>
<ul>
<li>简介：在卷积神经网络中，每一个卷积层中使用的过滤器中的参数都是一样的</li>
<li>直观理解：从直观上理解，共享过滤器的参数可以使得图像上的内容不受位置的影响</li>
<li>好处<ul>
<li>共享每一个卷积层中过滤器中的参数可以巨幅减少神经网络上的参数</li>
<li>卷积层的参数个数要远远小于全连接层</li>
<li>而且卷积层的参数个数和图片的大小无关，它只和过滤器的尺寸、深度以及当前层节点矩阵的深度有关，这使得卷积神经网络可以很好的扩展到更大的图像数据上</li>
</ul>
</li>
</ul>
</li>
<li><p>举例：使用全0填充、步长为2的卷积层前向传播的计算流程</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/卷积层前向传播举例.jpg" alt=""></li>
</ul>
</li>
<li><p>使用TensorFlow实现前向传播过程</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过tf.get_variable的方式创建过滤器的权重变量</span></span><br><span class="line"><span class="comment"># 卷积层的参数个数只和过滤器的尺寸、深度以及当前层节点矩阵的深度有关</span></span><br><span class="line"><span class="comment"># 此处声明的四维矩阵，前两维表示的是过滤器的尺寸，第三维是当前层的深度，第四维是过滤层的深度</span></span><br><span class="line">filter_weight = tf.get_variable(<span class="string">'weights'</span>, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">16</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过tf.get_variable的方式创建过滤器的偏重变量</span></span><br><span class="line"><span class="comment"># 偏置向量的大小等于过滤层的深度</span></span><br><span class="line"><span class="comment"># 原因是卷积层不仅共享权重矩阵也共享偏置向量</span></span><br><span class="line">biases = tf.get_variable(<span class="string">'biases'</span>, [<span class="number">16</span>], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.nn.conv2d是用来实现卷积层前向传播的函数</span></span><br><span class="line"><span class="comment"># 第一个参数为当前层的节点矩阵，这个矩阵为4维矩阵，后面三个维度表示一个节点矩阵，前面一个维度表示一个输入batch</span></span><br><span class="line"><span class="comment"># 第二个参数为卷积层的权重</span></span><br><span class="line"><span class="comment"># 第三个参数为不同维度上的步长，是一个长度为4的数组，但第一维和最后一维都是1，因为步长只对矩阵的长和宽有用</span></span><br><span class="line"><span class="comment"># 第四个参数为填充方法，其中"SAME"表示全0填充，"VALID"表示不填充</span></span><br><span class="line">conv = tf.nn.conv2d(input, filter_weight, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.nn.bias_add提供了方便的函数给每一个节点加上偏置项</span></span><br><span class="line"><span class="comment"># 这里不能直接使用加法</span></span><br><span class="line"><span class="comment"># 因为矩阵上不同位置上的节点都需要加上同样的偏置项，但是偏置项只有一个数</span></span><br><span class="line">bias = tf.nn.bias_add(conv, biases)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后使用ReLU激活函数完成去线性化</span></span><br><span class="line">actived_conv = tf.nn.relu(bias)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h4 id="6-3-2-池化层-Pooling-Layer"><a href="#6-3-2-池化层-Pooling-Layer" class="headerlink" title="6.3.2 池化层(Pooling Layer)"></a>6.3.2 池化层(Pooling Layer)</h4><ul>
<li><p>池化层背景</p>
<ul>
<li>池化层可以非常有效地缩小矩阵的尺寸，从而减少最后全连接层中的参数</li>
<li>使用池化层既可以加快计算速度也有防止过拟合问题的作用</li>
</ul>
</li>
<li><p>池化层的前向传播过程</p>
<ul>
<li>简介<ul>
<li>池化层前向传播也是通过移动一个类似过滤器的结构完成的</li>
<li>但是池化层过滤器中的计算不是节点的加权和，而是采用更加简单的最大值或者平均值运算</li>
</ul>
</li>
<li>几种池化层结构<ul>
<li>最大池化层(max pooling)<ul>
<li>使用最大值操作</li>
<li>被使用最多</li>
</ul>
</li>
<li>平均池化层(average pooling)<ul>
<li>使用平均值操作</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>池化层过滤器参数设定：也需要人工设定过滤器的大小、是否使用全0填充以及过滤器移动的步长等设置</p>
</li>
<li><p>池化层和卷积层过滤器的区别</p>
<ul>
<li>卷积层使用的过滤器是横跨整个深度的，而池化层使用的过滤器只影响一个深度上的节点</li>
<li>池化层的过滤器除了在长和宽两个维度移动，还需要在深度这个维度上移动</li>
</ul>
</li>
<li><p>举例：</p>
<ul>
<li>如图所示，是一个最大池化层前向传播计算过程<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/池化层前向传播举例.jpg" alt=""></li>
</ul>
</li>
<li><p>TensorFlow实现最大池化层的前向传播算法</p>
<ul>
<li><p>代码</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.nn.max_pool实现了最大池化层的前向传播过程</span></span><br><span class="line"><span class="comment"># 第一个参数代表输入矩阵</span></span><br><span class="line"><span class="comment"># ksize提供了过滤器的尺寸，其中第一维和最后一维必须是1，因为过滤器不可以跨不同输入样例或者节点矩阵的深度</span></span><br><span class="line"><span class="comment"># strides提供了步长信息，同样的，第一维和最后一维也只能是1</span></span><br><span class="line"><span class="comment"># padding表示填充方式</span></span><br><span class="line">pool = tf.nn.max_pool(actived_conv, ksize=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>除此之外，TensorFlow还提供了<code>tf.nn.avg_pool</code>来实现平均池化层</p>
</li>
</ul>
</li>
</ul>
<h3 id="6-4-经典卷积网络模型"><a href="#6-4-经典卷积网络模型" class="headerlink" title="6.4 经典卷积网络模型"></a>6.4 经典卷积网络模型</h3><ul>
<li>通过这些经典的卷积神经网络的网络结构可以总结出卷积神经网络结构设计的一些模式</li>
</ul>
<hr>
<h4 id="6-4-1-LeNet-5-模型"><a href="#6-4-1-LeNet-5-模型" class="headerlink" title="6.4.1 LeNet-5 模型"></a>6.4.1 LeNet-5 模型</h4><ul>
<li><p>简介：</p>
<ul>
<li>LeNet-5模型是第一个成功应用于数字识别问题的卷积神经网络</li>
<li>在MNIST数据集上，LeNet-5模型可以达到99.2%的正确率</li>
<li>LeNet-5模型总共有7层</li>
</ul>
</li>
<li><p>结构图<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/LeNet-5模型图.jpg" alt=""></p>
</li>
<li><p>结构详解</p>
<ol>
<li>第一层：卷积层<ul>
<li>输入：原始的图像像素，大小为<code>32*32*1</code></li>
<li>过滤器：尺寸为<code>5*5</code>，深度为<code>6</code>，不使用全0填充，步长为<code>1</code></li>
<li>输出：因为不使用全0填充，所以输出大小为<code>28*28*6</code></li>
</ul>
</li>
<li>第二层：池化层<ul>
<li>输入：第一层的大小为<code>28*28*6</code>的输出矩阵</li>
<li>过滤器：大小为<code>2*2</code>，步长也为<code>2</code>，采用max过滤器</li>
<li>输出：输出矩阵的大小为<code>14*14*6</code></li>
</ul>
</li>
<li>第三层：卷积层<ul>
<li>输入：第二层的大小为<code>14*14*6</code>的输出矩阵</li>
<li>过滤器：尺寸为<code>5*5</code>，深度为<code>16</code>，不使用全0填充，步长为<code>1</code></li>
<li>输出：输出矩阵的大小为<code>10*10*16</code></li>
</ul>
</li>
<li>第四层：池化层<ul>
<li>输入：第三层的大小为<code>10*10*16</code>的输出矩阵</li>
<li>过滤器：大小为<code>2*2</code>，步长为<code>2</code>，采用max过滤器</li>
<li>输出：输出矩阵的大小为<code>5*5*16</code></li>
</ul>
</li>
<li>第五层：全连接层<ul>
<li>输入：第四层的大小为<code>5*5*16</code>的输出矩阵</li>
<li>如何将卷积层的输出转化为全连接的输入?<ul>
<li>就是将这个三维数组，转化为一维数组，很简单</li>
<li>将<code>5*5*16</code>矩阵中的节点拉成一个向量</li>
<li>所以实际的输入为长度为400的一维数组</li>
</ul>
</li>
<li>输出：输出节点有120个</li>
</ul>
</li>
<li>第六层：全连接层<ul>
<li>本层的输入节点个数为120个，输出节点为84个</li>
</ul>
</li>
<li>第七层：全连接层<ul>
<li>这一层输出节点为10个，然后再使用softmax处理得到分类问题的答案即可</li>
</ul>
</li>
</ol>
</li>
<li><p>代码实现：完整的LeNet-5的TensorFlow实现</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">INPUT_NODE = <span class="number">784</span>                <span class="comment"># 输入节点数</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span>                <span class="comment"># 输出节点数</span></span><br><span class="line"></span><br><span class="line">IMAGE_SIZE = <span class="number">28</span>                 <span class="comment"># 图片的长和宽</span></span><br><span class="line">NUM_CHANNELS = <span class="number">1</span>                <span class="comment"># 图片的频道数，单色频道为1，多色RGB频道为3</span></span><br><span class="line">NUM_LABELS = <span class="number">10</span>                 <span class="comment"># 标记的数目</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一层卷积层的尺寸和深度</span></span><br><span class="line">CONV1_DEEP = <span class="number">32</span>                 <span class="comment"># 第一层卷积层的深度</span></span><br><span class="line">CONV1_SIZE = <span class="number">5</span>                  <span class="comment"># 第一层卷积层的尺寸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二层卷积层的尺寸和深度</span></span><br><span class="line">CONV2_DEEP = <span class="number">64</span>                 <span class="comment"># 第二层卷积层的深度</span></span><br><span class="line">CONV2_SIZE = <span class="number">5</span>                  <span class="comment"># 第二层卷积层的尺寸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层的节点数</span></span><br><span class="line">FC_SIZE = <span class="number">512</span></span><br><span class="line"></span><br><span class="line">BATCH_NODE = <span class="number">200</span>                <span class="comment"># batch的大小</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span>        <span class="comment"># 基础的学习率</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span>      <span class="comment"># 学习率的衰减率</span></span><br><span class="line">REGULARIZATION_RATE = <span class="number">0.0001</span>    <span class="comment"># 正则化项在损失函数中的系数</span></span><br><span class="line">TRAINING_STEPS = <span class="number">30000</span>          <span class="comment"># 训练轮数</span></span><br><span class="line"></span><br><span class="line">MODEL_SAVE_PATH = <span class="string">"/PycharmProjects/TFDemo/data/model/531/"</span></span><br><span class="line">MODEL_NAME = <span class="string">"model.ckpt"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积神经网络的前向传播过程</span></span><br><span class="line"><span class="comment"># 这里添加了一个参数regularizer，用来为全连接层添加正则化损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, regularizer)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 声明第一层卷积层并实现前向传播过程</span></span><br><span class="line">    <span class="comment"># 此层的输入为28*28*1的原始MNIST图片像素</span></span><br><span class="line">    <span class="comment"># 因为卷积层使用了全0填充，所以输出为28*28*32的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1-conv1'</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义和初始化第一层的权重和偏置</span></span><br><span class="line">        conv1_weights = tf.get_variable(<span class="string">"weight"</span>, [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        conv1_biases = tf.get_variable(<span class="string">"bias"</span>, [CONV1_DEEP], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用边长为5，深度为32的过滤器，过滤器移动的步长为1，且使用全0填充</span></span><br><span class="line">        conv1 = tf.nn.conv2d(input_tensor, filter=conv1_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用ReLu作为激活函数实现去线性化</span></span><br><span class="line">        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实现第二层池化层的前向传播过程</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer2-pool1'</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里使用的是最大池化层</span></span><br><span class="line">        <span class="comment"># 池化层过滤器的尺寸为2，使用全0填充并且步长为2</span></span><br><span class="line">        pool1 = tf.nn.max_pool(relu1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 声明第三层卷积层的变量并实现前向传播过程，与第一层相似</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer3-conv2'</span>):</span><br><span class="line">        conv2_weights = tf.get_variable(<span class="string">"weight"</span>, [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        conv2_biases = tf.get_variable(<span class="string">"bias"</span>, [CONV2_DEEP], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line">        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实现第四层池化层的前向传播过程，与第二层相似</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer4-pool2'</span>):</span><br><span class="line">        pool2 = tf.nn.max_pool(relu2, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将第四层池化层的输出转化为第五层全连接层的输入格式</span></span><br><span class="line">    pool_shape = pool2.get_shape().as_list()</span><br><span class="line">    <span class="comment"># 计算出向量的长度</span></span><br><span class="line">    nodes = pool_shape[<span class="number">1</span>] * pool_shape[<span class="number">2</span>] * pool_shape[<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># -1代表的是此位置的参数根据实际情况给出，并不直接提供定值</span></span><br><span class="line">    reshaped = tf.reshape(pool2, [<span class="number">-1</span>, nodes])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 声明第五层全连接层的变量并实现前向传播过程</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer5-fc1'</span>):</span><br><span class="line">        fc1_weights = tf.get_variable(<span class="string">"weight"</span>, [nodes, FC_SIZE], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        <span class="keyword">if</span> regularizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            tf.add_to_collection(<span class="string">"losses"</span>, regularizer(fc1_weights))</span><br><span class="line">        fc1_biases = tf.get_variable(<span class="string">"bias"</span>, [FC_SIZE], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 声明第六层全连接层的变量并实现前向传播过程</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer6-fc2'</span>):</span><br><span class="line">        fc2_weights = tf.get_variable(<span class="string">"weight"</span>, [FC_SIZE, NUM_LABELS], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        <span class="keyword">if</span> regularizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            tf.add_to_collection(<span class="string">'losses'</span>, regularizer(fc2_weights))</span><br><span class="line">        fc2_biases = tf.get_variable(<span class="string">"bias"</span>, [NUM_LABELS], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">        logit = tf.matmul(fc1, fc2_weights) + fc2_biases</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(mnist)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 首先定义输入数据，使用了placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS], <span class="string">'x-input'</span>)</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, OUTPUT_NODE], <span class="string">'y-input'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个储存训练轮数的变量，用于滑动平均模型和指数衰减的学习率</span></span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成一个L2正则化损失函数</span></span><br><span class="line">    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用inference函数计算前向传播的结果</span></span><br><span class="line">    y = inference(x, regularizer)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算交叉熵</span></span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 对整个batch的交叉熵求平均</span></span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将loss集合中所有元素相加，得到所有的正则化损失</span></span><br><span class="line">    regularization = tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将交叉熵和正则化损失求和，得到模型的总损失</span></span><br><span class="line">    loss = cross_entropy_mean + regularization</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置指数衰减的学习率</span></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,                     <span class="comment"># 基础的学习率，随着迭代的进行，在这个学习率的基础上递减</span></span><br><span class="line">        global_step,                            <span class="comment"># 之前定义的用于储存训练轮数的变量</span></span><br><span class="line">        mnist.train.num_examples/BATCH_NODE,    <span class="comment"># 衰减速度</span></span><br><span class="line">        LEARNING_RATE_DECAY)                    <span class="comment"># 衰减系数，若衰减速度为100，衰减系数为.96，则说明每过100轮学习率变成之前的.96</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用梯度下降算法定义优化过程</span></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检验对于batch中的每个数据，预测结果是否等于标记值</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 计算模型在这一组数据中的正确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 声明saver用于持久化训练模型</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开启一个会话，开始计算过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对所有变量进行初始化</span></span><br><span class="line">            tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 准备验证数据，一般可以在神经网络的训练过程中通过验证数据来大致判断停止的条件和评判训练的效果</span></span><br><span class="line">            validate_xs, validate_ys = mnist.validation.next_batch(BATCH_NODE)</span><br><span class="line">            reshaped_validate_xs = np.reshape(validate_xs, [BATCH_NODE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS])</span><br><span class="line">            validate_feed = &#123;</span><br><span class="line">                x: reshaped_validate_xs,</span><br><span class="line">                y_: validate_ys</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 准备测试数据，作为模型训练结束之后的最终评价标准</span></span><br><span class="line">            test_xs = mnist.test.images</span><br><span class="line">            reshaped_test_xs = np.reshape(test_xs, [mnist.test.num_examples, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS])</span><br><span class="line">            test_feed = &#123;</span><br><span class="line">                x: reshaped_test_xs,</span><br><span class="line">                y_: mnist.test.labels</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 迭代地训练神经网络</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 每过1000轮使用验证数据评价模型并对模型进行持久化</span></span><br><span class="line">                <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                    validate_acc = sess.run(accuracy, feed_dict=validate_feed)</span><br><span class="line">                    <span class="comment"># 使用saver对模型持久化</span></span><br><span class="line">                    saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)</span><br><span class="line">                    print(<span class="string">"After %d training step(s), validation accuracy using average model is %g"</span> % (i, validate_acc))</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果3000轮正确率还没到90%，就重新生成随机数，重新开始</span></span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">3000</span> <span class="keyword">and</span> float(validate_acc) &lt;= <span class="number">0.9</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 每轮都提取一个batch的数据，训练神经网络</span></span><br><span class="line">                xs, ys = mnist.train.next_batch(BATCH_NODE)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 将输入的xs调整为四维矩阵，以训练卷积神经网络</span></span><br><span class="line">                reshaped_xs = np.reshape(xs, [BATCH_NODE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS])</span><br><span class="line"></span><br><span class="line">                sess.run(train_step, feed_dict=&#123;x: reshaped_xs, y_: ys&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在训练结束之后，在测试数据上检测神经网络模型的最终正确率</span></span><br><span class="line">            test_acc = sess.run(accuracy, feed_dict=test_feed)</span><br><span class="line">            print(<span class="string">"After %d training step(s), test accuracy using average model is %g"</span> % (i, test_acc))</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    flag = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> flag:</span><br><span class="line">        flag = <span class="keyword">not</span> calculate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序入口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args=None)</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"/PycharmProjects/TFDemo/data/MNIST"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow提供的一个主程序入口，tf.app.run()会调用上面定义的main函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
</li>
<li><p>经典的用于图片分类问题的卷积网络架构</p>
<ul>
<li>公式：输入层 -&gt; (卷积层+ -&gt; 池化层? )+ -&gt; 全连接层+</li>
<li>解释<ul>
<li>“卷积层+”<ul>
<li>代表一层或多层卷积层</li>
<li>大部分卷积神经网络一般最多连续使用三层卷积层</li>
</ul>
</li>
<li>“池化层？”<ul>
<li>代表没有或者一层池化层</li>
<li>池化层虽然可以起到减少参数防止过拟合的作用，但是在部分论文中也发现可以直接通过调整卷积层步长来完成</li>
<li>所以有些卷积神经网络没有池化层</li>
</ul>
</li>
<li>卷积神经网络在输出之前一般会经过1-2个全连接层</li>
</ul>
</li>
<li>举例：LeNet-5模型结构<ul>
<li>输入层 -&gt; 卷积层 -&gt; 池化层 -&gt; 卷积层 -&gt; 池化层 -&gt; 全连接层 -&gt;全连接层 -&gt; 输出层</li>
</ul>
</li>
</ul>
</li>
<li><p>VGGNet</p>
<ul>
<li>简介：Very Deep Convolutional Networks for Large-Scale Image Recognization</li>
<li>尝试过的不同神经网络结构<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/VGGNet.jpg" alt=""></li>
</ul>
</li>
<li><p>每一层卷积层中参数的配置</p>
<ul>
<li>尺寸<ul>
<li>VGGNet中过滤器尺寸一般为3或1</li>
<li>LeNet-5中使用尺寸为5的过滤器</li>
<li>一般卷积层过滤器边长不会超过5</li>
</ul>
</li>
<li>深度<ul>
<li>在过滤器的深度上，大部分卷积神经网络都采用逐层递增的方式</li>
<li>在VGGNet中一般每经过一次池化层，卷积层过滤器的深度就乘2</li>
</ul>
</li>
<li>步长<ul>
<li>卷积层的步长一般为1，但有些模型也会使用2或者3</li>
</ul>
</li>
</ul>
</li>
<li><p>每一层池化层中参数的配置</p>
<ul>
<li>一般采用最大池化层</li>
<li>过滤器的尺寸一般为2或者3</li>
<li>步长也一般为2或者3</li>
</ul>
</li>
</ul>
<hr>
<h4 id="6-4-2-Inception-v3-模型"><a href="#6-4-2-Inception-v3-模型" class="headerlink" title="6.4.2 Inception-v3 模型"></a>6.4.2 Inception-v3 模型</h4><ul>
<li><p>Inception-v3简介</p>
<ul>
<li>Inception-v3模型中的Inception结构是将不同的卷积层通过并联的方式结合在一起</li>
<li>卷积层可以使用边长为1、3、5的过滤器，Inception同时使用所有不同尺寸的过滤器，然后再将矩阵拼接起来</li>
</ul>
</li>
<li><p>Inception模块的思想</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/Inception模块示意图.jpg" alt=""></li>
<li>Inception模块会首先使用不同尺寸的过滤器处理输入矩阵</li>
<li>不同的矩阵代表了Inception模块中的一条计算路径</li>
<li>虽然过滤器的大小不同，但如果所有的过滤器都使用全0填充且步长为1，那么前向传播得到的结果矩阵的长和宽与输入矩阵一致</li>
<li>这样经过不同过滤器处理的结果矩阵可以拼接为一个更深的矩阵</li>
</ul>
</li>
<li><p>Inception-v3模型的架构</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-6-图像识别与卷积神经网络/Inception-v3模型架构图.jpg" alt=""></li>
<li>Inception-v3模型总共有46层，由11个Inception模块组成</li>
<li>在Inception-v3模型中有大概96个卷积层</li>
</ul>
</li>
<li><p>TensorFlow-Slim工具</p>
<ul>
<li>可以用来更加简洁地实现卷积神经网络</li>
<li>代码对比  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接使用TensorFlow原始API实现卷积层</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'scope-name'</span>):</span><br><span class="line">    weights = tf.get_variable(<span class="string">'weights'</span>, [<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>, <span class="number">5</span>], initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">    biases = tf.get_variable(<span class="string">'biases'</span>, [<span class="number">5</span>], initializer=tf.constant_initializer(<span class="number">0.0</span> ))</span><br><span class="line">    <span class="comment"># 这里用input代表上一层的输入，并不是实际存在的变量</span></span><br><span class="line">    conv = tf.nn.conv2d(input, weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line">    relu = tf.nn.relu(tf.nn.bias_add(conv, biases))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用slim实现卷积层</span></span><br><span class="line"><span class="comment"># slim.conv2d函数有3个必填参数</span></span><br><span class="line"><span class="comment"># 第一个参数为输入矩阵</span></span><br><span class="line"><span class="comment"># 第二个参数为当前卷积层过滤器的深度</span></span><br><span class="line"><span class="comment"># 第三个参数为过滤器的尺寸</span></span><br><span class="line"><span class="comment"># 可选参数有过滤器移动的步长，是否使用全0填充，激活函数的选择，变量的命名空间等</span></span><br><span class="line">net = slim.conv2d(input, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>使用Slim工具实现Inception-v3中某个模块的前向传播</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br><span class="line"></span><br><span class="line"><span class="comment"># slim.arg_scope函数可以用于设置默认的参数取值</span></span><br><span class="line"><span class="comment"># 第一个参数是一个函数列表</span></span><br><span class="line"><span class="comment"># 后面的参数都是可选参数，代表默认的参数取值</span></span><br><span class="line"><span class="comment"># 在函数列表中的所有函数将使用默认的参数取值</span></span><br><span class="line"><span class="comment"># 例如：在调用slim.conv2d(net, 320, [1,1])函数时会自动加上stride=1和padding='SAME'的参数</span></span><br><span class="line"><span class="comment"># 如果在函数调用时指定了stride，那么默认的stride不会被调用</span></span><br><span class="line"><span class="comment"># 这进一步减少了代码冗余</span></span><br><span class="line"><span class="keyword">with</span> slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里省略了Inception-v3其他模块的代码，并假设net代表上一次的输出结果</span></span><br><span class="line">    net = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>], <span class="string">'x-input'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为一个Inception模块声明一个统一的变量命名空间</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'Mixed_7c'</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为Inception模块中每一条路径声明一个命名空间</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">            branch_0 = slim.conv2d(net, <span class="number">320</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'Conv2d_0a_1x1'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inception模块的第二条路径，这条计算路径上的结构本身也是一个Inception结构</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">            branch_1 = slim.conv2d(net, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'Conv2d_0a_1x1'</span>)</span><br><span class="line">            branch_1 = tf.concat(<span class="number">3</span>, [</span><br><span class="line">                slim.conv2d(branch_1, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">3</span>], scope=<span class="string">'Conv2d_0b_1x3'</span>),</span><br><span class="line">                slim.conv2d(branch_1, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">1</span>], scope=<span class="string">'Conv2d_0c_3x1'</span>)</span><br><span class="line">            ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inception模块的第三条路径，这条计算路径上的结构本身也是一个Inception结构</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">            branch_2 = slim.conv2d(net, <span class="number">448</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'Conv2d_0a_1x1'</span>)</span><br><span class="line">            branch_2 = slim.conv2d(branch_2, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'Conv2d_0b_3x3'</span>)</span><br><span class="line">            branch_2 = tf.concat(<span class="number">3</span>, [</span><br><span class="line">                slim.conv2d(branch_2, <span class="number">384</span>, [<span class="number">1</span>, <span class="number">3</span>], scope=<span class="string">'Conv2d_0c_1x3'</span>),</span><br><span class="line">                slim.conv2d(branch_2, <span class="number">384</span>, [<span class="number">3</span>, <span class="number">1</span>], scope=<span class="string">'Conv2d_0d_3x1'</span>)</span><br><span class="line">            ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inception模块的第四条路径，这条计算路径上的结构本身也是一个Inception结构</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">            branch_3 = slim.avg_pool2d(net, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'AvgPool_0a_3x3'</span>)</span><br><span class="line">            branch_3 = slim.conv2d(branch_3, <span class="number">192</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'Conv2d_0b_1x1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当前Inception模块的最后输出是由上面4个计算结果拼接得到的</span></span><br><span class="line">    net = tf.concat(<span class="number">3</span>, [branch_0, branch_1, branch_2, branch_3])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h3 id="6-5-卷积神经网络迁移学习"><a href="#6-5-卷积神经网络迁移学习" class="headerlink" title="6.5 卷积神经网络迁移学习"></a>6.5 卷积神经网络迁移学习</h3><h4 id="6-5-1-迁移学习介绍"><a href="#6-5-1-迁移学习介绍" class="headerlink" title="6.5.1 迁移学习介绍"></a>6.5.1 迁移学习介绍</h4><ul>
<li><p>背景</p>
<ul>
<li>随着模型层数及复杂度的增加，模型在ImageNet上的错误率也随之降低</li>
<li>然而训练复杂的卷积神经网络需要非常多的标注数据</li>
<li>并且训练一个复杂的神经网络需要几天甚至几周的时间</li>
</ul>
</li>
<li><p>简介</p>
<ul>
<li>所谓迁移学习，就是将一个问题上训练好的模型通过简单的调整使其适用于一个新的问题</li>
</ul>
</li>
<li><p>瓶颈层</p>
<ul>
<li>迁移到新问题时，可以保留训练好的Inception-v3模型中所有卷积层的参数，只是替换最后一层全连接层，在<strong>最后这一层全连接层之前</strong>的网络层称为<strong>瓶颈层</strong></li>
</ul>
</li>
<li><p>思想</p>
<ul>
<li>将新的图像通过训练好的卷积神经网络直到瓶颈层的过程可以看作是对图像进行特征提取的过程</li>
<li>在训练好的Inception-v3模型中，因为将瓶颈层的输出再通过一个单层的全连接层神经网络可以很好地区分1000种类别的图像，所以有理由认为瓶颈层输出的节点向量可以被作为任意图像的要给更加精简且表达能力更强的特征向量</li>
<li>在新数据集上，可以先用这个训练好的神经网络对图像进行特征提取，然后再将提取得到的特征向量作为输入来训练一个新的单层全连接神经网络处理新的分类问题</li>
</ul>
</li>
<li><p>优劣</p>
<ul>
<li>一般来说，如果数据量足够，迁移学习的效果不如完全重新训练</li>
<li>但是迁移学习所需的训练时间和训练样本数要远远小于训练完整的模型</li>
</ul>
</li>
</ul>
<hr>
<h4 id="6-5-2-TensorFlow实现迁移学习"><a href="#6-5-2-TensorFlow实现迁移学习" class="headerlink" title="6.5.2 TensorFlow实现迁移学习"></a>6.5.2 TensorFlow实现迁移学习</h4>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/10/Tensorflow-5-MNIST数字识别问题/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/Tensorflow-5-MNIST数字识别问题/" itemprop="url">[Tensorflow][5]MNIST数字识别问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-10T14:47:02+08:00">
                2019-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/10/Tensorflow-5-MNIST数字识别问题/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/10/Tensorflow-5-MNIST数字识别问题/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第五章-MNIST数字识别问题"><a href="#第五章-MNIST数字识别问题" class="headerlink" title="第五章 MNIST数字识别问题"></a>第五章 MNIST数字识别问题</h2><h3 id="5-1-MNIST数据处理"><a href="#5-1-MNIST数据处理" class="headerlink" title="5.1 MNIST数据处理"></a>5.1 MNIST数据处理</h3><ul>
<li><p>MNIST数据集简介</p>
<ul>
<li><p>MNIST数据集是NIST数据集的一个子集，是一个手写体数字识别数据集</p>
</li>
<li><p>它包含60000张图片作为训练数据，10000张图片作为测试数据</p>
</li>
<li><p>在MNIST数据集中的每一张图片都代表了0~9中的一个数字，图片的大小都是28*28，且数字都出现在图片的正中间</p>
</li>
<li><p>如图所示，一张数字图片及和它对应的像素矩阵<br><img src="/2019/03/10/Tensorflow-5-MNIST数字识别问题/数字图片及其像素矩阵.jpg" alt=""></p>
</li>
<li><p>为了验证模型训练的效果，一般还需要从训练数据中划分出一部分数据作为验证数据</p>
</li>
</ul>
</li>
<li><p>TensorFlow中的封装（基本过时）</p>
<ul>
<li><p>样例程序</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入MNIST数据集，如果指定地址/path/to/MNIST_data下没有已经下载好的数据</span></span><br><span class="line"><span class="comment"># 那么TensorFlow会自动下载数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"/path/to/MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印训练集的数据量</span></span><br><span class="line">print(<span class="string">"Training data size:"</span>, mnist.train.num_examples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印验证集的数据量</span></span><br><span class="line">print(<span class="string">"Validating data size:"</span>, mnist.validation.num_examples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印测试集的数据量</span></span><br><span class="line">print(<span class="string">"Testing data size:"</span>, mnist.test.num_examples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印训练集中某一样例数据</span></span><br><span class="line"><span class="comment"># 一张数字图片，它的每个像素点都被放到这个长度为128一维数组中</span></span><br><span class="line"><span class="comment"># 如果一个像素点越接近于1，则颜色越深；越接近于0，则颜色越浅</span></span><br><span class="line">print(<span class="string">"Example training data:"</span>, mnist.train.images[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印训练集中某一样例数据的标记</span></span><br><span class="line"><span class="comment"># 一个大小为10的一维数组</span></span><br><span class="line"><span class="comment"># 数组中其中一个数字取值为1,其余数字取值为0</span></span><br><span class="line">print(<span class="string">"Example training data label:"</span>, mnist.train.labels[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置batch的大小</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用next_batch()方法来获得下一个batch的输入数据</span></span><br><span class="line">xs, ys = mnist.train.next_batch(batch_size)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"X shape:"</span>, xs.shape)</span><br><span class="line">print(<span class="string">"Y shape"</span>, ys.shape)</span><br></pre></td></tr></table></figure>
</li>
<li><p>解读</p>
<ol>
<li><p>MNIST数据集被划分为train、validation和test三个数据集</p>
</li>
<li><p>处理后的每一张图片都是一个长度为784的一维数组，因为神经网络的输入是一个特征向量，所以在此把一张二维图像的像素矩阵放到一个一维数组中可以方便TensorFlow将图片的像素矩阵提供给神经网络的输入层</p>
</li>
<li><p>像素矩阵中元素的取值范围为[0,1]，它代表了颜色的深浅</p>
</li>
<li><p>使用mnist.train.next_batch()可以从所有的训练数据中读取一小部分作为一个batch</p>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-2-神经网络模型训练及不同模型结果对比"><a href="#5-2-神经网络模型训练及不同模型结果对比" class="headerlink" title="5.2 神经网络模型训练及不同模型结果对比"></a>5.2 神经网络模型训练及不同模型结果对比</h3><hr>
<h4 id="5-2-1-TensorFlow训练神经网络"><a href="#5-2-1-TensorFlow训练神经网络" class="headerlink" title="5.2.1 TensorFlow训练神经网络"></a>5.2.1 TensorFlow训练神经网络</h4><ul>
<li><p>回顾主要概念</p>
<ul>
<li>在神经网络的结构上，深度学习一方面需要使用激活函数实现神经网络模型的去线性化，另一方面需要使用一个或多个隐藏层使得神经网络的结构更深，以解决复杂问题</li>
<li>使用带指数衰减的学习率设置、使用正则化来避免过度拟合，以及使用滑动平均模型可以使得神经网络模型更加健壮</li>
</ul>
</li>
<li><p>直接给出<a href="https://github.com/T0UGH/TFDemo/blob/master/5.2.1MNISTSolution.py" target="_blank" rel="noopener">代码</a></p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># MNIST数据集相关参数</span></span><br><span class="line">INPUT_NODE = <span class="number">784</span>                <span class="comment"># 输入层的节点数，等于图片的像素</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span>                <span class="comment"># 输出层的节点数，等于类别的数目。0-9这10个数字，所以是10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置神经网络的参数</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span>               <span class="comment"># 隐藏层节点数，这里只有一个隐藏层，此隐藏层有500个节点</span></span><br><span class="line">BATCH_NODE = <span class="number">100</span>                <span class="comment"># batch的大小</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.8</span>        <span class="comment"># 基础的学习率</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span>      <span class="comment"># 学习率的衰减率</span></span><br><span class="line">REGULARIZATION_RATE = <span class="number">0.0001</span>    <span class="comment"># 正则化项在损失函数中的系数</span></span><br><span class="line">TRAINING_STEPS = <span class="number">30000</span>          <span class="comment"># 训练轮数</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span>     <span class="comment"># 滑动平均衰减率</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 给定神经网络的输入和所有参数，计算神经网络的前向传播结果</span></span><br><span class="line"><span class="comment"># 在这里定义了一个使用ReLU激活函数的三层全连接网络</span></span><br><span class="line"><span class="comment"># 通过一层隐藏层实现了多层网络结构</span></span><br><span class="line"><span class="comment"># 通过ReLU激活函数实现了去线性化</span></span><br><span class="line"><span class="comment"># 并且提供了对滑动平均模型的支持，此时需要传入avg_class</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, avg_class, weights1, biases1, weights2, biases2)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> avg_class <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, weights2) + biases2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型的整个过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(mnist)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 首先定义输入数据，使用了placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, INPUT_NODE], <span class="string">'x-input'</span>)</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, OUTPUT_NODE], <span class="string">'y-input'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义并初始化隐藏层的权重和偏置值</span></span><br><span class="line">    weight1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">    biases1 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[LAYER1_NODE]))</span><br><span class="line">    <span class="comment"># 定义并初始化输出层的权重和偏置值</span></span><br><span class="line">    weight2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">    biases2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[OUTPUT_NODE]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用inference函数计算前向传播的结果(不带滑动平均的)</span></span><br><span class="line">    y = inference(x, <span class="keyword">None</span>, weight1, biases1, weight2, biases2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个储存训练轮数的变量，用于滑动平均模型和指数衰减的学习率</span></span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类</span></span><br><span class="line">    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在所有代表神经网络参数的变量上使用滑动平均，即所有tf.trainable_variables集合中的变量</span></span><br><span class="line">    variable_averages_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算使用了滑动平均之后的前向传播结果</span></span><br><span class="line">    average_y = inference(x, variable_averages, weight1, biases1, weight2, biases2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算交叉熵</span></span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 对整个batch的交叉熵求平均</span></span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成一个L2正则化损失函数</span></span><br><span class="line">    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line">    <span class="comment"># 计算模型的正则化损失</span></span><br><span class="line">    regularization = regularizer(weight1) + regularizer(weight2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将交叉熵和正则化损失求和，得到模型的总损失</span></span><br><span class="line">    loss = cross_entropy_mean + regularization</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置指数衰减的学习率</span></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,                     <span class="comment"># 基础的学习率，随着迭代的进行，在这个学习率的基础上递减</span></span><br><span class="line">        global_step,                            <span class="comment"># 之前定义的用于储存训练轮数的变量</span></span><br><span class="line">        mnist.train.num_examples/BATCH_NODE,    <span class="comment"># 衰减速度</span></span><br><span class="line">        LEARNING_RATE_DECAY)                    <span class="comment"># 衰减系数，若衰减速度为100，衰减系数为.96，则说明每过100轮学习率变成之前的.96</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用梯度下降算法定义优化过程</span></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为使用了滑动平均模型，所以在训练神经网络时，每过一遍数据既需要通过反向传播来更新神经网络中的参数，又需要</span></span><br><span class="line">    <span class="comment"># 更新每个参数的滑动平均值。为了一次完成多个操作，可以使用tf.group()来将两个计算合并起来执行</span></span><br><span class="line">    train_op = tf.group(train_step, variable_averages_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检验对于batch中的每个数据，预测结果是否等于标记值</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(average_y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 计算模型在这一组数据中的正确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开启一个会话，开始计算过程</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对所有变量进行初始化</span></span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备验证数据，一般可以在神经网络的训练过程中通过验证数据来大致判断停止的条件和评判训练的效果</span></span><br><span class="line">        validate_feed = &#123;</span><br><span class="line">            x: mnist.validation.images,</span><br><span class="line">            y_: mnist.validation.labels</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备测试数据，作为模型训练结束之后的最终评价标准</span></span><br><span class="line">        test_feed = &#123;</span><br><span class="line">            x: mnist.test.images,</span><br><span class="line">            y_: mnist.test.labels</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 迭代地训练神经网络</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每过1000轮使用验证数据评价模型</span></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                validate_acc = sess.run(accuracy, feed_dict=validate_feed)</span><br><span class="line">                print(<span class="string">"After %d training step(s), validation accuracy using average model is %g"</span> % (i, validate_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每轮都提取一个batch的数据，训练神经网络</span></span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_NODE)</span><br><span class="line">            sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在训练结束之后，在测试数据上检测神经网络模型的最终正确率</span></span><br><span class="line">        test_acc = sess.run(accuracy, feed_dict=test_feed)</span><br><span class="line">        print(<span class="string">"After %d training step(s), test accuracy using average model is %g"</span> % (i, test_acc))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序入口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"/tmp/data"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow提供的一个主程序入口，tf.app.run()会调用上面定义的main函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
</li>
<li><p>输出</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">After 0 training step(s), validation accuracy using average model is 0.0778</span><br><span class="line">After 1000 training step(s), validation accuracy using average model is 0.9758</span><br><span class="line">After 2000 training step(s), validation accuracy using average model is 0.9802</span><br><span class="line">After 3000 training step(s), validation accuracy using average model is 0.9818</span><br><span class="line">After 4000 training step(s), validation accuracy using average model is 0.983</span><br><span class="line">After 5000 training step(s), validation accuracy using average model is 0.9826</span><br><span class="line">After 6000 training step(s), validation accuracy using average model is 0.9828</span><br><span class="line">After 7000 training step(s), validation accuracy using average model is 0.9838</span><br><span class="line">After 8000 training step(s), validation accuracy using average model is 0.985</span><br><span class="line">After 9000 training step(s), validation accuracy using average model is 0.9836</span><br><span class="line">After 10000 training step(s), validation accuracy using average model is 0.9852</span><br><span class="line">After 11000 training step(s), validation accuracy using average model is 0.9836</span><br><span class="line">After 12000 training step(s), validation accuracy using average model is 0.9846</span><br><span class="line">After 13000 training step(s), validation accuracy using average model is 0.9852</span><br><span class="line">After 14000 training step(s), validation accuracy using average model is 0.9846</span><br><span class="line">After 15000 training step(s), validation accuracy using average model is 0.9842</span><br><span class="line">After 16000 training step(s), validation accuracy using average model is 0.9844</span><br><span class="line">After 17000 training step(s), validation accuracy using average model is 0.9846</span><br><span class="line">After 18000 training step(s), validation accuracy using average model is 0.9848</span><br><span class="line">After 19000 training step(s), validation accuracy using average model is 0.9844</span><br><span class="line">After 20000 training step(s), validation accuracy using average model is 0.9848</span><br><span class="line">After 21000 training step(s), validation accuracy using average model is 0.9844</span><br><span class="line">After 22000 training step(s), validation accuracy using average model is 0.9842</span><br><span class="line">After 23000 training step(s), validation accuracy using average model is 0.9852</span><br><span class="line">After 24000 training step(s), validation accuracy using average model is 0.9852</span><br><span class="line">After 25000 training step(s), validation accuracy using average model is 0.9854</span><br><span class="line">After 26000 training step(s), validation accuracy using average model is 0.9852</span><br><span class="line">After 27000 training step(s), validation accuracy using average model is 0.9854</span><br><span class="line">After 28000 training step(s), validation accuracy using average model is 0.9852</span><br><span class="line">After 29000 training step(s), validation accuracy using average model is 0.9856</span><br><span class="line">After 29999 training step(s), test accuracy using average model is 0.9837</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h4 id="5-2-2-使用验证数据集判断模型效果"><a href="#5-2-2-使用验证数据集判断模型效果" class="headerlink" title="5.2.2 使用验证数据集判断模型效果"></a>5.2.2 使用验证数据集判断模型效果</h4><ul>
<li><p>超参数</p>
<ul>
<li>简介：一般无法通过学习获得，由程序员根据经验手动调整</li>
<li>例如：初始学习率、学习率衰减率、隐藏层节点数、迭代轮数等等</li>
</ul>
</li>
<li><p>如何设置超参数的取值</p>
<ul>
<li>配置这些超参数都是需要通过实验去调整的</li>
</ul>
</li>
<li><p>使用测试数据来选取超参数会导致神经网络模型过度拟合测试数据，从而导致失去对未知数据的预测能力</p>
</li>
<li><p>为了评估神经网络模型在不同超参数下的效果，一般会从训练数据中抽取一部分作为验证数据，使用验证数据来验证模型效果</p>
</li>
<li><p>交叉验证</p>
<ul>
<li>另一种验证模型效果的方式</li>
<li>适用于训练数据不够的情况，但整体速度偏慢</li>
</ul>
</li>
<li><p>在MNIST问题上，完全可以通过模型在验证数据上的表现来判断一个模型的优劣<br><img src="/2019/03/10/Tensorflow-5-MNIST数字识别问题/测试与验证对比.jpg" alt=""></p>
</li>
<li><p>一般来说选取的验证数据分布越接近测试数据分布，模型在验证数据上的表现越可以体现模型在测试数据上的表现</p>
</li>
</ul>
<hr>
<h4 id="5-2-3-不同模型效果比较"><a href="#5-2-3-不同模型效果比较" class="headerlink" title="5.2.3 不同模型效果比较"></a>5.2.3 不同模型效果比较</h4><ul>
<li><p>五种优化方法：</p>
<ul>
<li>在神经网络的设计上，需要使用<strong>激活函数</strong>和<strong>多层隐藏层</strong>。</li>
<li>在神经网络优化时，可以使用<strong>指数衰减的学习率</strong>、<strong>加入正则化的损失函数</strong>以及<strong>滑动平均模型</strong></li>
</ul>
</li>
<li><p>在超参数相同的情况下，使用不同优化方法，经过30000训练迭代后，得到的最终模型的正确率<br><img src="/2019/03/10/Tensorflow-5-MNIST数字识别问题/不同模型的正确率.jpg" alt=""></p>
<ul>
<li>神经网络的结构对最终模型的效果有本质性的影响</li>
<li>使用滑动平均、指数衰减和正则化带来的影响并不特别明显</li>
</ul>
</li>
<li><p>由于MNIST问题本身相对简单，所以滑动平均、指数衰减和正则化带来的提升效果并不特别明显</p>
</li>
<li><p>正则化和不使用正则化的损失函数的变化趋势<br><img src="/2019/03/10/Tensorflow-5-MNIST数字识别问题/不同模型的损失函数的变化趋势.jpg" alt=""></p>
</li>
</ul>
<hr>
<h3 id="5-3-变量管理"><a href="#5-3-变量管理" class="headerlink" title="5.3 变量管理"></a>5.3 变量管理</h3><hr>
<ul>
<li><p>简介</p>
<ul>
<li><p>TensorFlow提供了通过变量名称来创建或者获得一个变量的机制</p>
</li>
<li><p>通过这个机制，在不同的函数中可以直接通过变量的名字来使用变量，而不需要将变量通过参数的形式到处传递</p>
</li>
<li><p>TensorFlow中通过变量名称获得变量的机制主要通过<code>tf.get_variable()</code>和<code>tf.variable_scope()</code>来实现</p>
</li>
</ul>
</li>
<li><p>当<code>tf.get_variable()</code>用来创建变量时，它和tf.Variable()的功能是基本等价的</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面两个定义是等价的</span></span><br><span class="line">v = tf.get_variable(<span class="string">"v"</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line">v = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>], name=<span class="string">"v"</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于<code>tf.get_variable()</code>，变量名称是一个必填的参数</p>
<ul>
<li><code>tf.get_variable()</code>会根据这个名字去创建或者获取变量</li>
<li><code>tf.get_variable()</code>首先试图去创建一个名字为v的参数，如果创建失败，这个程序会报错，这是为了避免无意识的变量复用造成的问题</li>
</ul>
</li>
<li><p>如果需要通过<code>tf.get_variable()</code>获得一个已经创建的变量，需要通过<code>tf.variable_scope()</code>函数来生成一个上下文管理器，并明确指定在这个上下文管理器中，<code>tf.get_variable()</code>将直接获得已经生成的变量</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在名字为foo的命名空间内创建名字为v的变量</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</span><br><span class="line">    v = tf.get_variable(</span><br><span class="line">        <span class="string">"v"</span>, [<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为在命名空间foo中已经存在名字为v的变量，所以以下代码将会报错</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在生成上下文管理器时，将参数reuse设置为True。这样tf.get_variable()函数将直接获得已经声明的变量</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line">    print(v == v1)      <span class="comment"># 输出为True，代表v,v1表示的是相同的TensorFlow中变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将参数reuse设置为True时，tf.variable_scope将只能获得已经创建过的变量，因为在命名空间bar中还没有创建变量v，所以下面会报错</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    v3 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>当<code>tf.variable_scope()</code>函数使用参数reuse=True生成上下文管理器时，这个上下文管理器内所有的<code>tf.get_variable()</code>函数会直接获得已经创建的变量,如果变量不存在就会报错</li>
<li>当<code>tf.variable_scope()</code>函数不使用参数reuse=True生成上下文管理器时，<code>tf.get_variable()</code>会创建新的变量，如果同名的变量已经存在,则<code>tf.get_variable()</code>函数会报错</li>
</ul>
</li>
<li><p><code>tf.variable_scope()</code>不仅生成上下文管理器，也会生成命名空间，在命名空间中创建的所有变量名称都会带上这个命名空间名的前缀</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">v1 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line">print(v1.name)          <span class="comment"># 输出v:0, "v"为变量名称, ":0"表示此变量是生成这个变量的运算的第一个输出</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</span><br><span class="line">    v2 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line">    print(v2.name)      <span class="comment"># 输出foo/v:0, 因为在名字空间中，所以变量名称前会加入名字空间的名称</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</span><br><span class="line">        v3 = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line">        print(v3.name)  <span class="comment"># 输出foo/bar/v:0,</span></span><br><span class="line">    v4 = tf.get_variable(<span class="string">"v1"</span>, [<span class="number">1</span>])</span><br><span class="line">    print(v4.name)      <span class="comment"># 输出foo/v1:0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个名称为空的名字空间，并设置reuse=True</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">""</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以直接通过带名字空间名称的变量名来获取其他名字空间下的变量</span></span><br><span class="line">    <span class="comment"># 比如这里直接通过指定"foo/bar/v"，来获得名字空间foo/bar下的v变量</span></span><br><span class="line">    v5 = tf.get_variable(<span class="string">"foo/bar/v"</span>, [<span class="number">1</span>])</span><br><span class="line">    print(v5 == v3)     <span class="comment"># 输出True</span></span><br><span class="line"></span><br><span class="line">    v6 = tf.get_variable(<span class="string">"foo/v"</span>, [<span class="number">1</span>])</span><br><span class="line">    print(v6 == v2)     <span class="comment"># 输出True</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h3 id="5-4-TensorFlow模型持久化"><a href="#5-4-TensorFlow模型持久化" class="headerlink" title="5.4 TensorFlow模型持久化"></a>5.4 TensorFlow模型持久化</h3><ul>
<li>为了让训练结果可以复用，需要将训练得到的神经网络模型持久化</li>
</ul>
<hr>
<h4 id="5-4-1-持久化代码实现"><a href="#5-4-1-持久化代码实现" class="headerlink" title="5.4.1 持久化代码实现"></a>5.4.1 持久化代码实现</h4><ul>
<li><p>简介：TensorFlow提供了<code>tf.train.Saver</code>类来保存和还原一个神经网络模型</p>
</li>
<li><p>保存TensorFlow计算图的代码样例</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明两个变量并计算它们的和</span></span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明tf.train.Saver类用来保存模型</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    </span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将模型保存到D:PycharmProjects/TFDemo/data/model/540/model.ckpt</span></span><br><span class="line">    saver.save(sess, <span class="string">"/PycharmProjects/TFDemo/data/model/540/model.ckpt"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>TensorFlow模型的文件组织</p>
<ul>
<li>TensorFlow模型一般会存在后缀为<code>.ckpt</code>的文件中</li>
<li>在这个文件目录下会出现三个文件<ul>
<li><code>model.ckpt.meta</code>：保存TensorFlow计算图的结构</li>
<li><code>model.ckpt</code>：保存TensorFlow程序中每一个变量的取值</li>
<li><code>checkpoint</code>：保存了一个目录下所有的模型文件列表</li>
</ul>
</li>
<li>如图<br><img src="/2019/03/10/Tensorflow-5-MNIST数字识别问题/文件组织.jpg" alt=""></li>
</ul>
</li>
<li><p>加载已经保存的TensorFlow模型的代码样例</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用和保存模型代码中一样的方式来声明变量</span></span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 通过saver.restore加载v1和v2的具体取值，不需要初始化过程</span></span><br><span class="line">    saver.restore(sess, <span class="string">"/PycharmProjects/TFDemo/data/model</span></span><br><span class="line"><span class="string">    /540/model.ckpt"</span>)</span><br><span class="line">    print(sess.run(result))</span><br></pre></td></tr></table></figure>
<ul>
<li>此处并没有运行变量的初始化过程，而是将变量的值通过已经保存的模型加载进来</li>
</ul>
</li>
<li><p>直接加载已经持久化的图的代码样例（为了不重复定义图上的运算）</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接加载持久化的图</span></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">"/PycharmProjects/TFDemo/data/model/540/model.ckpt.meta"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过名字获得result变量</span></span><br><span class="line">result = tf.get_default_graph().get_tensor_by_name(<span class="string">"add:0"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过saver.restore加载v1和v2的具体取值，不需要初始化过程</span></span><br><span class="line">    saver.restore(sess, <span class="string">"/PycharmProjects/TFDemo/data/model/540/model.ckpt"</span>)</span><br><span class="line">    <span class="comment"># 输出[3.]</span></span><br><span class="line">    print(sess.run(result))</span><br></pre></td></tr></table></figure>
</li>
<li><p>如何在保存和加载时对变量进行重命名</p>
<ul>
<li>在声明 <code>tf.train.Saver</code> 类时可以提供一个列表来指定需要保存或者加载的变量，比如 <code>saver = tf.train.Saver([v1])</code></li>
<li>TensorFlow通过字典将模型保存时的变量名和需要加载的变量联系起来<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里声明的变量名称与已经保存的模型中的变量的名称不同</span></span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"other-v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"other-v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若直接使用tf.train.Saver()加载模型会报变量找不到的错误</span></span><br><span class="line"><span class="comment"># 使用一个字典来重命名变量可以加载原来的模型</span></span><br><span class="line">saver = tf.train.Saver(&#123;<span class="string">"v1"</span>: v1, <span class="string">"v2"</span>: v2&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"/PycharmProjects/TFDemo/data/model/540/model.ckpt"</span>)</span><br><span class="line">    print(sess.run(result))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>使用Saver保存和加载滑动平均</p>
<ul>
<li>每个变量的滑动平均值是通过影子变量维护的，所以要获得变量的滑动平均值实际上就是获得这个影子变量的取值。</li>
<li>如果在加载模型时直接将影子变量映射到变量自身，那么在使用训练好的模型时就不需要再调用函数来获得变量的滑动平均值了</li>
</ul>
</li>
<li><p>将计算图中的变量及其取值通过常量的形式保存</p>
<ul>
<li>背景：使用<code>Saver</code>会保存运行TensorFlow程序所需要的全部信息，但是有时并不需要某些信息。比如，在离线预测时，只需要知道如何从神经网络的输入层经过前向传播计算得到输出层即可，而不需要类似于变量初始化、模型保存等辅助节点的信息</li>
<li><p>解决：TensorFlow提供了<code>convert_variables_to_constants()</code>将计算图中的变量及其取值通过常量的方式保存</p>
<ul>
<li><p>导出为常量</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> graph_util</span><br><span class="line"></span><br><span class="line">v1 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.constant(<span class="number">2.0</span>, shape=[<span class="number">1</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">result = v1 + v2</span><br><span class="line"></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment"># 导出当前计算图的GraphDef部分，只需要这一部分就可以完成从输入层到输出层的计算过程</span></span><br><span class="line">    graph_def = tf.get_default_graph().as_graph_def()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图中的变量及其取值转化为常量，同时将图中不必要的节点去掉</span></span><br><span class="line">    output_graph_def = graph_util.convert_variables_to_constants(sess, graph_def, [<span class="string">'add'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将导出的模型存入文件</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(<span class="string">"/PycharmProjects/TFDemo/data/model/547/combined_model.pb"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(output_graph_def.SerializeToString())</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入常量</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    model_filename = <span class="string">"/PycharmProjects/TFDemo/data/model/547/combined_model.pb"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取保存的模型文件，将文件解析为对应的GraphDef Protocal Buffer</span></span><br><span class="line">    <span class="keyword">with</span> gfile.FastGFile(model_filename, <span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(file.read())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将graph_def中保存的图加载到当前的图中。</span></span><br><span class="line">    <span class="comment"># return_elements给出了返回的张量的名称。在保存时给出的是计算节点的名称，所以为"add"，在加载时给出的是张量的名称，所以为add:0</span></span><br><span class="line">    result = tf.import_graph_def(graph_def, return_elements=[<span class="string">"add:0"</span>])</span><br><span class="line">    <span class="comment"># 输出[3.0]　　</span></span><br><span class="line">    print(sess.run(result))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="5-4-2-持久化原理及数据格式"><a href="#5-4-2-持久化原理及数据格式" class="headerlink" title="5.4.2 持久化原理及数据格式"></a>5.4.2 持久化原理及数据格式</h4><ul>
<li><p>当调用<code>Saver.save()</code>时，会生成4个文件。TensorFlow持久化通过这四个文件实现</p>
</li>
<li><p>MetaGraphDef(model.ckpt.meta)</p>
<ul>
<li>TensorFlow通过元图(MetaGraph)来记录计算图中节点的信息以及一些运行所需的元数据</li>
<li>具体信息<ul>
<li>meta_info_def：记录计算图中的元数据和TensorFlow中使用的所有运算的信息</li>
<li>graph_def：记录TensorFlow计算图上的节点信息</li>
<li>saver_def：记录持久化模型时所需的一些参数信息</li>
<li>collection_def：在TensorFlow计算图中可以维护不同集合，而维护这些集合的底层实现是通过collection_def实现的</li>
</ul>
</li>
</ul>
</li>
<li><p>model.ckpt.data和model.ckpt.index</p>
<ul>
<li>用于持久化TensorFlow中所有变量的取值</li>
<li>其中model.ckpt.data是通过SSTable格式存储的，可以理解为一个键值对列表</li>
</ul>
</li>
<li><p>checkpoint</p>
<ul>
<li>维护由Saver类的某个实例持久化的所有TensorFlow模型文件的文件名</li>
<li>用Saver自动生成且自动维护</li>
</ul>
</li>
</ul>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/10/Tensorflow-4-深层神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/Tensorflow-4-深层神经网络/" itemprop="url">[Tensorflow][4]深层神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-10T14:43:21+08:00">
                2019-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/10/Tensorflow-4-深层神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/10/Tensorflow-4-深层神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第四章-深层神经网络"><a href="#第四章-深层神经网络" class="headerlink" title="第四章 深层神经网络"></a>第四章 深层神经网络</h2><hr>
<h3 id="4-1-深度学习与深层网络"><a href="#4-1-深度学习与深层网络" class="headerlink" title="4.1 深度学习与深层网络"></a>4.1 深度学习与深层网络</h3><ul>
<li><p>深度学习的精确定义：一类通过多层非线性变换对高复杂性数据建模算法的合集</p>
</li>
<li><p>深层神经网络</p>
<ul>
<li>深层神经网络是实现“多层非线性变换”最常用的一种方法</li>
<li>实际中深度学习基本上就是深层神经网络的代名词</li>
</ul>
</li>
<li><p>深度学习的两个特性</p>
<ol>
<li>多层</li>
<li>非线性</li>
</ol>
</li>
</ul>
<hr>
<h4 id="4-1-1-线性模型的局限性"><a href="#4-1-1-线性模型的局限性" class="headerlink" title="4.1.1 线性模型的局限性"></a>4.1.1 线性模型的局限性</h4><ul>
<li><p>线性模型定义：假设一个模型的输入y和输入xi满足以下关系，那么这个模型就是一个线性模型<br>  <img src="/2019/03/10/Tensorflow-4-深层神经网络/线性模型公式.jpg" alt=""></p>
</li>
<li><p>线性模型特点：线性模型的最大特点是任意线性模型的组合仍然是线性模型</p>
</li>
<li><p>为什么被称为线性模型？</p>
<ul>
<li>当模型的输入只有一个的时候，x和y形成了二维坐标系上的一条直线</li>
<li>当模型有n个输入时，x和y形成了n+1维空间中的一个平面</li>
</ul>
</li>
<li><p>线性模型局限性：</p>
<ul>
<li>只通过线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别，而且它们都是线性模型</li>
<li>线性模型只能解决线性可分问题，但是绝大多数问题都是无法线性分割的</li>
</ul>
</li>
<li><p>样例</p>
<ul>
<li><p>使用线性模型无法解决线性不可分问题<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/线性模型解决线性不可分问题.jpg" alt=""></p>
</li>
<li><p>使用线性模型可以解决线性可分问题<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/线性模型解决线性可分问题.jpg" alt=""></p>
</li>
<li><p>使用非线性模可以解决线性不可分问题<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/非线性模型解决线性不可分问题.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="4-1-2-激活函数实现去线性化"><a href="#4-1-2-激活函数实现去线性化" class="headerlink" title="4.1.2 激活函数实现去线性化"></a>4.1.2 激活函数实现去线性化</h4><ul>
<li><p>激活函数简介</p>
<ul>
<li>如果将每个神经元的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性了</li>
<li>这个非线性函数就是激活函数</li>
</ul>
</li>
<li><p>加入了激活函数和偏置项之后的<strong>神经元</strong>结构<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/加入偏置项和激活函数的神经元结构.jpg" alt=""></p>
</li>
<li><p>常见的非线性激活函数<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/常见的激活函数.jpg" alt=""><br>这些激活函数的函数图像都不是一条直线，所以通过这些激活函数，每个节点不再是线性变换，于是整个神经网络模型也就不再是线性的</p>
</li>
<li><p>加入了激活函数和偏置项之后的<strong>神经网络</strong><br><img src="/2019/03/10/Tensorflow-4-深层神经网络/加入偏置项和激活函数的神经网络.jpg" alt=""><br>此处偏置项可以表达为一个永远输出为1的节点</p>
</li>
<li><p>TensorFlow中的激活函数</p>
<ul>
<li>TensorFlow目前提供7种非线性激活函数</li>
<li>比较常见的有<ul>
<li><code>tf.nn.relu</code></li>
<li><code>tf.sigmoid</code></li>
<li><code>tf.tanh</code></li>
</ul>
</li>
<li>使用激活函数  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.nn.relu(tf.matmul(x, w1) + biases1)</span><br><span class="line">y = tf.nn.relu(tf.matmul(a, w2) + biases2)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="4-1-3-多层网络解决异或运算"><a href="#4-1-3-多层网络解决异或运算" class="headerlink" title="4.1.3 多层网络解决异或运算"></a>4.1.3 多层网络解决异或运算</h4><ul>
<li><p>异或运算：如果两个输入的符号相同时则输出为0，否则输出为1</p>
</li>
<li><p>感知机：可以理解为单层的神经网络<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/加入偏置项和激活函数的神经元结构.jpg" alt=""></p>
</li>
<li><p>感知机的缺陷：感知机无法模拟异或运算<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/感知机无法解决异或问题.jpg" alt=""></p>
</li>
<li><p>当加入隐藏层以后，异或问题可以得到很好的解决</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/加入隐藏层解决异或问题.jpg" alt=""></li>
<li>另外<ul>
<li>隐藏层的4个节点中，每个节点都有一个角是黑色的</li>
<li>这4个隐藏节点可以被认为代表了从输入特征中抽取的更高维的特征</li>
<li>深层神经网络实际上有组合特征提取的功能</li>
<li>这个特性对于解决不易提取特征向量的问题有很大帮助</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-2-损失函数定义"><a href="#4-2-损失函数定义" class="headerlink" title="4.2 损失函数定义"></a>4.2 损失函数定义</h3><ul>
<li>损失函数简介：神经网络模型的效果以及优化的目标是通过损失函数来定义的</li>
</ul>
<hr>
<h4 id="4-2-1-经典损失函数"><a href="#4-2-1-经典损失函数" class="headerlink" title="4.2.1 经典损失函数"></a>4.2.1 经典损失函数</h4><ul>
<li><p>分类问题</p>
<ul>
<li>简介：分类问题希望解决的是将不同的样本分到事先定义好的类别中</li>
<li>例子<ul>
<li>判断一个零件是否合格的问题就是一个二分类问题</li>
<li>手写体数字识别问题可以归纳为一个十分类问题</li>
</ul>
</li>
<li>如何通过神经网络解决分类问题<ul>
<li>通过神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数</li>
<li>在理想情况下，如果一个样本属于类别k，那么这个类别所对应的输出节点的输出值应该为1，而其他节点的输出都为0</li>
<li>例如：[1,0,0,0,0,0,0,0,0]</li>
</ul>
</li>
<li>如何判断一个输出向量和期望的向量有多接近？<ul>
<li>交叉熵(cross entropy)是常用的评判方法之一</li>
</ul>
</li>
</ul>
</li>
<li><p>交叉熵</p>
<ul>
<li>用途：交叉熵用来判断一个输出向量与期望的向量有多接近</li>
<li>公式<ul>
<li>给定两个概率分布p和q，通过q来表示p的交叉熵为<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/交叉熵公式.jpg" alt=""></li>
</ul>
</li>
<li>简介<ul>
<li>交叉熵刻画的是两个概率分布之间的距离</li>
<li>也可以理解为，交叉熵刻画的是通过概率分布q来表达概率分布p的困难程度</li>
<li>交叉熵越小，两个概率分布越接近</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>但是神经网络的输出却不一定是一个概率分布，如何将神经网络前向传播得到的结果也变成概率分布呢？</p>
<ul>
<li><p>如图<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/将神经网络输出转化为概率分布.jpg" alt=""></p>
</li>
<li><p>使用Softmax函数作为一层额外的处理层，把神经网络的输出变成一个概率分布</p>
</li>
<li><p>假设原始的神经网络输出为y1,y2,…yn，那么经过Softmax回归处理之后的输出为：<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/Softmax函数.jpg" alt=""></p>
</li>
<li><p>原始神经网络的输出被用作置信值来生成新的输出，而新的输出满足概率分布的所有要求</p>
</li>
<li><p>这个新的输出可以理解为经过神经网络的推导，一个样例为不同类别的概率分别是多大。</p>
</li>
<li><p>这样就把神经网络的输出也变成了一个概率分布，从而可以通过交叉熵来计算预测的概率分布与真实答案的概率分布之间的距离了</p>
</li>
</ul>
</li>
<li><p>使用TensorFlow实现交叉熵</p>
<ul>
<li><p>样例代码</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = - tf.reduce_mean(y_ * </span><br><span class="line">tf.log(tf.clip_by_value(y,<span class="number">1e-10</span>,<span class="number">1.0</span>)))</span><br></pre></td></tr></table></figure>
</li>
<li><p>解读</p>
<ul>
<li><code>y_</code>：代表正确结果，来自于输入</li>
<li><code>y</code>：代表预测结果，来自于神经网络的运算结果</li>
<li><code>tf.clip_by_value()</code>：把一个张量中的数值限制在一个范围之内</li>
<li><code>tf.log()</code>：对张量中所有元素依次求对数</li>
<li><code>*</code>：不是矩阵乘法，而是张量之间的每个元素相乘，矩阵乘法要用<code>tf.matmul()</code></li>
<li><code>tf.reduce_mean()</code>：对张量中所有元素求平均，并输出一个数字作为平均值</li>
</ul>
</li>
<li>另外<ul>
<li>可以直接使用<code>tf.nn.softmax_cross_entropy_with_logits()</code>函数实现上述过程<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">    labels=y_, logits=y)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>回归问题</p>
<ul>
<li>简介<ul>
<li>回归问题解决的是对具体数值的预测</li>
<li>这些问题需要预测的不是一个事先定义好的类别，而是一个任意实数</li>
<li>解决回归问题的神经网络一般只有一个输出节点，这个节点的输出值就是预测值</li>
</ul>
</li>
<li>回归问题常用损失函数—–均方误差(MSE,mean squared error)<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/均方误差公式.jpg" alt=""></li>
</ul>
</li>
</ul>
<hr>
<h4 id="4-2-2-自定义损失函数"><a href="#4-2-2-自定义损失函数" class="headerlink" title="4.2.2 自定义损失函数"></a>4.2.2 自定义损失函数</h4><ul>
<li><p>TensorFlow支持自定义损失函数，使得神经网络优化的结果更加接近实际问题的需求</p>
</li>
<li><p>不同的损失函数会对训练得到的模型产生重大影响</p>
</li>
<li><p><code>tf.greater()</code>：输入两个向量，比较两个输入向量中每个元素的大小，并返回比较结果</p>
</li>
<li><p><code>tf.where()</code>：当第一参数为True时，选择第二个参数的值，否则选择第三个参数的值</p>
</li>
</ul>
<hr>
<h3 id="4-3-神经网络优化算法"><a href="#4-3-神经网络优化算法" class="headerlink" title="4.3 神经网络优化算法"></a>4.3 神经网络优化算法</h3><hr>
<ul>
<li><p>梯度下降算法(gradient decent)</p>
<ul>
<li>主要用于优化单个参数的取值</li>
<li>目前还没有一个通用的方法可以对任意损失函数直接求解<strong>最佳的参数</strong>取值</li>
<li>梯度下降算法会迭代式更新参数，不断沿着梯度的反方向让参数朝着总损失更小的方向更新</li>
</ul>
</li>
<li><p>反向传播算法(backpropagation)</p>
<ul>
<li>反向传播算法给出了一个高效的方式在所有参数上使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小</li>
<li>换句话说，反向传播算法就是根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值</li>
</ul>
</li>
<li><p>参数的梯度：可以通过求偏导的方式计算</p>
</li>
<li><p>学习率(learning rate)：用来定义每次参数更新的幅度</p>
</li>
<li><p>神经网络的优化过程可以分为两个阶段</p>
<ol>
<li>通过前向传播算法计算得到的预测值，并将预测值和真实值做对比得出两者之间的差距(损失函数)</li>
<li>通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数</li>
</ol>
</li>
<li><p>需要注意</p>
<ul>
<li>梯度下降算法并不能保证被优化的函数达到全局最优解<ul>
<li>当函数的偏导为0，参数就不再更新了，因此梯度下降算法只能达到局部最优解</li>
</ul>
</li>
<li>在海量训练数据下，要计算所有训练数据的损失函数是非常消耗时间的<ul>
<li>使用随机梯度下降算法加速训练过程：这个算法优化的不是在全部训练数据上的损失函数，而是在每一轮迭代，随机优化某一条训练数据上的损失函数</li>
<li>但是在某一条数据上损失函数更小并不代表在全部数据上损失函数更小，甚至无法达到局部最优</li>
</ul>
</li>
</ul>
</li>
<li><p>为了综合梯度下降算法和随机梯度下降算法的优缺点</p>
<ul>
<li>每次计算一小部分训练数据的损失函数，这一小部分数据被称为一个batch</li>
<li>通过矩阵运算，每次在一个batch上优化神经网络的参数并不会比单个数据慢太多</li>
<li>每次使用一个batch可以大大减少收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-4-神经网络进一步优化"><a href="#4-4-神经网络进一步优化" class="headerlink" title="4.4 神经网络进一步优化"></a>4.4 神经网络进一步优化</h3><hr>
<h4 id="4-4-1-学习率的设置"><a href="#4-4-1-学习率的设置" class="headerlink" title="4.4.1 学习率的设置"></a>4.4.1 学习率的设置</h4><ul>
<li><p>学习率：学习率决定参数每次更新的幅度</p>
</li>
<li><p>背景</p>
<ul>
<li>如果学习率过大，可能会导致参数在极优值的两侧来回移动</li>
<li>如果学习率过小，虽然能保证收敛性，但是会大大降低优化速度</li>
</ul>
</li>
<li><p>指数衰减法</p>
<ul>
<li>简介：可以在训练的前期使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练的后期表现较为稳定</li>
<li>公式：<code>decayed_learning_rate = learning_rate * decay_rate ** (global_step / decay_steps)</code><ul>
<li><code>decayed_learning_rate</code>：每一轮优化时使用的学习率</li>
<li><code>learning_rate</code>：事先设置的初始学习率</li>
<li><code>decay_rate</code>：衰减系数</li>
<li><code>global_steps</code>：当前的迭代轮数</li>
<li><code>decay_steps</code>：衰减速度</li>
<li>例如：若 <code>decay_rate = 0.96</code>，并且 <code>decay_steps = 100</code>；则说明每训练100轮，学习率就是100轮之前的0.96倍</li>
</ul>
</li>
<li>TensorFlow的实现：<code>tf.train.expoential_decay()</code>  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明一个变量</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过exponential_decay函数生成学习率</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(<span class="number">0.1</span>, global_step, <span class="number">100</span>, <span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用指数衰减的学习率，首先在构造器中传入learning_rate,然后在minimize函数中传入global_step将自动更新global_step参数，从而使得学习率也得到相应更新</span></span><br><span class="line"></span><br><span class="line">learning_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_func, global_step=global_step)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>对 <code>tf.train.exponential</code>中<code>staircase</code> 参数的解释</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/指数衰减.jpg" alt=""></li>
<li>通过设置 <code>staircase</code> 参数可以选择不同的衰减方式</li>
<li>当 <code>staircase = False</code> 时<ul>
<li>学习率随迭代轮次变化的趋势如图中灰线所示</li>
</ul>
</li>
<li>当 <code>staircase = True</code> 时<ul>
<li>global_step / decay_steps会被转化为整数</li>
<li>如图中黑线所示</li>
<li>在这种设置下，<code>decay_steps</code> 通常代表了完整的使用一遍训练数据所需要的迭代轮数，这种设置的常用场景是每完整过完一遍训练数据，学习率就减小一次，这可以使得训练数据集中的所有数据对模型训练有相等的作用</li>
</ul>
</li>
</ul>
</li>
<li><p>一般来说初始学习率、衰减系数和衰减速度都是根据经验设置的</p>
</li>
</ul>
<hr>
<h4 id="4-4-2-过拟合问题"><a href="#4-4-2-过拟合问题" class="headerlink" title="4.4.2 过拟合问题"></a>4.4.2 过拟合问题</h4><ul>
<li><p>背景</p>
<ul>
<li>在真实的应用中想要的并不是让模型尽量模拟训练数据的行为，而是希望通过训练出来的模型对<strong>未知数据</strong>给出判断</li>
<li>模型在训练数据上的表现并不一定代表了它在<strong>未知数据</strong>上的表现</li>
</ul>
</li>
<li><p>过拟合问题</p>
<ul>
<li>简介：过拟合指的是当一个模型过于复杂之后，它可以很好地记忆每个训练数据中随机噪音的部分而“忘记”了要去学习训练数据中通用的趋势</li>
<li>问题：过度拟合训练数据中的随机噪音虽然可以得到非常小的损失函数，但是对于未知数据可能无法做出可靠的判断</li>
<li>例子<br>  <img src="/2019/03/10/Tensorflow-4-深层神经网络/神经网络模型的三种情况.jpg" alt=""><ol>
<li>第一种情况下，模型过于简单，无法刻画问题的趋势，也可以称之为欠拟合</li>
<li>第二个模型是合理的，它既不会过于关注训练数据中的噪音，又能比较好地刻画问题的整体趋势</li>
<li>第三个模型就是过拟合了，虽然它完美地划分了不同形状的点，但是这样的划分并不能很好地对未知数据做出判断，因为它<strong>过度拟合</strong>了训练数据中的噪音而忽视了问题的整体规律</li>
</ol>
</li>
</ul>
</li>
<li><p>正则化（regularization）</p>
<ul>
<li>背景：为了避免过拟合问题，可以使用正则化</li>
<li>思想：<ul>
<li>正则化的思想就是在损失函数中加入刻画模型复杂程度的指标：<code>R(w)</code></li>
<li>其中 <code>w</code> 表示每条边上的权重，也就是训练的参数</li>
<li>这样通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪音</li>
</ul>
</li>
</ul>
</li>
<li><p>常用的刻画模型复杂度的函数 <code>R(w)</code> 有两种</p>
<ul>
<li>L1正则化<ul>
<li>公式<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/L1正则化.jpg" alt=""></li>
<li>特点<ul>
<li>会使参数变得更稀疏，即有大量参数取值为0</li>
<li>L1正则化的计算公式不可导，因此优化更加复杂，优化方法更多</li>
</ul>
</li>
</ul>
</li>
<li>L2正则化<ul>
<li>公式<br><img src="/2019/03/10/Tensorflow-4-深层神经网络/L2正则化.jpg" alt=""></li>
<li>特点<ul>
<li>不会使参数变得稀疏</li>
<li>L2正则化的计算公式可导，因此优化要更加简洁</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>TensorFlow实现正则化</p>
<ul>
<li>TensorFlow提供了<code>tf.contrib.layers.l2_regularizer()</code>函数，它可以返回一个函数，这个函数可以计算一个给定参数的L2正则化项的值</li>
<li><p>样例</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line"><span class="comment"># 在损失函数中额外添加R(w)项，以正则化</span></span><br><span class="line"><span class="comment"># 其中lamb表示正则化的权重</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lamb)(w)</span><br></pre></td></tr></table></figure>
</li>
<li><p>样例2：使用集合来为多层复杂神经网络添加正则项</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成并返回一层神经网络边上的权重，并将这个权重的L2正则化损失加入名称为'losses'的集合中</span></span><br><span class="line"><span class="comment"># lamb代表权重的大小</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, lamb)</span>:</span></span><br><span class="line">    <span class="comment"># 生成一个变量</span></span><br><span class="line">    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将新生成变量的L2正则化损失项加入集合</span></span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(lamb)(var))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回这个变量</span></span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印所有可训练参数的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_weights</span><span class="params">(sess)</span>:</span></span><br><span class="line">    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)</span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> variables:</span><br><span class="line">        print()</span><br><span class="line">        print(var)</span><br><span class="line">        print(sess.run(var))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line">x = tf.placeholder(tf.float32, (<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, (<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义batch的大小</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义每层网络中神经元的个数</span></span><br><span class="line">layer_dimension = [<span class="number">2</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 神经网络的层数</span></span><br><span class="line">n_layers = len(layer_dimension)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前层的神经元，初始化为输入层神经元</span></span><br><span class="line">cur_layer = x</span><br><span class="line"><span class="comment"># 当前层的神经元个数</span></span><br><span class="line">in_dimension = layer_dimension[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过一个循环来生成5层全连接的神经网络</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">    <span class="comment"># 下一层的节点个数</span></span><br><span class="line">    out_dimension = layer_dimension[i]</span><br><span class="line">    <span class="comment"># 根据这一层和下一层的节点数来生成当前层的权重，并将L2正则添加到集合</span></span><br><span class="line">    weight = get_weight([in_dimension, out_dimension], <span class="number">0.001</span>)</span><br><span class="line">    bias = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[out_dimension]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用RuLU激活函数计算下一层的节点，并赋值给当前层变量</span></span><br><span class="line">    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)</span><br><span class="line">    <span class="comment"># 进入下一层之前将下一层的节点数更新为当前层的节点数</span></span><br><span class="line">    in_dimension = layer_dimension[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用均方误差作为损失函数</span></span><br><span class="line">mes_loss = tf.reduce_mean(tf.square(y_ - cur_layer))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将均方误差添加到losses集合</span></span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>, mes_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将loess集合中所有元素相加，得到最后的损失函数（包括MSE和正则）</span></span><br><span class="line">loss = tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用指数衰减来设置学习率</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(<span class="number">0.1</span>, global_step, <span class="number">100</span>, <span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化过程,带指数衰减的梯度下降算法</span></span><br><span class="line">learning_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Numpy生成随机数</span></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据集的大小</span></span><br><span class="line">dataset_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用前面的随机数来生成输入数据X和Y</span></span><br><span class="line">X = rdm.rand(dataset_size, <span class="number">2</span>)</span><br><span class="line">Y = [[int(x1+x2 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启会话开始运算</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 初始化所有变量</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印所有参数</span></span><br><span class="line">    print_weights(sess)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置训练的轮数</span></span><br><span class="line">    STEPS = <span class="number">5000</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开启迭代的训练过程</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置当前组batch的开始值和结束值</span></span><br><span class="line">        start = (i * batch_size) % dataset_size</span><br><span class="line">        end = min(start+batch_size, dataset_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练神经网络并更新参数</span></span><br><span class="line">        sess.run(learning_step, feed_dict=&#123;</span><br><span class="line">            x: X[start:end],</span><br><span class="line">            y_: Y[start:end]</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印所有参数</span></span><br><span class="line">    print_weights(sess)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="滑动平均模型"><a href="#滑动平均模型" class="headerlink" title="滑动平均模型"></a>滑动平均模型</h4><ul>
<li><p>背景：在采用随机梯度下降算法训练神经网络时，使用滑动平均模型在很多应用中都可以在一定程度上提高最终模型在测试数据上的表现</p>
</li>
<li><p>TensorFlow使用<code>tf.train.ExponentialMovingAverage()</code>来实现滑动平均模型</p>
</li>
<li><p>衰减率(decay)：用来控制模型更新的速度，decay越大模型就越趋于稳定</p>
</li>
<li><p>影子变量</p>
<ul>
<li>公式：<code>shadow_variable = decay * shadow_variable + (1 - decay) * variable</code><ul>
<li><code>shadow_variable</code>：影子变量</li>
<li><code>variable</code>：待更新的变量</li>
<li><code>decay</code>：衰减率</li>
</ul>
</li>
<li>影子变量的初始值为相应变量的初始值，而每次运行变量更新后，影子变量会按照上述公式来更新自己的值</li>
</ul>
</li>
<li><p>动态设置衰减率</p>
<ul>
<li>背景：为了使得模型在训练前期可以更新得更快</li>
<li>简介<ul>
<li>通过设置<code>num_updates</code>参数来动态设置<code>decay</code>的大小</li>
<li>此时衰减率为 <code>min(decay, ((1 + num_updates) / (10 + num_updates))</code></li>
</ul>
</li>
</ul>
</li>
<li><p>样例代码</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个变量用来计算滑动平均，这个变量的初始值为0</span></span><br><span class="line">v1 = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line"><span class="comment"># 定义step用来模拟神经网络迭代的轮数，可以用来动态控制衰减率</span></span><br><span class="line">step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个滑动平均的类，初始化时给定了衰减率(0.99)和控制衰减率的变量step</span></span><br><span class="line">ema = tf.train.ExponentialMovingAverage(<span class="number">0.99</span>, step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个更新变量滑动平均的操作，这里需要给定一个列表，每次执行这个操作时，这个列表中的变量都会被更新</span></span><br><span class="line">maintain_averages_op = ema.apply([v1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过ema.average(v1)来获得滑动平均之后变量的取值</span></span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新变量v1的值到5</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">5</span>))</span><br><span class="line">    <span class="comment"># 使用maintain_averages_op操作来更新v1的滑动平均值</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新变量step的值到10000</span></span><br><span class="line">    sess.run(tf.assign(step, <span class="number">10000</span>))</span><br><span class="line">    <span class="comment"># 更新变量v1的值到10</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">10</span>))</span><br><span class="line">    <span class="comment"># 使用maintain_averages_op操作来更新v1的滑动平均值</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 再次更新滑动平均值</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/10/Tensorflow-3-Tensorflow入门/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/Tensorflow-3-Tensorflow入门/" itemprop="url">[Tensorflow][3]Tensorflow入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-10T14:36:44+08:00">
                2019-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/10/Tensorflow-3-Tensorflow入门/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/10/Tensorflow-3-Tensorflow入门/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第三章-TensorFlow入门"><a href="#第三章-TensorFlow入门" class="headerlink" title="第三章 TensorFlow入门"></a>第三章 TensorFlow入门</h2><hr>
<h3 id="3-1-TensorFlow计算模型—计算图"><a href="#3-1-TensorFlow计算模型—计算图" class="headerlink" title="3.1 TensorFlow计算模型—计算图"></a>3.1 TensorFlow计算模型—计算图</h3><ul>
<li>计算图是TensorFlow中最基本的一个概念，TensorFlow中的所有计算都会被转化为计算图上的节点</li>
</ul>
<hr>
<h4 id="3-1-1-计算图的概念"><a href="#3-1-1-计算图的概念" class="headerlink" title="3.1.1 计算图的概念"></a>3.1.1 计算图的概念</h4><ul>
<li><p>张量(Tensor)：可以被简单地理解为多维数组</p>
</li>
<li><p>流(Flow)：直观地表达了张量之间通过计算相互转化的过程</p>
</li>
<li><p>计算图</p>
<ul>
<li>简介：TensorFlow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系</li>
<li>节点<ul>
<li>每个节点都是一个运算</li>
<li>为了建模方便，TensorFlow会将常量转化为一种永远输出固定值的常量</li>
</ul>
</li>
<li>边<ul>
<li>每条边表示了计算之间的依赖关系</li>
<li>如果一个运算的输入依赖于另一个运算的输出，那么这两个运算有依赖关系</li>
</ul>
</li>
<li>例子<br><img src="/2019/03/10/Tensorflow-3-Tensorflow入门/计算图样例.jpg" alt=""><br>a和b这两个常量不依赖于任何其他计算，而add计算则依赖读取两个常量的取值</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-1-2-计算图的使用"><a href="#3-1-2-计算图的使用" class="headerlink" title="3.1.2 计算图的使用"></a>3.1.2 计算图的使用</h4><ul>
<li><p>TensorFlow程序一般可以分为两个阶段</p>
<ul>
<li>计算定义阶段：定义计算图中所有的计算</li>
<li>计算执行阶段：详见3.3节</li>
</ul>
</li>
<li><p>计算定义阶段的样例</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#计算定义阶段开始</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>],name=<span class="string">"a"</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>,<span class="number">3.0</span>],name=<span class="string">"b"</span>)</span><br><span class="line">result=a+b</span><br><span class="line"><span class="comment">#计算定义阶段结束</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>其他</p>
<ul>
<li>在计算定义阶段，TensorFlow会自动将定义的计算转化为计算图上的节点</li>
<li>在TensorFlow中，系统会自动维护一个默认的计算图</li>
<li>通过<code>tf.get_default_graph()</code>可以获得当前默认的计算图</li>
</ul>
</li>
<li><p>计算图的两个功能</p>
<ul>
<li>隔离张量和变量    <ul>
<li>通过<code>tf.graph()</code>生成新的计算图</li>
<li>不同计算图上的张量和运算都不会共享</li>
</ul>
</li>
<li><p>提供了管理张量和计算的机制</p>
<ul>
<li><p>计算图可以通过<code>tf.Graph.device()</code>函数来指定运行计算的设备，这为使用GPU提供了机制</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g=tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    result = a + b</span><br></pre></td></tr></table></figure>
</li>
<li><p>在一个计算图中，可以通过集合(collection)来管理不同类别的资源</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-2-TensorFlow数据模型—张量"><a href="#3-2-TensorFlow数据模型—张量" class="headerlink" title="3.2 TensorFlow数据模型—张量"></a>3.2 TensorFlow数据模型—张量</h3><hr>
<ul>
<li>张量是TensorFlow管理数据的形式</li>
</ul>
<hr>
<h4 id="3-2-1-张量的概念"><a href="#3-2-1-张量的概念" class="headerlink" title="3.2.1 张量的概念"></a>3.2.1 张量的概念</h4><ul>
<li><p>在TensorFlow中所有的数据都通过张量的形式表示</p>
</li>
<li><p>张量可以被理解为多维数组</p>
<ul>
<li>零阶张量表示标量(scalar)，也就是一个数</li>
<li>第一阶张量为向量(vector)，也就是一个一维数组</li>
<li>第n阶张量为n维数组</li>
</ul>
</li>
<li><p>在张量中并没有真正保存数字，它保存的是如何得到这些数字的计算过程</p>
</li>
<li><p>一个张量中主要保存了三个属性</p>
<ul>
<li><p>例子</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>],name=<span class="string">"a"</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>,<span class="number">3.0</span>],name=<span class="string">"b"</span>)</span><br><span class="line">result = a + b</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#output</span></span><br><span class="line">Tensor(<span class="string">"add:0"</span>, shape=(<span class="number">2</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>
</li>
<li><p>名字(name)</p>
<ul>
<li>简介：一个张量的唯一标识符</li>
<li>来源：计算图中的每个节点代表了一个计算，计算的结果保存在张量中，所以张量与计算图上节点所代表的计算结果是对应的</li>
<li>命名：<code>node:src_output</code><br>  <code>node</code>：节点的名称<br>  <code>src_output</code>：表示当前张量是<code>node</code>的第几个输出</li>
</ul>
</li>
<li><p>维度(shape)</p>
<ul>
<li>简介：描述一个张量的维度信息</li>
<li>例如：<code>shape=(2,)</code>说明了张量是一个二维数组，这个数组的长度为2</li>
</ul>
</li>
<li><p>类型(type)</p>
<ul>
<li>简介：每个张量都有一个唯一的类型</li>
<li>类型包含：实数 ( <code>tf.float32、tf.float64</code> ) 、整数 ( <code>tf.int8、tf.int16、tf.int32、tf.int64、tf.uint8</code> )、布尔型 ( <code>tf.bool</code> ) 和复数 ( <code>tf.complex64、tf.complex128</code> )</li>
<li>报错：TensorFlow会对参加计算的所有张量进行类型的检查，当发现类型不匹配时会报错</li>
<li>默认类型：如果不指定类型，TensorFlow会给出默认的类型，比如不带小数点的数会被默认为<code>int32</code>，带小数点的数会默认为<code>float32</code></li>
<li>建议：因为使用默认类型可能会导致潜在的类型不匹配问题，所以一般建议通过指定<code>dtype</code>来明确指出变量或常量的类型</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-2-2-张量的使用"><a href="#3-2-2-张量的使用" class="headerlink" title="3.2.2 张量的使用"></a>3.2.2 张量的使用</h4><ol>
<li><p>对中间计算结果的引用</p>
<ul>
<li>当计算复杂度增加时，通过张量引用中间结果可以增加代码可读性</li>
<li>通过张量引用中间结果可以方便获得中间结果</li>
</ul>
</li>
<li><p>当计算图构建完成后用来获得计算结果</p>
<ul>
<li>虽然张量本身没有存储具体的数字，但通过session可以获得这些具体的数字</li>
</ul>
</li>
</ol>
<hr>
<h3 id="3-3-TensorFlow运行模型—会话"><a href="#3-3-TensorFlow运行模型—会话" class="headerlink" title="3.3 TensorFlow运行模型—会话"></a>3.3 TensorFlow运行模型—会话</h3><ul>
<li><p>简介</p>
<ul>
<li>会话(session)用来执行定义好的运算</li>
<li>会话拥有并管理TensorFlow程序运行时的所有资源</li>
<li>所有计算完成之后需要关闭会话来帮助系统回收资源，否则会资源泄漏</li>
</ul>
</li>
<li><p>使用会话的两种模式</p>
<ol>
<li><p>明确调用生成和关闭函数</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个会话</span></span><br><span class="line">sess=tf.Session()</span><br><span class="line"><span class="comment">#使用创建好的会话来得到运算结果</span></span><br><span class="line">sess.run(result)</span><br><span class="line"><span class="comment">#关闭这个会话</span></span><br><span class="line">sess.close()</span><br><span class="line"><span class="comment">#但是如果因抛出异常而退出，未执行close，则还是会出现资源泄漏</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>使用python上下文管理器<code>with</code>来自动管理</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个会话，并通过Python中的上下文管理器来管理这个会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#使用创建好的会话来计算关心的结果</span></span><br><span class="line">    sess.run(result)</span><br><span class="line"><span class="comment">#当上下文退出时自动关闭会话，释放资源</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>默认的会话</p>
<ul>
<li><p>可以设置一个会话为默认的会话，这样，调用一个张量的<code>eval()</code>方法，就相当于将计算放到这个默认的会话中运行</p>
<ul>
<li><p>例子</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess=tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    print(result.eval())</span><br></pre></td></tr></table></figure>
<p>  或者</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess=tf.InteractiveSession()</span><br><span class="line">print(result.eval())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>通过ConfigProto Protocol Buffer来配置需要生成的会话  </p>
<ul>
<li><p>例如</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">config=tf.ConfigProto(</span><br><span class="line">    allow_soft_placement=True,</span><br><span class="line">    log_device_placement=True</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过ConfigProto可以配置类似并行的线程数、GPU分配策略、运算超时时间等参数</p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-4-TensorFlow实现神经网络"><a href="#3-4-TensorFlow实现神经网络" class="headerlink" title="3.4 TensorFlow实现神经网络"></a>3.4 TensorFlow实现神经网络</h3><hr>
<h4 id="3-4-1-TensorFlow-Playground及神经网络简介"><a href="#3-4-1-TensorFlow-Playground及神经网络简介" class="headerlink" title="3.4.1 TensorFlow Playground及神经网络简介"></a>3.4.1 TensorFlow Playground及神经网络简介</h4><ul>
<li><p><a href="http://playground.tensorflow.org/" target="_blank" rel="noopener">TensorFlow Playground—可视化神经网络demo</a></p>
</li>
<li><p>Playground设置<br>  <img src="/2019/03/10/Tensorflow-3-Tensorflow入门/playground.jpg" alt=""></p>
</li>
<li><p>Playground迭代203轮的结果<br>  <img src="/2019/03/10/Tensorflow-3-Tensorflow入门/playgroundFin.jpg" alt=""></p>
</li>
<li><p>获得的启示</p>
<ul>
<li><p>在机器学习中，所有用于描述实体的数字的组合就是一个实体的<strong>特征向量(feature vector)</strong>，例如：可以用半径和质量来描述一个齿轮，gear –&gt; (r,m)</p>
</li>
<li><p>第一层是<strong>输入层</strong>，代表特征向量中每个特征的取值</p>
</li>
<li><p>同一层的节点不会相互连接，而且每一层只和下一层连接，直到最后一层作为<strong>输出层</strong>得到计算的结果</p>
</li>
<li><p>在输入和输出层之间的神经网络叫做<strong>隐藏层</strong>，一般一个神经网络的隐藏层越多，这个神经网络就越’深’</p>
</li>
<li><p>神经网络就是通过对参数的合理设置来解决分类或者回归问题的</p>
</li>
</ul>
</li>
<li><p>使用神经网络解决分类问题的4个步骤</p>
<ol>
<li><p>提取问题中的实体的特征向量作为神经网络的输入，不同实体可以提取不同的特征向量</p>
</li>
<li><p>定义神经网络的结构，并定义如何从神经网络的输入得到输出，这个过程就是神经网络的前向传播算法</p>
</li>
<li><p>通过训练数据来调整神经网络中参数的取值，这就是训练神经网络的过程</p>
</li>
<li><p>使用训练好的神经网络来预测未知的数据</p>
</li>
</ol>
</li>
</ul>
<hr>
<h4 id="3-4-2-前向传播算法简介"><a href="#3-4-2-前向传播算法简介" class="headerlink" title="3.4.2 前向传播算法简介"></a>3.4.2 前向传播算法简介</h4><ul>
<li><p>前向传播简介</p>
<ul>
<li>前向传播算法主要用来解决如何将输入的特征向量经过层层推导得到最后的输出的问题</li>
</ul>
</li>
<li><p>神经元</p>
<ul>
<li>例子<br><img src="/2019/03/10/Tensorflow-3-Tensorflow入门/神经元样例.jpg" alt=""></li>
<li>神经元是构成一个神经网络的最小单元</li>
<li>一个神经元有多个输入和一个输出</li>
<li>一个最简单神经元结构的输出就是所有输入的加权和，而不同输入的权重就是神经元的参数</li>
</ul>
</li>
<li><p>神经网络</p>
<ul>
<li>所谓神经网络的结构指的是不同神经元之间的连接结构</li>
<li>例子<br><img src="/2019/03/10/Tensorflow-3-Tensorflow入门/神经网络样例.jpg" alt=""></li>
</ul>
</li>
<li><p>全连接神经网络</p>
<ul>
<li>全连接神经网络就是相邻两层之间任意两个节点之间都有连接</li>
</ul>
</li>
<li><p>计算神经网络的<strong>前向传播</strong>结果需要<strong>三部分</strong>信息</p>
<ol>
<li>神经网络的输入<ul>
<li>这个输入是从实体中提取的特征向量</li>
</ul>
</li>
<li>神经网络的连接结构<ul>
<li>神经网络是由神经元构成的</li>
<li>神经网络的结构给出不同神经元之间输入输出的连接关系</li>
</ul>
</li>
<li>每个神经元中的参数</li>
</ol>
</li>
<li><p>前向传播算法样例<br><img src="/2019/03/10/Tensorflow-3-Tensorflow入门/前向传播样例.jpg" alt="">)</p>
</li>
<li><p>前向传播算法可以通过矩阵乘法的方式表达<br><img src="/2019/03/10/Tensorflow-3-Tensorflow入门/前向传播矩阵表示_0.jpg" alt=""><br><img src="/2019/03/10/Tensorflow-3-Tensorflow入门/前向传播矩阵表示_1.jpg" alt=""></p>
</li>
<li><p>TensorFlowFlow中的矩阵乘法</p>
<ul>
<li>使用<code>tf.matmul()</code>实现矩阵的乘法</li>
<li>例如  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.matmul(x,w1)</span><br><span class="line">b = tf.matmul(a,w2)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-4-3-神经网络参数与TensorFlow变量"><a href="#3-4-3-神经网络参数与TensorFlow变量" class="headerlink" title="3.4.3 神经网络参数与TensorFlow变量"></a>3.4.3 神经网络参数与TensorFlow变量</h4><ul>
<li><p>变量简介：变量 ( <code>tf.Variable</code> ) 的作用就是保存和更新神经网络中的参数</p>
</li>
<li><p>变量初始化</p>
<ul>
<li>TensorFlow中的变量需要指定初始值</li>
<li>一个变量的值在被使用之前，这个变量的初始化过程需要被明确调用</li>
<li><p>使用随机数初始化</p>
<ul>
<li><p>样例</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>说明</p>
<ul>
<li>首先调用了TensorFlow变量的声明函数<code>tf.Variable</code></li>
<li><code>tf.random_normal([2,3],stddev=2)</code> <ul>
<li>用来产生一个2*3的矩阵</li>
<li>矩阵中的元素是均值为0，标准差为2的随机数</li>
<li>可以通过参数<code>mean</code>来指定平均值，在没有指定时默认为0</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>使用常数初始化</p>
<ul>
<li><p>样例</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">biases = tf.Variable(tf.zeros([<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
</li>
<li><p>说明：将产生一个初始值为0，长度为3的变量</p>
</li>
</ul>
</li>
<li>使用其他变量的初始值初始化<ul>
<li>样例  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.Variable(weights.initialized_value())</span><br><span class="line">w2 = tf.Variable(weights.initialized_value() * <span class="number">2.0</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>通过变量实现神经网络的参数并实现前向传播</p>
<ul>
<li><p>样例</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明w1、w2两个变量，并通过seed参数设置了随机种子</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal((<span class="number">2</span>,<span class="number">3</span>),stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal((<span class="number">3</span>,<span class="number">1</span>),stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入的特征向量定义为一个常量，x是一个2*1的矩阵</span></span><br><span class="line">x = tf.constant([[<span class="number">0.7</span>,<span class="number">0.9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过矩阵乘法获得神经网络的输出</span></span><br><span class="line">a = tf.matmul(x,w1)</span><br><span class="line">y = tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># session的配置信息</span></span><br><span class="line">config=tf.ConfigProto(</span><br><span class="line">    allow_soft_placement=<span class="keyword">True</span>,</span><br><span class="line">    log_device_placement=<span class="keyword">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用session获得计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对w1、w2进行初始化的过程</span></span><br><span class="line">    sess.run(w1.initializer)</span><br><span class="line">    sess.run(w2.initializer)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到最终的运算结果</span></span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
</li>
<li><p>说明</p>
<ul>
<li>第一步：定义计算图中的所有计算，也就是定义w1、w2、a和y的过程</li>
<li>第二步：声明一个会话，并通过会话计算结果<ul>
<li>计算前需要初始化<ul>
<li>在计算最后的y时，需要将所有变量初始化</li>
<li>也就是说，虽然在变量定义时给出了变量初始化的方法，但这个方法并没有真正的运行</li>
</ul>
</li>
<li>使用函数<code>tf.global_variables_initializer()</code>直接初始化所有变量<ul>
<li>样例  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">sess.run(init_op)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>变量与张量</p>
<ul>
<li>简介：变量只是一种特殊的张量</li>
<li>说明：在TensorFlow中，变量的声明函数tf.Variable是一个运算，这个运算的输出结果就是一个张量，这个张量也就是变量，所以变量只是一种特殊的张量</li>
</ul>
</li>
<li><p>变量与集合</p>
<ul>
<li>所有变量都自动加入到<code>GraphKeys.VARIABLES</code>这个集合中</li>
<li>通过<code>tf.global_variables()</code>函数可以拿到当前计算图中的所有变量</li>
<li>通过变量声明函数中的<code>trainable</code>参数来区分需要优化的参数(如神经网络中的参数)和其他不需要优化参数(如迭代的轮数)</li>
<li>如果声明变量时参数<code>trainable</code>为<code>True</code>，则这个变量将会被加入到<code>GraphKeys.TRAINABLE_VARIABLES</code>集合中</li>
<li>神经网络优化算法会将<code>GraphKeys.TRAINABLE_VARIABLES</code>集合中的变量作为默认的优化对象</li>
</ul>
</li>
<li><p>变量的类型</p>
<ul>
<li>一个变量在构建之后，它的类型就不能再改变了</li>
</ul>
</li>
<li><p>变量的维度</p>
<ul>
<li>维度在程序运行中是有可能改变的</li>
<li>设置参数<code>validate_shape=False</code>就可以修改维度</li>
<li>修改维度在实践中比较罕见</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-4-4-通过TensorFlow训练神经网络模型"><a href="#3-4-4-通过TensorFlow训练神经网络模型" class="headerlink" title="3.4.4 通过TensorFlow训练神经网络模型"></a>3.4.4 通过TensorFlow训练神经网络模型</h4><ul>
<li><p>训练</p>
<ul>
<li>设置神经网络参数的过程就是神经网络的训练过程</li>
<li>只有经过有效训练的神经网络模型才可以真正地解决分类或者回归问题</li>
</ul>
</li>
<li><p>监督学习</p>
<ul>
<li>数据集：使用监督学习的方式设置神经网络参数需要有一个标注好的训练数据集</li>
<li>思想<ul>
<li>在已知答案(就是标注)的标注数据集上，模型给出的预测结果要尽量接近真实的答案</li>
<li>通过调整神经网络中的参数对训练数据进行拟合，可以使得模型对未知的样本提供预测的能力</li>
</ul>
</li>
</ul>
</li>
<li><p>反向传播算法(backpropagation algorithm)</p>
<ul>
<li>流程<br><img src="/2019/03/10/Tensorflow-3-Tensorflow入门/反向传播流程图.jpg" alt=""></li>
<li>简介：反向传播算法实现了一个迭代的过程</li>
<li>步骤<ol>
<li>在每次迭代的开始，首先需要选取一小部分训练数据，这一部分数据叫做一个<strong>batch</strong></li>
<li>然后，这个batch的样例会通过前向传播算法得到神经网络模型的预测结果</li>
<li>计算出当前神经网络模型的预测结果与正确答案之间的差距</li>
<li>基于预测值和真实值之间的差距，反向传播算法会相应更新神经网络参数的取值</li>
</ol>
</li>
</ul>
</li>
<li><p>placeholder机制</p>
<ul>
<li>背景：如果每轮迭代中选取的数据都要通过常量来表示，那么TensorFlow的计算图将会太大，而且利用率低，为了避免这个问题，提供了placeholder机制用于提供输入数据</li>
<li>简介<ul>
<li>placeholder相当于定义了一个位置，这个位置中的数据在程序运行时再指定</li>
<li>程序不需要生成大量常量来提供输入数据，只需要将数据通过placeholder传入TensorFlow计算图</li>
<li>在placeholder定义时，这个位置上的数据类型是需要指定的，而且类型不可更改</li>
</ul>
</li>
<li>样例代码  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w1 = tf.Variable(tf.random_normal((<span class="number">2</span>, <span class="number">3</span>), stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal((<span class="number">3</span>, <span class="number">1</span>), stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 此处使用tf.placeholder来声明一个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="number">3</span>, <span class="number">2</span>), name=<span class="string">"input"</span>)</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line">config=tf.ConfigProto(</span><br><span class="line">    allow_soft_placement=<span class="keyword">True</span>,</span><br><span class="line">    log_device_placement=<span class="keyword">True</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment"># 在程序运行时，才通过feed_dict给placeholder赋值</span></span><br><span class="line">    <span class="comment"># feed_dick是一个字典(map)，在字典中需要给出每个用到的placeholder的取值</span></span><br><span class="line">    print(sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.9</span>], [<span class="number">0.1</span>, <span class="number">0.4</span>], [<span class="number">0.5</span>, <span class="number">0.8</span>]]&#125;))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>损失函数</p>
<ul>
<li>在得到一个batch的前向传播结果后，需要定义一个损失函数来刻画当前的预测值和真实答案之间的差距</li>
<li>然后通过反向传播算法来调整神经网络参数的取值使得差距可以被缩小</li>
<li>交叉熵(cross entropy)是一个常用的损失函数</li>
</ul>
</li>
<li><p>反向传播的优化方法</p>
<ul>
<li>即通过调整参数使得损失函数刻画的差距被缩小</li>
<li>常用的优化方法有:<code>tf.train.GradientDescentOptimizer</code>、<code>tf.train.AdamOptimizer</code>和<code>tf.train.MomenttumOptimizer</code></li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-4-5-完整神经网络样例程序"><a href="#3-4-5-完整神经网络样例程序" class="headerlink" title="3.4.5 完整神经网络样例程序"></a>3.4.5 完整神经网络样例程序</h4><ul>
<li><p>样例程序</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据batch的大小</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的参数</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用placeholder来定义输入数据</span></span><br><span class="line"><span class="comment"># 其中将shape的一个维度设置为None可以方便使用不同的batch大小</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>), name=<span class="string">"x-input"</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">"y-input"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络前向传播的过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">y = tf.sigmoid(y)</span><br><span class="line">cross_entropy = -tf.reduce_mean(</span><br><span class="line">    y_ * tf.log(tf.clip_by_value(y, <span class="number">1e-10</span>, <span class="number">1.0</span>)) + (<span class="number">1</span>-y_) * tf.log(tf.clip_by_value(<span class="number">1</span>-y, <span class="number">1e-10</span>, <span class="number">1.0</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化过程</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Numpy生成随机数</span></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据集的大小</span></span><br><span class="line">dataset_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用前面的随机数来生成输入数据X和Y</span></span><br><span class="line">X = rdm.rand(dataset_size, <span class="number">2</span>)</span><br><span class="line">Y = [[int(x1+x2 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启会话开始运算</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 初始化所有变量</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练开始之前打印参数</span></span><br><span class="line">    print(sess.run(w1))</span><br><span class="line">    print(sess.run(w2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置训练的轮数</span></span><br><span class="line">    STEPS = <span class="number">5000</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开启迭代的训练过程</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置这一组batch的开始值和结束值</span></span><br><span class="line">        start = (i * batch_size) % dataset_size</span><br><span class="line">        end = min(start+batch_size, dataset_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练神经网络并更新参数</span></span><br><span class="line">        sess.run(train_step, feed_dict=&#123;</span><br><span class="line">            x: X[start:end],</span><br><span class="line">            y_: Y[start:end]</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每隔1000轮算一遍交叉熵，以观察训练结果</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            total_cross_entropy = sess.run(cross_entropy, feed_dict=&#123;x: X, y_: Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d training step(s), cross entropy on all data is %g"</span> % (i, total_cross_entropy))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练结束打印参数</span></span><br><span class="line">    print(sess.run(w1))</span><br><span class="line">    print(sess.run(w2))</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练神经网络的过程可以分为三个步骤</p>
<ol>
<li>定义神经网络的结构和前向传播的输出结果</li>
<li>定义损失函数以及选择反向传播优化的算法</li>
<li>生成会话(<code>tf.Session</code>)并且在训练数据上反复运行反向传播优化算法</li>
</ol>
<ul>
<li>无论神经网络的结构如何变化，这三个步骤是不变的</li>
</ul>
</li>
</ul>
<hr>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><hr>
<ul>
<li><p>计算图</p>
<ul>
<li>计算图是TensorFlow的计算模型</li>
<li>所有TensorFlow的程序都是通过计算图的形式表示</li>
<li>计算图的每一个节点都是一个运算</li>
<li>计算图上的边表示了运算之间的数据传递关系</li>
<li>计算图还保存了运行每个运算的设备信息以及运算之间的依赖关系</li>
<li>计算图提供了管理不同集合的功能，并且TensorFlow会自动维护5个不同的默认集合</li>
</ul>
</li>
<li><p>张量</p>
<ul>
<li>张量是TensorFlow的数据模型</li>
<li>TensorFlow中所有运算的输入、输出都是张量</li>
<li>张量本身并不存储任何数据，它只是对运算结果的引用</li>
<li>通过张量，可以更好地组织TensorFlow程序</li>
</ul>
</li>
<li><p>会话</p>
<ul>
<li>会话是TensorFlow的运算模型</li>
<li>它管理一个TensorFlow程序拥有的系统资源</li>
<li>所有的运算都是通过会话执行时</li>
</ul>
</li>
</ul>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/10/Tensorflow-2-Tensorflow环境搭建/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/Tensorflow-2-Tensorflow环境搭建/" itemprop="url">[Tensorflow][2]Tensorflow环境搭建</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-10T14:35:36+08:00">
                2019-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/10/Tensorflow-2-Tensorflow环境搭建/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/10/Tensorflow-2-Tensorflow环境搭建/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第2章-TensorFlow环境搭建"><a href="#第2章-TensorFlow环境搭建" class="headerlink" title="第2章 TensorFlow环境搭建"></a>第2章 TensorFlow环境搭建</h2><hr>
<h3 id="2-1-TensorFlow的主要依赖包"><a href="#2-1-TensorFlow的主要依赖包" class="headerlink" title="2.1 TensorFlow的主要依赖包"></a>2.1 TensorFlow的主要依赖包</h3><hr>
<h4 id="2-1-1-Protoval-Buffer"><a href="#2-1-1-Protoval-Buffer" class="headerlink" title="2.1.1 Protoval Buffer"></a>2.1.1 Protoval Buffer</h4><ul>
<li><p>Protocal Buffer是谷歌开发的处理结构化数据的工具</p>
</li>
<li><p>什么是结构化数据</p>
<ul>
<li><p>例如：用户信息包括用户的名字、ID和Email</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name:张三</span><br><span class="line">id:12345</span><br><span class="line">email:zhangsan@abc.com</span><br></pre></td></tr></table></figure>
<p>上面的用户信息就是一个结构化的数据</p>
</li>
</ul>
</li>
<li><p>结构化数据处理</p>
<ul>
<li>背景：要将这些结构化的用户信息持久化或进行网络传输时，就需要先将它们序列化。</li>
<li>序列化：所谓序列化，是将结构化的数据变成数据流的格式，简单地说就是变成一个字符串</li>
<li>定义：将结构化的数据序列化，并从序列化之后的数据流中还原出原来的结构化数据，统称为结构化数据处理</li>
</ul>
</li>
<li><p>常见的结构化数据处理工具</p>
<ul>
<li>XML</li>
<li>JSON</li>
<li>Protocal Buffer</li>
</ul>
</li>
<li><p>Protocal Buffer</p>
<ul>
<li><p>与XML和JSON格式的区别</p>
<ul>
<li>Protocal Buffer序列化之后得到的数据不是可读的字符串，而是二进制流。</li>
<li>使用Protocal Buffer时需要先定义数据的格式(schema)。还原一个序列化之后的数据将需要使用到这个定义好的数据格式<br>例如<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">message user&#123;</span><br><span class="line">    optional string name = 1;</span><br><span class="line">    required int32 id = 2;</span><br><span class="line">    repeated string email = 3;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>基本语法</p>
<ul>
<li>Protocal Buffer定义数据格式的文件一般保存在.proto文件中</li>
<li>每个message代表了一类结构化的数据</li>
<li>message里面定义了每个属性的类型和名字</li>
<li>属性的类型可以是像布尔型、整数型、实数型、字符型这样的基本类型，也可以使另外一个message</li>
<li>也需要定义一个属性是必需的(required)、可选的(optional)、或者可重复的(repeated)<ul>
<li>required：这个message的所有实例都需要这个属性</li>
<li>optional：这个属性的取值可以为空</li>
<li>repeated：这个属性的取值可以使一个列表</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="2-1-2-Bazel"><a href="#2-1-2-Bazel" class="headerlink" title="2.1.2 Bazel"></a>2.1.2 Bazel</h4><ul>
<li><p>简介：Bazel是谷歌开源的自动化构建工具</p>
</li>
<li><p>项目空间(workspace)</p>
<ul>
<li>一个项目空间可以简单地理解为一个文件夹</li>
<li>在这个文件夹中包含了编译一个软件所需要的源代码以及输出编译结果的软连接(symbolic link)地址</li>
</ul>
</li>
<li><p>WORKSPACE文件</p>
<ul>
<li>一个项目空间所对应的文件夹是这个项目的根目录，在这个根目录中需要一个WORKSPACE文件，此文件定义了对外部资源的依赖关系</li>
</ul>
</li>
<li><p>BUILD文件</p>
<ul>
<li>在一个项目空间中,Bazel通过BUILD文件来找到需要编译的目标</li>
<li>Bazel对Python支持的编译方式<ol>
<li>py_binary：将Python程序编译为可执行文件</li>
<li>py_test：编译Python测试程序</li>
<li>py_library：编译为库函数供其他py_binary或py_test调用</li>
</ol>
</li>
<li><p>例子</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">py_library(</span><br><span class="line">    name=<span class="string">"hello_lib"</span>,</span><br><span class="line">    srcs=[</span><br><span class="line">        <span class="string">"hello_lib.py"</span>,</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">py_binary(</span><br><span class="line">    name=<span class="string">"hello_main"</span>,</span><br><span class="line">    srcs=[</span><br><span class="line">        <span class="string">"hello_main.py"</span>,</span><br><span class="line">    ]</span><br><span class="line">    deps=[</span><br><span class="line">        <span class="string">":hello_lib"</span>,</span><br><span class="line">    ],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他</p>
<ul>
<li>BUILD文件由一系列编译目标组成</li>
<li>定义编译目标的先后顺序不会影响编译的结果</li>
<li>在每个编译目标的第一行要指定辨析方式</li>
<li>在每个编译目标的主体中需要给出编译的具体信息，例如：names、srcs、deps等</li>
</ul>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/07/树形动态规划-节点选择/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/07/树形动态规划-节点选择/" itemprop="url">树形动态规划--节点选择</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-07T15:55:25+08:00">
                2019-03-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/蓝桥杯/" itemprop="url" rel="index">
                    <span itemprop="name">蓝桥杯</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/07/树形动态规划-节点选择/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/07/树形动态规划-节点选择/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="树形动态规划–节点选择"><a href="#树形动态规划–节点选择" class="headerlink" title="树形动态规划–节点选择"></a>树形动态规划–节点选择</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>有一棵 <code>n</code> 个节点的树，树上每个节点都有一个正整数权值。如果一个点被选择了，那么在树上和它相邻的点都不能被选择。求选出的点的权值和最大是多少？</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>此题目可以使用动态规划来求解，一个节点只有被选择和不被选择两种情况。</p>
<p>首先构建一个名为dp的 <code>n * 2</code> 大小的矩阵，矩阵中的<code>dp[i][0]</code>表示不选择<code>i</code>节点时的权重最大值,<code>dp[i][1]</code>表示选择<code>i</code>节点时的权重最大值。最初时将矩阵中所有元素全部初始化为 <code>0</code> 。</p>
<p>然后我们列出状态转移方程</p>
<ul>
<li><p>假如 <code>i</code> 是叶子节点</p>
<blockquote>
<p>dp[i][0] = 0</p>
<p>dp[i][1] = w(i)</p>
</blockquote>
</li>
<li><p>假如 <code>i</code> 不是叶子节点</p>
<blockquote>
<p>dp[i][0] = ∑ max(dp[j][0], dp[j][1])</p>
<p>dp[i][1] = w(i) + ∑ dp[j][0]</p>
</blockquote>
</li>
</ul>
<p>接下来按照后序遍历的顺序遍历整个树，来完成dp矩阵中所有值的计算，举例如下</p>
<h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>输入的树如图所示，共有五个节点，每个节点的权重值标注在其右侧。<br><img src="/2019/03/07/树形动态规划-节点选择/Tree_dp_0.png" alt=""></p>
<p>按照后序遍历，节点遍历顺序为 <code>3 -&gt; 4 -&gt; 5 -&gt; 2 -&gt; 1</code></p>
<h4 id="1-初始化状态"><a href="#1-初始化状态" class="headerlink" title="1 初始化状态"></a>1 初始化状态</h4><table>
<thead>
<tr>
<th>Select\Node</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<h4 id="2-后序遍历到3号节点"><a href="#2-后序遍历到3号节点" class="headerlink" title="2 后序遍历到3号节点"></a>2 后序遍历到3号节点</h4><table>
<thead>
<tr>
<th>Select\Node</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<h4 id="3-后序遍历到4号节点"><a href="#3-后序遍历到4号节点" class="headerlink" title="3 后序遍历到4号节点"></a>3 后序遍历到4号节点</h4><table>
<thead>
<tr>
<th>Select\Node</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
</tbody>
</table>
<h4 id="4-后序遍历到5号节点"><a href="#4-后序遍历到5号节点" class="headerlink" title="4 后序遍历到5号节点"></a>4 后序遍历到5号节点</h4><table>
<thead>
<tr>
<th>Select\Node</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
</tbody>
</table>
<h4 id="5-后序遍历到2号节点"><a href="#5-后序遍历到2号节点" class="headerlink" title="5 后序遍历到2号节点"></a>5 后序遍历到2号节点</h4><table>
<thead>
<tr>
<th>Select\Node</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
</tbody>
</table>
<h4 id="6-后序遍历到1号节点"><a href="#6-后序遍历到1号节点" class="headerlink" title="6 后序遍历到1号节点"></a>6 后序遍历到1号节点</h4><table>
<thead>
<tr>
<th>Select\Node</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>5</td>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>10</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>此时取根节点位置 <code>0</code> 和 位置 <code>1</code> 中值较大者即可，本例中显然 <code>5 &lt; 10</code> ,所以最终结果为 <code>10</code> </p>
<h3 id="代码-java"><a href="#代码-java" class="headerlink" title="代码(java)"></a>代码(java)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> node_num;</span><br><span class="line">    <span class="keyword">int</span>[] weight;</span><br><span class="line">    ArrayList&lt;Integer&gt;[] adjacencyTable;</span><br><span class="line">    <span class="keyword">int</span>[][] dp;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">generate_dp</span><span class="params">(<span class="keyword">int</span> node_id, <span class="keyword">int</span> parent_id)</span></span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> temp_id: adjacencyTable[node_id]) &#123;</span><br><span class="line">            <span class="keyword">if</span>(temp_id != parent_id)&#123;</span><br><span class="line">                generate_dp(temp_id, node_id);</span><br><span class="line">                dp[node_id][<span class="number">0</span>] += Math.max(dp[temp_id][<span class="number">0</span>], dp[temp_id][<span class="number">1</span>]);</span><br><span class="line">                dp[node_id][<span class="number">1</span>] += dp[temp_id][<span class="number">0</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        dp[node_id][<span class="number">1</span>] += weight[node_id];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        node_num = scanner.nextInt();</span><br><span class="line">        weight = <span class="keyword">new</span> <span class="keyword">int</span>[node_num];</span><br><span class="line">        adjacencyTable = <span class="keyword">new</span> ArrayList[node_num];</span><br><span class="line">        dp = <span class="keyword">new</span> <span class="keyword">int</span>[node_num][<span class="number">2</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; node_num; i ++)&#123;</span><br><span class="line">            weight[i] = scanner.nextInt();</span><br><span class="line">            adjacencyTable[i]= <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; node_num - <span class="number">1</span>; i ++)&#123;</span><br><span class="line">            <span class="keyword">int</span> node_1 = scanner.nextInt() - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">int</span> node_2 = scanner.nextInt() - <span class="number">1</span>;</span><br><span class="line">            adjacencyTable[node_1].add(node_2);</span><br><span class="line">            adjacencyTable[node_2].add(node_1);</span><br><span class="line">        &#125;</span><br><span class="line">        generate_dp(<span class="number">0</span>, -<span class="number">1</span>);</span><br><span class="line">        System.out.println(Math.max(dp[<span class="number">0</span>][<span class="number">0</span>], dp[<span class="number">0</span>][<span class="number">1</span>]));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> Main().run();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注: oj只有50分，后5个点超时</p>
<h3 id="代码-转-c"><a href="#代码-转-c" class="headerlink" title="代码(转, c ++)"></a>代码(转, c ++)</h3><p>原文链接 <a href="https://www.liuchuo.net/archives/3937" target="_blank" rel="noopener">蓝桥杯 ALGO-4 算法训练 结点选择</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">100010</span>][<span class="number">2</span>];</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; v;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> node, <span class="keyword">int</span> pre)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v[node].size(); i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> temp = v[node][i];</span><br><span class="line">        <span class="keyword">if</span> (temp != pre) &#123;</span><br><span class="line">            dfs(temp, node);</span><br><span class="line">            dp[node][<span class="number">0</span>] += max(dp[temp][<span class="number">0</span>], dp[temp][<span class="number">1</span>]);</span><br><span class="line">            dp[node][<span class="number">1</span>] += dp[temp][<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n, a, b;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;dp[i][<span class="number">1</span>]);</span><br><span class="line">    v.resize(n + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;a, &amp;b);</span><br><span class="line">        v[a].push_back(b);</span><br><span class="line">        v[b].push_back(a);</span><br><span class="line">    &#125;</span><br><span class="line">    dfs(<span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; max(dp[<span class="number">1</span>][<span class="number">0</span>], dp[<span class="number">1</span>][<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另: 柳神是我的!!!</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/06/动态规划/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/06/动态规划/" itemprop="url">动态规划</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-06T20:16:50+08:00">
                2019-03-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/蓝桥杯/" itemprop="url" rel="index">
                    <span itemprop="name">蓝桥杯</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/06/动态规划/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/06/动态规划/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="动态规划-Dynamic-Programing-DP"><a href="#动态规划-Dynamic-Programing-DP" class="headerlink" title="动态规划(Dynamic Programing, DP)"></a>动态规划(Dynamic Programing, DP)</h2><p>动态规划是一种通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法</p>
<p>动态规划常常适用于有重叠子问题和最优子结构性质的问题，动态规划方法所耗时间远少于朴素解法</p>
<p>动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分，再根据子问题的解以得出原问题的解</p>
<p>通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量：一旦某个给定子问题的解已经算出，则将其记忆化存储，以便下次需要同一个子问题解时直接查表。这种做法在重复子问题的数目关于输出的规模呈指数增长时特别有用</p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>动态规划在查找有许多重叠子问题的情况的最优解时有效，它将问题重新组合为子问题。为了避免多次解决这些子问题，它们的结果都逐渐被计算并被保存，从简单的问题直到整个问题都被解决。因此，动态规划保存递归时的结果，因而不会在解决同样的问题时花费时间。</p>
<p>动态规划只能应用于有最优子结构的问题。最优子结构的意思是局部最优解能决定全局最优解。简单来说，问题可以被划分为子问题来解决</p>
<h3 id="适用情况"><a href="#适用情况" class="headerlink" title="适用情况"></a>适用情况</h3><ol>
<li>最优子结构性质: 如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质。最优子结构性质为动态规划算法解决问题提供了重要的线索</li>
<li>无后效性: 即子问题的解一旦确定，就不再改变，不受在这之后、包含它的更大的问题的求解决策影响</li>
<li>子问题重叠性质: 子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题重叠性质，对每个子问题只计算一次，然后将其计算结果保存到一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率</li>
</ol>
<h3 id="实例–01背包问题"><a href="#实例–01背包问题" class="headerlink" title="实例–01背包问题"></a>实例–01背包问题</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>有<code>N</code>件物品和一个容量为<code>V</code>的背包。第i件物品的费用是<code>c[i]</code>，价值为<code>w[i]</code>。求解将哪些物品装入背包可使价值总和最大</p>
<h4 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h4><p>这是最基础的背包问题，特点是：每种物品仅有一件，可以选择放或不放。 用子问题定义状态：即<code>f[i][v]</code>表示前<code>i</code>件物品恰放入一个容量为<code>v</code>的背包可以获得的最大价值。其状态转移方程为</p>
<blockquote>
<p>f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]+w[i]}</p>
</blockquote>
<p>其中<code>i</code>表示放第<code>i</code>个物品</p>
<p><code>v</code>表示背包所容纳的重量</p>
<p><code>f[i][v]</code>表示前<code>i</code>件物品恰放入一个容量为<code>v</code>的背包可以获得的最大价值</p>
<p><code>w[i]</code>表示第<code>i</code>件商品的价值</p>
<p>若只考虑第<code>i</code>件物品的策略（放或不放），那么就可以转化为一个只牵扯前<code>i-1</code>件物品的问题。</p>
<p>如果不放第<code>i</code>件物品，那么问题就转化为前<code>i-1</code>件物品放入容量为<code>v</code>的背包中，价值为<code>f[i-1][v]</code>；</p>
<p>如果放第<code>i</code>件物品，那么问题就转化为前<code>i-1</code>件物品放入剩下的容量为<code>v-c[i]</code>的背包中”，此时能获得的最大价值就是<code>f[i-1][v-c[i]]</code>再加上通过放入第i件物品获得的价值<code>w[i]</code></p>
<h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>假设现在有5件商品，每件商品的的价值和重量如下图所示，背包的容量 <code>C = 11</code>，现在来求解这个01背包问题</p>
<p><img src="/2019/03/06/动态规划/dp_1.jpg" alt=""></p>
<p>首先，我们制作一个表格</p>
<p><img src="/2019/03/06/动态规划/dp_3.jpg" alt=""></p>
<p>表格的每一列表示产品，列数等于产品数加一，即列数为6列。表格的行数等于背包的容量数加一，即行数为12行。</p>
<p>假设我们按照商品序号从<code>1</code>到<code>5</code>的顺序放入商品，表格中的每个元素表示：当背包容量为j时，我们放到第<code>i</code>个商品时，背包价值的最大值。</p>
<p>首先当背包为空时，价值为0，所以我们把第一行初始化为0。</p>
<p>而当背包的容量为0时，我们无法放入任何商品，价值为0，所以我们把第一列初始化为0。</p>
<p>对于其他元素，进行如下判断</p>
<ul>
<li><p>当商品重量大于背包容量时，我们一定无法放入商品，所以此时背包的价值继承自放入上一商品时的最大价值，即表格同列的上一个值。</p>
</li>
<li><p>当重量小于总重时，我们需要考虑是否放入此商品。我们比较放与不放的价值哪个较大。对于放入商品的价值，我们将背包容量减去此商品的重量后，查表。对于不放入商品的价值，继承自放入上一商品时的最大价值。例如，当表格运算到<code>[{1,2,3}, 5]</code>时,我们首先考虑将商品放入背包，则此时背包重量为0，我们查询<code>[{1,2},0]</code>发现此时价值为0，则这种情况下的价值为<code>0 + 18 = 18</code>。然后我们考虑不将商品放入背包，此时背包的价值继承自放入上一商品时的最大价值，我们查询<code>[{1, 2}, 5]</code>,这种情况下的价值为 <code>7</code>。明显第一种情况更好，则我们将这个元素的值设置为<code>18</code>。</p>
</li>
</ul>
<p>当我们计算到表格的右下角元素时，此问题已经解决了，就是右下角元素的值</p>
<h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><p><img src="/2019/03/06/动态规划/dp_2.jpg" alt=""></p>
<h3 id="与内容无关的部分"><a href="#与内容无关的部分" class="headerlink" title="与内容无关的部分"></a>与内容无关的部分</h3><p><img src="/2019/03/06/动态规划/Raspberry_Pi.png" alt=""></p>
<p><img src="/2019/03/06/动态规划/ubuntu魔改MAC.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/05/Tensorflow-1-深度学习简介/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/05/Tensorflow-1-深度学习简介/" itemprop="url">[Tensorflow][1]深度学习简介</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-05T20:35:42+08:00">
                2019-03-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/05/Tensorflow-1-深度学习简介/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/05/Tensorflow-1-深度学习简介/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Tensorflow-1-深度学习简介"><a href="#Tensorflow-1-深度学习简介" class="headerlink" title="[Tensorflow][1]深度学习简介"></a>[Tensorflow][1]深度学习简介</h2><h3 id="1-1-人工智能、机器学习与深度学习"><a href="#1-1-人工智能、机器学习与深度学习" class="headerlink" title="1.1 人工智能、机器学习与深度学习"></a>1.1 人工智能、机器学习与深度学习</h3><h4 id="1-1-1-人工智能"><a href="#1-1-1-人工智能" class="headerlink" title="1.1.1 人工智能"></a>1.1.1 人工智能</h4><ul>
<li><p>背景：一些人类通过直觉可以很快解决的问题，目前却很难通过计算机解决。这些问题包括自然语言处理、图像识别、语音识别等。</p>
</li>
<li><p>人工智能：一类非常广泛的问题，它旨在通过计算机实现类似人类的智能</p>
</li>
<li><p>特定环境：很多早期的人工智能系统只能成功应用于相对 <strong>特定的环境(specific domain)</strong>，在这些特定环境下，计算机需要了解的知识很容易被严格和完整地定义</p>
</li>
<li><p>知识图库(Ontology)</p>
<ul>
<li>定义：将单词整理为<strong>近义词集(synsets)</strong>，并定义近义词集之间的关系</li>
<li>缺点<ul>
<li>花费大量人力物力</li>
<li>可以通过知识图库方式明确定义的知识有限，很大部分无法明确定义的知识，就是人类的经验</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-1-2-机器学习"><a href="#1-1-2-机器学习" class="headerlink" title="1.1.2 机器学习"></a>1.1.2 机器学习</h4><ul>
<li><p>背景：如何让计算机可以和人类一样从历史的经验中获取新的知识？这就是机器学习需要解决的问题</p>
</li>
<li><p>机器学习：</p>
<ul>
<li>定义：如果 <strong>一个程序</strong> 可以在 <strong>任务T</strong> 上，随着 <strong>经验E</strong> 的增加，<strong>效果P</strong> 也可以随之增加，则称这个程序可以从经验中学习</li>
<li>以垃圾邮件分类的方法来解释机器学习<ul>
<li>一个程序：需要用到的机器学习算法，比如逻辑回归算法</li>
<li>任务T：区分垃圾邮件的任务</li>
<li>经验E：已经区分过是否为垃圾邮件的历史邮件</li>
<li>效果P：机器学习算法在区分是否为垃圾邮件任务上的正确率</li>
</ul>
</li>
</ul>
</li>
<li><p>特征(feature)：例如进行垃圾邮件分类时，会从每封邮件中提取对分类结果可能有影响的因素，每个因素称为一个特征(feature)</p>
</li>
<li><p>逻辑回归算法可以从训练数据中计算出每个特征和预测结果的相关度</p>
</li>
<li><p>在训练数据达到一定数量之前，越多的训练数据可以使逻辑回归算法对未知邮件做出的判断越精确</p>
</li>
<li><p>逻辑回归算法可以根据训练数据(经验E)提高在垃圾邮件分类问题(任务T)上的正确率(效果P)</p>
</li>
<li><p>逻辑回归算法的效果也依赖于从数据中提取的特征</p>
</li>
<li><p>如何从实体中提取特征，对于很多传统机器学习算法的性能有巨大影响</p>
</li>
</ul>
<h4 id="1-1-3-深度学习"><a href="#1-1-3-深度学习" class="headerlink" title="1.1.3 深度学习"></a>1.1.3 深度学习</h4><ul>
<li><p>背景：对许多机器学习问题来说，特征提取不是一件简单的事情，在一些复杂问题上，要通过手动的方式设计有效的特征集合，需要很多的时间和精力</p>
</li>
<li><p>深度学习</p>
<ul>
<li>定义：深度学习是机器学习的一个分支，它除了可以学习特征和任务之间的关联，还能自动从简单特征中提取更加复杂的特征</li>
<li>深度学习解决的核心问题之一就是自动地将简单的特征组合成更加复杂的特征，并使用这些组合特征解决问题</li>
<li>深度学习算法可以从数据中学习更加复杂的特征表达，使得最后一步权重学习变得更加简单且高效</li>
</ul>
</li>
<li><p>深度学习与传统机器学习流程的对比<br><img src="/2019/03/05/Tensorflow-1-深度学习简介/深度学习与传统机器学习流程对比.jpg" alt=""></p>
</li>
</ul>
<h4 id="1-1-4-深度学习与传统神经科学"><a href="#1-1-4-深度学习与传统神经科学" class="headerlink" title="1.1.4 深度学习与传统神经科学"></a>1.1.4 深度学习与传统神经科学</h4><ul>
<li><p>早期深度学习受到神经科学的启发</p>
</li>
<li><p>例如</p>
<ul>
<li>神经科学的研究：虽然哺乳动物大脑分为了很多区域，但这些区域的学习机制却是相似的</li>
<li>深度学习的运用：深度学习算法具有通用性，深度学习研究者往往可以跨越多个研究方向甚至同时活跃于所有的研究方向</li>
</ul>
</li>
<li><p>现代深度学习的发展并不拘泥于模拟人脑神经元和人脑的工作机理，模拟人类大脑也不再是深度学习研究的主导方向</p>
</li>
<li><p>计算神经学：试图从算法层理解大脑的工作机制</p>
</li>
</ul>
<hr>
<h3 id="1-2-深度学习的发展历程"><a href="#1-2-深度学习的发展历程" class="headerlink" title="1.2 深度学习的发展历程"></a>1.2 深度学习的发展历程</h3><p>神经网络的发展史大致可以分为三个阶段</p>
<h4 id="1-2-1-第一个阶段—仿生"><a href="#1-2-1-第一个阶段—仿生" class="headerlink" title="1.2.1 第一个阶段—仿生"></a>1.2.1 第一个阶段—仿生</h4><ul>
<li><p>早期的神经网络模型类似于仿生机器学习，它试图模拟大脑的学习机理</p>
</li>
<li><p>McCulloch-Pitts Neuron结构</p>
<ul>
<li>简介：McCulloch-Pitts Neuron结构大致模拟了人类神经元的工作原理，它们都有一些输入，然后将输入进行一些变换后得到输出结果</li>
<li>如图<br><img src="/2019/03/05/Tensorflow-1-深度学习简介/神经元结构.jpg" alt=""></li>
<li>理解：McCulloch-Pitts Neuron结构使用简单的线性加权和的方式来模拟这个变换，将n个输入值提供给McCulloch-Pitts Neuron结构后，McCulloch-Pitts Neuron结构会通过n个权重w1,w2,…,wn来计算这n个输入的加权和，然后用这个加权和经过一个阈值函数得到一个0或1的输出</li>
</ul>
</li>
<li><p>感知机( perceptron )</p>
<ul>
<li>首个可以根据样例数据学习特征权重的模型</li>
<li>感知机模型只能解决线性不可分问题</li>
</ul>
</li>
</ul>
<h4 id="1-2-2-第二个阶段—分布式知识表达和反向传播算法"><a href="#1-2-2-第二个阶段—分布式知识表达和反向传播算法" class="headerlink" title="1.2.2 第二个阶段—分布式知识表达和反向传播算法"></a>1.2.2 第二个阶段—分布式知识表达和反向传播算法</h4><ul>
<li><p>分布式知识表达</p>
<ul>
<li>核心思想：现代世界中的知识和概念应该由多个神经元(neuron)来表达，而模型中的每个神经元也应该参与表达多个概念</li>
<li>意义：大大加强了模型的表达能力，让神经网络从宽度的方向走向了深度的方向</li>
<li>深层神经网络解决了线性不可分问题</li>
</ul>
</li>
<li><p>反向传播算法</p>
<ul>
<li>大幅降低了训练神经网络所需要的时间</li>
</ul>
</li>
<li><p>此阶段的两大阻碍</p>
<ol>
<li>在当时的计算资源下，要训练深层的神经网络仍然是非常困难的</li>
<li>当时的数据量较少，无法满足训练深层神经网络的需求</li>
</ol>
</li>
</ul>
<h4 id="1-2-2-第三个阶段—计算能力和数据量的提升"><a href="#1-2-2-第三个阶段—计算能力和数据量的提升" class="headerlink" title="1.2.2 第三个阶段—计算能力和数据量的提升"></a>1.2.2 第三个阶段—计算能力和数据量的提升</h4><ul>
<li>此阶段解决了第二阶段的两大阻碍<ol>
<li>随着云计算和GPU的出现，计算量已经不再是阻碍神经网络发展的问题</li>
<li>随着互联网+的发展，获得海量数据也不再困难 </li>
</ol>
</li>
</ul>
<hr>
<h3 id="1-3-深度学习的应用"><a href="#1-3-深度学习的应用" class="headerlink" title="1.3 深度学习的应用"></a>1.3 深度学习的应用</h3><h4 id="1-3-1-计算机视觉"><a href="#1-3-1-计算机视觉" class="headerlink" title="1.3.1 计算机视觉"></a>1.3.1 计算机视觉</h4><ul>
<li><p>传统的机器学习算法很难抽象出足够有效地特征，使得学习模型既可以区分不同的个体，又可以区分相同个体在不同环境下<br>的变化</p>
</li>
<li><p>深度学习技术通过从海量数据中自动习得更加有效地特征表达，可以很好的解决这个问题</p>
</li>
<li><p>光学字符识别(OCR)：使用计算机程序将计算机无法理解的图片中的字符，比如数字、字母、汉字等符号，转化为计算机可以理解的文本格式</p>
</li>
</ul>
<h4 id="1-3-2-语音识别"><a href="#1-3-2-语音识别" class="headerlink" title="1.3.2 语音识别"></a>1.3.2 语音识别</h4><ul>
<li>随着数据量的增大，使用深度学习模型无论在正确率的增长数值上还是在增长比率上都要优于使用混合高斯模型的算法</li>
</ul>
<h4 id="1-3-3-自然语言处理"><a href="#1-3-3-自然语言处理" class="headerlink" title="1.3.3 自然语言处理"></a>1.3.3 自然语言处理</h4><ul>
<li><p>包括：语言模型、机器翻译、词性标注、实体识别、情感分析、广告推荐、搜索排序等</p>
</li>
<li><p>单词向量(word embedding)</p>
<ul>
<li>单词向量提供了一种更加灵活的方式来刻画单词的语义</li>
<li>单词向量会将一个单词表示为一个相对较低维度的向量(比如100维或200维)。对于语义相近的单词，其对应的单词向量在空间中的距离也应该接近。于是单词语义上的相似度可以通过空间中的距离来描述。</li>
<li>通过单词向量可以进行单词之间的运算</li>
</ul>
</li>
</ul>
<h4 id="1-3-4-人机博弈"><a href="#1-3-4-人机博弈" class="headerlink" title="1.3.4 人机博弈"></a>1.3.4 人机博弈</h4><ul>
<li>AlphaGo<ul>
<li>AlphaGo的组成部分：蒙特卡罗树搜索(Monte Carlo tree search,MCTS)、估值网络(value network)和走棋网络(policy network)。</li>
<li>蒙特卡罗树搜索：实现对不同落子点的搜索</li>
<li>走棋网络：预测下一步在哪里落子</li>
<li>估值网络：给定当前的棋盘，判断黑棋赢的概率</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/03/理解LSTM网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/03/理解LSTM网络/" itemprop="url">理解LSTM网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-03T15:27:26+08:00">
                2019-03-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/03/理解LSTM网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/03/理解LSTM网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="理解LSTM网络"><a href="#理解LSTM网络" class="headerlink" title="理解LSTM网络"></a>理解LSTM网络</h2><h3 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络(RNN)"></a>循环神经网络(RNN)</h3><p>人类不会每时每刻都重新开始思考。当你阅读这篇文章时，你会根据你对之前单词的理解来理解每个单词。你不要丢掉之前的内容，然后再从头开始思考。你的思考是有持久性的。</p>
<p>传统的神经网络不能做到这一点，这似乎是一个主要的缺点。例如，假设您想要对电影中每个点发生的事件进行分类。目前尚不清楚传统神经网络如何利用其对电影中先前事件的推理来得出后面的事件。</p>
<p>循环神经网络解决了这个问题。它们是带有循环的网络，允许信息持续存在。</p>
<p><img src="/2019/03/03/理解LSTM网络/RNN-rolled.png" alt=""></p>
<p>在上图中，神经网络结构 A 通过一些输入 x<sub>t</sub> 并输出一个值 h<sub>t</sub>。循环允许信息从网络的一个步骤传递到下一个步骤。</p>
<p>这些循环使得循环神经网络看起来有点神秘。但是，如果你多想一点，事实证明它们与普通的神经网络并没有什么不同。可以将循环神经网络视为同一网络的多个副本，每个副本都将消息传递给后继者。考虑如果我们展开循环会发生什么：</p>
<p><img src="/2019/03/03/理解LSTM网络/RNN-unrolled.png" alt=""></p>
<p>这种类似链的性质表明，递归神经网络与序列和列表密切相关。它们是用于此类数据的神经网络的自然架构。 </p>
<p>他们肯定会被使用！在过去几年中，将RNN应用于各种问题取得了令人难以置信的成功：语音识别，语言建模，翻译，图像识别……这个列表还在继续。我将不展开讨论使用RNNs可以实现的惊人壮举，以及Andrej Karpathy的优秀博客文章，回归神经网络的不合理有效性。但他们真的很棒。</p>
<p>这些成功的关键在于使用“LSTM”，这是一种非常特殊的递归神经网络，对于许多任务而言，它比标准版本好得多。几乎所有基于递归神经网络的令人兴奋的结果都是用它们实现的。这篇论文将探讨这些LSTM。</p>
<h3 id="长期依赖问题"><a href="#长期依赖问题" class="headerlink" title="长期依赖问题"></a>长期依赖问题</h3><p>RNN的一个优点是他们可能能够将先前信息连接到当前任务，例如使用先前的视频帧中的信息可能得出对当前帧的理解。如果RNN可以做到这一点，它们将非常有用。 </p>
<p>有时，我们只需要通过很近的信息来得到当前任务。例如，考虑一种语言模型，试图根据之前的单词预测下一个单词。如果我们试图预测 “the clouds are in the sky” 的最后一个词，我们不需要任何进一步的背景 - 很明显下一个词将是 “sky”。在这种情况下，如果相关信息与所需信息之间的距离很小，则RNN可以学习使用过去的信息。</p>
<p><img src="/2019/03/03/理解LSTM网络/RNN-shorttermdepdencies.png" alt=""></p>
<p>但也有一些情况需要更多的背景知识。考虑尝试预测文本中的最后一个词“我在法国长大……我说流利的法语。”最近的信息表明，下一个词可能是一种语言的名称，但如果我们想缩小哪种语言，我们需要从更进一步的背景来看，法国的背景。相关信息之间距离很大是完全可能的。 </p>
<p>不幸的是，随着距离的扩大，RNN无法学习连接信息</p>
<p><img src="/2019/03/03/理解LSTM网络/RNN-longtermdependencies.png" alt=""></p>
<p>理论上，RNN绝对能够处理这种“长期依赖性”。人类可以仔细挑选参数来解决这种形式的玩具问题。遗憾的是，在实践中，RNN似乎无法学习它们。 Hochreiter和Bengio等人对该问题进行了深入探讨。他们找到了一些非常根本的原因来说明为什么它可能很难。</p>
<p>值得庆幸的是，LSTM结构不存在这个问题！</p>
<h3 id="LSTM-网络"><a href="#LSTM-网络" class="headerlink" title="LSTM 网络"></a>LSTM 网络</h3><p>长短期记忆网络 - 通常只称为“LSTM” - 是一种特殊的RNN，能够学习长期依赖性。它们是由Hochreiter和Schmidhuber提出的.它在各种各样的问题上都表现得非常好，现在被广泛使用。</p>
<p>LSTM明确旨在解决长期依赖性问题。长时间记住信息实际上是他们的默认行为，而不是他们难以学习的东西！</p>
<p>所有递归神经网络都具有一个循环体。在标准RNN中，该循环体将具有非常简单的结构，例如单个tanh层。</p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM3-SimpleRNN.png" alt=""></p>
<p>LSTM也具有循环体，但循环体具有不同的结构。它由四个被称为门的结构组成，而不是一个简单的tanh，并以一种非常特殊的方式进行交互。</p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM3-chain.png" alt=""></p>
<p>不要担心发生了什么的细节。我们将逐步介绍LSTM图。现在，让我们试着理解我们将要使用的符号。 </p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM2-notation.png" alt=""></p>
<p>在上图中，每条线都携带一个整个向量，从一个节点的输出到其他节点的输入。粉色圆圈表示逐位运算，如矢量加法，而黄色框表示简单的神经网络结构。行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。</p>
<h3 id="LSTM的核心理念"><a href="#LSTM的核心理念" class="headerlink" title="LSTM的核心理念"></a>LSTM的核心理念</h3><p>LSTM的关键是单元状态，水平线贯穿图的顶部。 </p>
<p>单元状态有点像传送带。它直接沿着整个链运行，只有一些次要的线性交互。信息很容易沿着它不变地流动。</p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM3-C-line.png" alt=""></p>
<p>LSTM能够通过被称为门的结构来移除或添加信息到单元状态。 </p>
<p>门是一种可选择通过信息的方式。它们由sigmoid神经网络层和逐位乘法运算组成。</p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM3-gate.png" alt=""></p>
<p>sigmoid层输出0到1之间的数字，描述每个组件应该通过多少信息。值为零意味着“不让任何信息通过”，而值为1则意味着“让一切都通过！</p>
<p>LSTM具有三个这样的门，用于保护和控制单元状态。</p>
<h3 id="LSTM详细运算过程"><a href="#LSTM详细运算过程" class="headerlink" title="LSTM详细运算过程"></a>LSTM详细运算过程</h3><p>LSTM的第一步是确定我们将从单元状态中丢弃哪些信息。该判定由称为“遗忘门层”的sigmoid层来进行。它输入h<sub>t-1</sub>和x<sub>t</sub>，并在单元状态C<sub>t-1</sub>中为每个数字输出一个0到1之间的数字。 1代表“完全保留这个”，而0代表“完全丢弃这个”。</p>
<p>让我们回到之前提到的试图根据以前的所有单词预测下一个单词的那个语言模型的示例。在这样的问题中，单元状态可能包括当前受试者的性别，因此可以使用正确的代词。当我们看到一个新受试者时，我们想要忘记旧受试者的性别。</p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM3-focus-f.png" alt=""></p>
<p>下一步是确定我们将在单元状态中存储哪些新信息。这有两个部分。首先，称为“输入门层”的sigmoid层决定我们将更新哪些值。接下来，tanh层创建可以添加到状态的新候选值C<sub>t</sub>的向量。然后，我们将结合他们两个的结构来创建状态更新。</p>
<p>在我们的语言模型的例子中，我们想要将新受试者的性别添加到单元状态中，以替换我们需要忘记的旧受试者。</p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM3-focus-i.png" alt=""></p>
<p>现在是时候将旧的单元状态C<sub>t-1</sub>更新为新的单元状态C<sub>t</sub>。之前的步骤已经决定要做什么，我们只需要实际做到这一点。</p>
<p>我们将旧状态逐位乘以f<sub>t</sub>，忘记我们之前的步骤中决定忘记的内容。然后我们把它和i<sub>t</sub> * C<sub>t</sub>相加。这是新的状态。</p>
<p>在语言模型的情况下，我们实际上丢弃了关于旧主题的性别的信息并添加新信息，正如我们在前面的步骤中所做的那样。</p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM3-focus-C.png" alt=""></p>
<p>最后，我们需要决定我们要输出的内容。此输出将基于我们的单元状态，但将是被过滤的版本。首先，我们运行一个sigmoid层，它决定我们要输出的单元状态的哪些部分。然后，我们将单元状态通过tanh（将值变到-1和1之间）并将其乘以sigmoid门的输出，这样我们只输出我们决定输出的部分。</p>
<p><img src="/2019/03/03/理解LSTM网络/LSTM3-focus-o.png" alt=""></p>
<h3 id="长期记忆的变种"><a href="#长期记忆的变种" class="headerlink" title="长期记忆的变种"></a>长期记忆的变种</h3><p>到目前为止我所描述的是一个非常常规的LSTM。但并非所有LSTM都与上述相同。事实上，似乎几乎所有涉及LSTM的论文都使用略有不同的版本。差异很小，但值得一提。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>早些时候，我提到了人们使用RNN取得的显着成果。基本上所有这些都是使用LSTM实现的。对于大多数任务来说，它们确实工作得更好！ </p>
<p>写下来作为一组方程式，LSTM看起来非常令人生畏。希望在这篇文章中逐步走过它们，使它们更加平易近人。 </p>
<p>LSTM是我们通过RNN实现的重要一步。很自然地想知道：还有另一个重要的步骤吗？研究人员普遍认为：“是的！下一步是它的注意力！“我们的想法是让RNN的每一步都从一些更大的信息集中选择信息。例如，如果您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个单词。实际上，徐等人做到这一点，如果你想探索注意力，它可能是一个有趣的起点！使用注意力已经取得了许多非常令人兴奋的结果，而且似乎还有很多事情即将来临……</p>
<p>注意力不是RNN研究中唯一激动人心的线索。例如，Kalchbrenner等人的Grid LSTMs似乎非常有希望。在生成模型中使用RNN的工作似乎也很有趣。过去几年对于递归神经网络来说是一个激动人心的时刻，即将到来的神经网络只会更加激动人心！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/03/YAML学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/03/YAML学习/" itemprop="url">YAML学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-03T13:57:29+08:00">
                2019-03-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/other/" itemprop="url" rel="index">
                    <span itemprop="name">other</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/03/YAML学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/03/YAML学习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="YAML学习"><a href="#YAML学习" class="headerlink" title="YAML学习"></a>YAML学习</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p>YAML语言的设计目标，就是方便人类书写。它实质上是一种通用的数据串行化格式</p>
<h4 id="1-1-YAML的基本语法规则"><a href="#1-1-YAML的基本语法规则" class="headerlink" title="1.1 YAML的基本语法规则"></a>1.1 YAML的基本语法规则</h4><ol>
<li><p>大小写敏感</p>
</li>
<li><p>使用缩进表示层级关系</p>
</li>
<li><p>缩进时不允许使用tab键，只允许使用空格</p>
</li>
<li><p>缩进的空格数目不重要，只要相同层级的元素左侧对齐即可</p>
</li>
<li><p>#表示注释，从这个字符一直到行尾，都会被解释器忽略</p>
</li>
</ol>
<h4 id="1-2-YAML支持的数据结构有三种"><a href="#1-2-YAML支持的数据结构有三种" class="headerlink" title="1.2 YAML支持的数据结构有三种"></a>1.2 YAML支持的数据结构有三种</h4><ol>
<li><p>对象: 键值对的集合, 又称为映射(mapping)/哈希(hashes)/字典(dictionary)</p>
</li>
<li><p>数组: 一组按次序排列的值, 又称为序列(sequence)/列表(list)</p>
</li>
<li><p>纯量(scalars): 单个的, 不可再分的值</p>
</li>
</ol>
<h3 id="2-对象"><a href="#2-对象" class="headerlink" title="2 对象"></a>2 对象</h3><p>对象是一组键值对, 使用冒号结构表示<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">animal:</span> <span class="string">pets</span></span><br></pre></td></tr></table></figure></p>
<p>转为JavaScript如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">animal</span>: <span class="string">'pets'</span>&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="3-数组"><a href="#3-数组" class="headerlink" title="3 数组"></a>3 数组</h3><p>一组连词线开头的行，构成一个数组</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="string">Cat</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Dog</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Goldfish</span></span><br></pre></td></tr></table></figure>
<p>转为JavaScript如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'Cat'</span>, <span class="string">'Dog'</span>, <span class="string">'Goldfish'</span>]</span><br></pre></td></tr></table></figure></p>
<p>数据结构的子成员是一个数组，则可以在该项下面缩进一个空格<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span></span><br><span class="line"><span class="bullet"> -</span> <span class="string">Cat</span></span><br><span class="line"><span class="bullet"> -</span> <span class="string">Dog</span></span><br><span class="line"><span class="bullet"> -</span> <span class="string">Goldfish</span></span><br></pre></td></tr></table></figure></p>
<p>转为 JavaScript 如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ [ <span class="string">'Cat'</span>, <span class="string">'Dog'</span>, <span class="string">'Goldfish'</span> ] ]</span><br></pre></td></tr></table></figure></p>
<p>数组也可以采用行内表示法<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">animal:</span> <span class="string">[Cat,</span> <span class="string">Dog]</span></span><br></pre></td></tr></table></figure></p>
<p>转为 JavaScript 如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">animal</span>: [ <span class="string">'Cat'</span>, <span class="string">'Dog'</span> ] &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="4-复合结构"><a href="#4-复合结构" class="headerlink" title="4 复合结构"></a>4 复合结构</h3><p>对象和数组可以结合使用，形成复合结构。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">languages:</span></span><br><span class="line"><span class="bullet"> -</span> <span class="string">Ruby</span></span><br><span class="line"><span class="bullet"> -</span> <span class="string">Perl</span></span><br><span class="line"><span class="bullet"> -</span> <span class="string">Python</span> </span><br><span class="line"><span class="attr">websites:</span></span><br><span class="line"><span class="attr"> YAML:</span> <span class="string">yaml.org</span> </span><br><span class="line"><span class="attr"> Ruby:</span> <span class="string">ruby-lang.org</span> </span><br><span class="line"><span class="attr"> Python:</span> <span class="string">python.org</span> </span><br><span class="line"><span class="attr"> Perl:</span> <span class="string">use.perl.org</span></span><br></pre></td></tr></table></figure></p>
<p>转为 JavaScript 如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">languages</span>: [ <span class="string">'Ruby'</span>, <span class="string">'Perl'</span>, <span class="string">'Python'</span> ],</span><br><span class="line">  websites: </span><br><span class="line">   &#123; <span class="attr">YAML</span>: <span class="string">'yaml.org'</span>,</span><br><span class="line">     Ruby: <span class="string">'ruby-lang.org'</span>,</span><br><span class="line">     Python: <span class="string">'python.org'</span>,</span><br><span class="line">     Perl: <span class="string">'use.perl.org'</span> &#125; &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="5-纯量"><a href="#5-纯量" class="headerlink" title="5 纯量"></a>5 纯量</h3><p>纯量是最基本的、不可再分的值。以下数据类型都属于JavaScript纯量</p>
<ul>
<li>字符串</li>
<li>布尔值</li>
<li>整数</li>
<li>浮点数</li>
<li>Null</li>
<li>时间</li>
<li>日期</li>
</ul>
<p>数值直接以字面量的形式表示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">number:</span> <span class="number">12.30</span></span><br></pre></td></tr></table></figure>
<p>转为 JavaScript 如下</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">number</span>: <span class="number">12.30</span> &#125;</span><br></pre></td></tr></table></figure>
<p>布尔值用true和false表示</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">isSet:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>转为 JavaScript 如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">isSet</span>: <span class="literal">true</span> &#125;</span><br></pre></td></tr></table></figure></p>
<p><code>null</code>用<code>~</code>表示<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">parent:</span> <span class="string">~</span></span><br></pre></td></tr></table></figure></p>
<p>转为 JavaScript 如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">parent</span>: <span class="literal">null</span> &#125;</span><br></pre></td></tr></table></figure></p>
<p>时间采用 ISO8601 格式<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">start_time:</span> <span class="number">2001</span><span class="bullet">-12</span><span class="bullet">-14</span><span class="attr">t21:59:43.10-05:00</span></span><br></pre></td></tr></table></figure></p>
<p>转为 JavaScript 如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">iso8601</span>: <span class="keyword">new</span> <span class="built_in">Date</span>(<span class="string">'2001-12-14t21:59:43.10-05:00'</span>) &#125;</span><br></pre></td></tr></table></figure></p>
<p>日期采用复合 iso8601 格式的年、月、日表示<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">date:</span> <span class="number">1976</span><span class="bullet">-07</span><span class="bullet">-31</span></span><br></pre></td></tr></table></figure></p>
<p>转为JavaScript如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">date</span>: <span class="keyword">new</span> <span class="built_in">Date</span>(<span class="string">'1976-07-31'</span>) &#125;</span><br></pre></td></tr></table></figure></p>
<p>YAML 允许使用两个感叹号，强制转换数据类型<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">e:</span> <span class="type">!!str</span> <span class="number">123</span></span><br><span class="line"><span class="attr">f:</span> <span class="type">!!str</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>转为 JavaScript 如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">e</span>: <span class="string">'123'</span>, <span class="attr">f</span>: <span class="string">'true'</span> &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="6-字符串"><a href="#6-字符串" class="headerlink" title="6 字符串"></a>6 字符串</h3><p>字符串是最常见，也是最复杂的一种数据类型</p>
<p>字符串默认不使用引号表示</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">str:</span> <span class="string">这是一行字符串</span></span><br></pre></td></tr></table></figure>
<p>如果字符串之中包含空格或特殊字符，需要放在引号之中<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">str:</span> <span class="string">'内容： 字符串'</span></span><br></pre></td></tr></table></figure></p>
<h3 id="7-引用"><a href="#7-引用" class="headerlink" title="7 引用"></a>7 引用</h3><p>锚点<code>&amp;</code>和别名<code>*</code>，可以用来引用。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">defaults:</span> <span class="meta">&amp;defaults</span></span><br><span class="line"><span class="attr">  adapter:</span>  <span class="string">postgres</span></span><br><span class="line"><span class="attr">  host:</span>     <span class="string">localhost</span></span><br><span class="line"></span><br><span class="line"><span class="attr">development:</span></span><br><span class="line"><span class="attr">  database:</span> <span class="string">myapp_development</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="meta">*defaults</span></span><br><span class="line"></span><br><span class="line"><span class="attr">test:</span></span><br><span class="line"><span class="attr">  database:</span> <span class="string">myapp_test</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="meta">*defaults</span></span><br></pre></td></tr></table></figure>
<p>等同于下面的代码<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">defaults:</span></span><br><span class="line"><span class="attr">  adapter:</span>  <span class="string">postgres</span></span><br><span class="line"><span class="attr">  host:</span>     <span class="string">localhost</span></span><br><span class="line"></span><br><span class="line"><span class="attr">development:</span></span><br><span class="line"><span class="attr">  database:</span> <span class="string">myapp_development</span></span><br><span class="line"><span class="attr">  adapter:</span>  <span class="string">postgres</span></span><br><span class="line"><span class="attr">  host:</span>     <span class="string">localhost</span></span><br><span class="line"></span><br><span class="line"><span class="attr">test:</span></span><br><span class="line"><span class="attr">  database:</span> <span class="string">myapp_test</span></span><br><span class="line"><span class="attr">  adapter:</span>  <span class="string">postgres</span></span><br><span class="line"><span class="attr">  host:</span>     <span class="string">localhost</span></span><br></pre></td></tr></table></figure></p>
<p><code>&amp;</code>用来建立锚点（<code>defaults</code>），<code>&lt;&lt;</code>表示合并到当前数据，<code>*</code>用来引用锚点</p>
<p>下面是另一个例子<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="meta">&amp;showell</span> <span class="string">Steve</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">Clark</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">Brian</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">Oren</span> </span><br><span class="line"><span class="bullet">-</span> <span class="meta">*showell</span></span><br></pre></td></tr></table></figure></p>
<p>转为 JavaScript 代码如下<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="string">'Steve'</span>, <span class="string">'Clark'</span>, <span class="string">'Brian'</span>, <span class="string">'Oren'</span>, <span class="string">'Steve'</span> ]</span><br></pre></td></tr></table></figure></p>
<p><img src="/2019/03/03/YAML学习/IBM_5100.jpg" alt=""></p>
<p><img src="/2019/03/03/YAML学习/zhizhi.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg" alt="T0UGH(GuiPing Wang)">
            
              <p class="site-author-name" itemprop="name">T0UGH(GuiPing Wang)</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">112</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">56</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/T0UGH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="tough.neu.edu@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friends
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://neusoftware.top/solo" title="ChengYi" target="_blank">ChengYi</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://mike4ellis.github.io/" title="Mike" target="_blank">Mike</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">T0UGH(GuiPing Wang)</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'E1MH6h0YP3yhA0PJsohNBgiT-gzGzoHsz',
        appKey: 'YOiN6zLq3XGfKmlR0b8vyHtN',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
