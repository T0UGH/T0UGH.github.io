<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="打怪升级日常">
<meta property="og:url" content="http://yoursite.com/page/10/index.html">
<meta property="og:site_name" content="打怪升级日常">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="打怪升级日常">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/10/">





  <title>打怪升级日常</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    <a href="https://github.com/T0UGH" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">打怪升级日常</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">两星炸弹人(╯‵□′)╯炸弹！•••</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/22/Tensorflow-9-自然语言处理/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/22/Tensorflow-9-自然语言处理/" itemprop="url">[Tensorflow][9]自然语言处理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-22T21:57:03+08:00">
                2019-05-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/22/Tensorflow-9-自然语言处理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/05/22/Tensorflow-9-自然语言处理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第9章-自然语言处理"><a href="#第9章-自然语言处理" class="headerlink" title="第9章 自然语言处理"></a>第9章 自然语言处理</h2><ul>
<li>发展历程<ol>
<li>在自然语言处理研究的早期，人们试图整理出关于自然语言的语法，并根据这些语法去理解和生成句子</li>
<li>从20世纪80年代起，随着硬件计算能力的增强和大型语料库的出现，使用统计方法对语言进行概率建模的方式开始变成主流</li>
<li>从2010年起，基于循环神经网络的方法在许多自然语言处理的问题上超越了传统的统计模型</li>
</ol>
</li>
</ul>
<hr>
<h3 id="9-1-语言模型的背景知识"><a href="#9-1-语言模型的背景知识" class="headerlink" title="9.1 语言模型的背景知识"></a>9.1 语言模型的背景知识</h3><h4 id="9-1-1-语言模型简介"><a href="#9-1-1-语言模型简介" class="headerlink" title="9.1.1 语言模型简介"></a>9.1.1 语言模型简介</h4><ul>
<li><p>语言模型简介</p>
<ul>
<li>假设一门语言中所有可能的句子服从某一个概率分布，每个句子出现的概率加起来为1，那么语言模型的任务就是预测每个句子在语言中出现的概率</li>
<li>把句子看成单词的序列，语言模型可以表示为一个计算<code>p(w1,w2,w3,...,wm)</code>的模型</li>
<li>语言模型仅仅对句子出现的概率进行建模，并不尝试“理解”句子的内容含义</li>
</ul>
</li>
<li><p>语言模型应用</p>
<ul>
<li>很多生成自然语言文本的应用都依赖语言模型来优化输出文本的流畅性<ul>
<li>生成的句子在语言模型中的概率越高，说明其越有可能是一个流畅、自然的句子</li>
</ul>
</li>
<li>神经网络机器翻译的Seq2Seq模型可以看作是一个条件语言模型(Conditional Language Model)，它相当于是在给定输入的情况下对目标语言的所有句子估算概率，并选择其中概率最大的句子作为输出</li>
</ul>
</li>
<li><p>如何计算一个句子的概率</p>
<ul>
<li>一个句子可以被看成一个单词序列<code>S = (w1, w2, w3, w4, ... wm)</code><ul>
<li>其中<code>m</code>代表句子的长度</li>
</ul>
</li>
<li>它的概率可以表示为：<code>p(S) = p(w1, w2, w3, w4, ... wm) = p(w1) * p(w2 | w1) * p(w3 | w1, w2)...p(wm | w1, w2, w3,..., wm-1)</code><ul>
<li>其中<code>p(wm | w1, w2, w3,..., wm-1)</code>表示，已知前m-1个单词时，第m个单词为wm的条件概率</li>
</ul>
</li>
</ul>
</li>
<li><p>然而</p>
<ul>
<li>任何一门语言的词汇量都很大，词汇的组合更是不计其数</li>
<li>假设一门语言的词汇量为V，如果要将<code>p(wm | w1, w2, w3,..., wm-1)</code>的所有参数保存在一个模型中，则需要<code>pow(v, m)</code>个参数，数量太多</li>
</ul>
</li>
<li><p>n-gram</p>
<ul>
<li>为了控制参数的数量，n-gram模型做了一个有限历史假设：当前单词的出现概率仅仅与前面的<code>n-1</code>个单词相关</li>
<li>公式如下<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/n-gram概率公式.jpg" alt=""></li>
<li>n-gram模型中的n指的是当前单词依赖它前面的单词的个数</li>
<li>通常n可以取1、2、3、4，其中n取1、2、3时分别称为unigram、bigram、trigram</li>
<li>假设某种语言的单词表大小为V，那么n-gram模型需要估计的不同参数数量为O(pow(v, n))量级</li>
<li>当n越大时，n-gram模型在理论上越准确，但也越复杂，需要的计算量和训练语料数据量也越大</li>
<li>n-gram模型的参数一般采用最大似然估计(Maximum Likelihood Estimation, MLE)方法计算<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/n-gramMLE方法.jpg" alt=""></li>
<li>训练语料的规模越大，参数估计的结果越可靠</li>
<li>但即使训练数据的规模非常大时，还是有许多单词序列在训练语料中不会出现，这就导致许多参数为0，为了避免因为乘以0而导致整个句子概率为0，使用最大似然估计的方法时需要加入平滑避免参数取值为0</li>
</ul>
</li>
</ul>
<hr>
<h4 id="9-1-2-语言模型的评价方法—–复杂度-perplexity"><a href="#9-1-2-语言模型的评价方法—–复杂度-perplexity" class="headerlink" title="9.1.2 语言模型的评价方法—–复杂度(perplexity)"></a>9.1.2 语言模型的评价方法—–复杂度(perplexity)</h4><ul>
<li><p>复杂度简介(perplexity)</p>
<ul>
<li>语言模型效果好坏的常用评价指标是复杂度(perplexity)</li>
<li>在一个测试集上得到的perplexity越低，说明建模的效果越好</li>
<li>复杂度计算公式</li>
<li>perplexity值刻画的是语言模型预测一个语言样本的能力<ul>
<li>比如，已知(w1, w2, w3,…, wm)这句话会出现在语料库中，那么通过语言模型计算得到的这句话的概率越高，说明语言模型对这个语料库拟合的越好</li>
</ul>
</li>
</ul>
</li>
<li><p>perplexity的理解</p>
<ul>
<li>perplexity实际上计算每一个单词得到的概率倒数的几何平均</li>
<li>perplexity可以理解为模型预测下一个词时的平均可选择数量</li>
<li>目前在PTB数据集上最好的语言模型perplexity为47.7，就是在平均情况下，该模型预测下一个词时，有47.7个词等可能地可以作为下一个词的合理选择</li>
</ul>
</li>
<li><p>perplexity与交叉熵</p>
<ul>
<li>交叉熵(Cross Entropy)用来描述两个概率分布之间的一种距离</li>
<li>在数学上log perplexity可以看作真实分布与预测分布之间的交叉熵</li>
<li>即每个位置上单词的真实分布与模型的预测分布之间的交叉熵</li>
</ul>
</li>
<li><p>TensorFlow提供了两个方便计算交叉熵的函数</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设词汇表的大小为3，语料包含两个单词"2 0"</span></span><br><span class="line">word_labels = tf.constant([<span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设模型对两个单词预测时，产生的logit分别为[2.0, -1.0, 3.0]和[1.0, 0.0, -0.5]</span></span><br><span class="line"><span class="comment"># 注意这里的logit不是概率，因此它们不是0.0-1.0之间的数字</span></span><br><span class="line"><span class="comment"># 如需要计算概率，则需要调用prob = tf.nn.softmax(logits)</span></span><br><span class="line"><span class="comment"># 但这里计算交叉熵的函数直接输入logits即可</span></span><br><span class="line">predict_logits = tf.constant([[<span class="number">2.0</span>, <span class="number">-1.0</span>, <span class="number">3.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sparse_softmax_cross_entropy_with_logits函数计算交叉熵，即log perplexity的值</span></span><br><span class="line">loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=word_labels, logits=predict_logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算perplexity损失的值</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(loss))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[0.32656264 0.4643688 ]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax_cross_entropy_with_logits与上面的函数类似，但是需要将预测目标以概率分布的形式给出</span></span><br><span class="line">word_prob_distribution = tf.constant([[<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits(labels=word_prob_distribution, logits=predict_logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算perplexity损失的值</span></span><br><span class="line">print(sess.run(loss))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[0.32656264 0.4643688 ]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于softmax_cross_entropy_with_logits允许提供一个概率分布，因此在使用时有更大的自由度</span></span><br><span class="line"><span class="comment"># 一种叫label smoothing的技巧是将正确数据的概率设为一个比1.0略小的值，将错误数据的概率设为比0.0略大的值</span></span><br><span class="line"><span class="comment"># 这样可以避免模型与数据过拟合，在某些时候可以提高训练效果</span></span><br><span class="line">word_prob_smooth = tf.constant([[<span class="number">0.01</span>, <span class="number">0.01</span>, <span class="number">0.98</span>], [<span class="number">0.98</span>, <span class="number">0.01</span>, <span class="number">0.01</span>]])</span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits(labels=word_prob_smooth, logits=predict_logits)</span><br><span class="line">print(sess.run(loss))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[0.37656265 0.48936883]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="9-2-神经语言模型"><a href="#9-2-神经语言模型" class="headerlink" title="9.2 神经语言模型"></a>9.2 神经语言模型</h3><hr>
<ul>
<li><p>循环神经网络可以将任意长度的上文信息存储在隐藏状态中，因此使用循环神经网络作为语言模型有着天然的优势</p>
</li>
<li><p>RNN自然语言模型DEMO</p>
<ul>
<li>如图<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/RNN自然语言建模.jpg" alt=""></li>
<li>工作原理<ul>
<li>每个时刻的输入为一个句子中的单词wi</li>
<li>每个时刻的输出为一个概率分布，表示句子中下一个位置为不同单词的概率p(wi+1 | w1, w2, w3,…, wi)</li>
<li>比如<ul>
<li>第一个时刻输入的单词为“大海”，而输出为p(x|”大海”)</li>
<li>即知道第一个词为大海后，其他不同单词出现在下一个位置的概率</li>
<li>若p(“的”|”大海”)=0.8，则“大海”之后的单词为“的”出现的概率为0.8</li>
</ul>
</li>
<li>类似的，可以得到p(“颜色”|”大海”,”的”)、p(“是”|”大海”,”的”,”颜色”)、p(“蓝色|”大海”,”的”,”颜色”,”是”)</li>
<li>然后将这些概率取对数再平均起来，就可以得到在这个句子上计算的log perplexity<h4 id="9-2-1-PTB数据集的预处理"><a href="#9-2-1-PTB数据集的预处理" class="headerlink" title="9.2.1 PTB数据集的预处理"></a>9.2.1 PTB数据集的预处理</h4></li>
</ul>
</li>
</ul>
</li>
<li><p>背景:自然语言文本数据无法直接被当成数值提供给神经网络，所以需要对它进行预处理</p>
</li>
<li><p>PTB(Penn Treebank Dataset)文本数据集是目前语言模型学习中使用最为广泛的数据集</p>
</li>
<li><p>数据集中共含有9998个词汇，加上稀有词语的特殊符号<code>&lt;unk&gt;</code>和语句结束换行符<code>&lt;eos&gt;</code>在内，一共10000个词汇</p>
</li>
<li><p>为了将文本转化为模型可以读入的单词序列，需要将这10000个不同的词汇分别映射到0-9999之间的整数编号</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集数据文件</span></span><br><span class="line">RAW_DATA = <span class="string">"/PycharmProjects/TFDemo/data/PTB/data/ptb.train.txt"</span></span><br><span class="line"><span class="comment"># 输出的词汇表文件</span></span><br><span class="line">VOCAB_OUTPUT = <span class="string">"/PycharmProjects/TFDemo/data/PTB/generate/ptb.vocab"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计单词出现的频率</span></span><br><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">with</span> codecs.open(RAW_DATA, <span class="string">'r'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> line.strip().split():</span><br><span class="line">            counter[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照词频顺序对单词进行排序</span></span><br><span class="line">sorted_word_to_cnt = sorted(counter.items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">sorted_words = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> sorted_word_to_cnt]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入句子结束符&lt;eos&gt;</span></span><br><span class="line">sorted_words = [<span class="string">"&lt;eos&gt;"</span>] + sorted_words</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将排序好的单词写入输出文件，其中行号就代表了单词的编号</span></span><br><span class="line"><span class="keyword">with</span> codecs.open(VOCAB_OUTPUT, <span class="string">'w'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> file_output:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sorted_words:</span><br><span class="line">        file_output.write(word + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在确定了词汇表之后，再将训练文件、测试文件等都根据词汇文件转化为单词编号，每个单词编号就是它在词汇文件中的行号</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line">RAW_DATA = <span class="string">"/PycharmProjects/TFDemo/data/PTB/data/ptb.train.txt"</span></span><br><span class="line">VOCAB = <span class="string">"/PycharmProjects/TFDemo/data/PTB/generate/ptb.vocab"</span></span><br><span class="line">OUTPUT_DATA = <span class="string">"/PycharmProjects/TFDemo/data/PTB/generate/ptb.train"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取词汇表，并建立词汇到单词编号的映射</span></span><br><span class="line"><span class="keyword">with</span> codecs.open(VOCAB, <span class="string">"r"</span>, <span class="string">"utf-8"</span>) <span class="keyword">as</span> f_vocab:</span><br><span class="line">    vocab = [w.strip() <span class="keyword">for</span> w <span class="keyword">in</span> f_vocab.readlines()]</span><br><span class="line">word_to_code = &#123;k: v <span class="keyword">for</span> (k, v) <span class="keyword">in</span> zip(vocab, range(len(vocab)))&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开输入与输出文件</span></span><br><span class="line">fin = codecs.open(RAW_DATA, <span class="string">"r"</span>, <span class="string">"utf-8"</span>)</span><br><span class="line">fout = codecs.open(OUTPUT_DATA, <span class="string">"w"</span>, <span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据词汇映射进行转码</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">    words = line.strip().split() + [<span class="string">"&lt;eos&gt;"</span>]</span><br><span class="line">    out_line = <span class="string">' '</span>.join([str(word_to_code[w]) <span class="keyword">for</span> w <span class="keyword">in</span> words]) + <span class="string">'\n'</span></span><br><span class="line">    fout.write(out_line)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭文件</span></span><br><span class="line">fin.close()</span><br><span class="line">fout.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p>在实际工程中，通常使用TFRecords格式来提高读写效率</p>
</li>
</ul>
<hr>
<h4 id="9-2-2-PTB数据的batching方法"><a href="#9-2-2-PTB数据的batching方法" class="headerlink" title="9.2.2 PTB数据的batching方法"></a>9.2.2 PTB数据的batching方法</h4><ul>
<li><p>背景</p>
<ul>
<li>在文本数据中，由于每个句子的长度不同，又无法像图像一样调整到固定维度，因此batching时需要一些特殊操作</li>
<li>最常见的方法是使用填充(padding)，将同一batch内的句子长度补齐</li>
<li>在PTB数据集中，每个句子并非随机抽取的文本，而是在上下文之间有关联的内容</li>
<li>语言模型为了利用上下文信息，必须将前面句子的消息传递到后面的句子</li>
</ul>
</li>
<li><p>解决方案1</p>
<ul>
<li>简介：如果模型大小不限，最理想的设计是将整个文档前后连接起来，当作一个句子来训练</li>
<li>如图<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/将整个文档前后连接示意图.jpg" alt=""></li>
<li>问题<ul>
<li>这样会导致计算图过大</li>
<li>另外序列过长还会导致梯度爆炸问题</li>
</ul>
</li>
</ul>
</li>
<li><p>解决方案2</p>
<ul>
<li>简介<ul>
<li>将长序列切割为固定长度的子序列</li>
<li>循环神经网络在处理完一个子序列后，它最终的隐藏状态将复制到下一个序列作为初始值</li>
<li>这样，在前向计算时，效果等同于一次性顺序读取了整个文档</li>
<li>而在反向传播时，梯度则只会在每个子序列内部传播</li>
</ul>
</li>
<li>如图<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/按固定长度切分文档示意图.jpg" alt=""></li>
</ul>
</li>
<li><p>解决方案3</p>
<ul>
<li>背景<ul>
<li>为了利用计算时的并行能力</li>
<li>我们希望每一次计算可以对多个句子进行并行处理</li>
<li>同时又要尽量暴增batch之间的上下文连续</li>
</ul>
</li>
<li>简介<ul>
<li>先将整个文档切分为若干个连续段落</li>
<li>再让batch中的每个位置负责其中一段</li>
</ul>
</li>
<li>如图<br><img src="/2019/05/22/Tensorflow-9-自然语言处理/将长序列切分为batch示意图.jpg" alt=""></li>
</ul>
</li>
<li><p>下面的代码从文本文件中读取数据，并按照解决方案3将数据整理为batch</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">TRAIN_DATA = <span class="string">"/PycharmProjects/TFDemo/data/PTB/generate/ptb.train"</span></span><br><span class="line">TRAIN_BATCH_SIZE = <span class="number">20</span></span><br><span class="line">TRAIN_NUM_STEP = <span class="number">35</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从文件中读取数据，并返回包含单词编号的数组</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        <span class="comment"># 将整个文档读入一个长字符串</span></span><br><span class="line">        id_string = <span class="string">' '</span>.join([line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> fin.readlines()])</span><br><span class="line">    id_list = [int(w) <span class="keyword">for</span> w <span class="keyword">in</span> id_string.split()]</span><br><span class="line">    <span class="keyword">return</span> id_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_batches</span><span class="params">(id_list, batch_size, num_step)</span>:</span></span><br><span class="line">    <span class="comment"># 计算总的batch数量，每个batch包含的单词数量是batch_size * num_steps</span></span><br><span class="line">    num_batches = (len(id_list) <span class="number">-1</span>) // (batch_size * num_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据整理为一个维度为[batch_size, num_batches * num_step]的二维数组</span></span><br><span class="line">    data = np.array(id_list[:num_batches * batch_size * num_step])</span><br><span class="line">    data = np.reshape(data, [batch_size, num_batches * num_step])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 沿着第二个维度将数据切分为num_batches个batch，存入一个数组中</span></span><br><span class="line">    data_batches = np.split(data, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重复上述操作，但是每个位置向右移动一位</span></span><br><span class="line">    <span class="comment"># 作为输入数据的标记值，也就是RNN每一步输入所需要预测的下一个单词</span></span><br><span class="line">    label = np.array(id_list[<span class="number">1</span>: num_batches * batch_size * num_step + <span class="number">1</span>])</span><br><span class="line">    label = np.reshape(label, [batch_size, num_batches * num_step])</span><br><span class="line">    label_batches = np.split(label, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵</span></span><br><span class="line">    <span class="keyword">return</span> list(zip(data_batches, label_batches))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    train_batches = make_batches(read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line">    print(train_batches)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h4 id="9-2-3-基于循环神经网络的神经语言模型"><a href="#9-2-3-基于循环神经网络的神经语言模型" class="headerlink" title="9.2.3 基于循环神经网络的神经语言模型"></a>9.2.3 基于循环神经网络的神经语言模型</h4><ul>
<li><p>神经语言模型与第8章介绍的循环神经网络相比，多了两层：词向量层(embedding)和softmax层</p>
</li>
<li><p>词向量层</p>
<ul>
<li>词向量<ul>
<li>在输入层，每一个单词用一个实数向量表示，这个向量被称为“词向量”(word embedding)</li>
<li>词向量可以形象地理解为将词汇表嵌入到一个固定维度的实数空间中</li>
</ul>
</li>
<li>将单词编号转化为词向量的用处<ol>
<li>降低输入的维度<ul>
<li>如果不使用词向量层，而直接将单词以one-hot vector的形式输入循环神经网络，那么输入的维数大小将与词汇表大小相同</li>
<li>而词向量的维度通常小于词汇表大小</li>
<li>这将大大减少循环神经网络的参数数量与计算量</li>
</ul>
</li>
<li>增加语义信息<ul>
<li>简单的单词编号不包含任何语义信息<ul>
<li>两个单词之间编号相近，并不代表它们的含义有任何关联</li>
</ul>
</li>
<li>词向量层将稀疏的编号转化为稠密的向量表示，这使得词向量有可能包含更加丰富的信息</li>
<li>在自然语言应用中学习得到的词向量通常将含义相似的词赋予取值相近的词向量，使得上层的网络可以更容易地抓住相似单词之间的共性</li>
</ul>
</li>
</ol>
</li>
<li>词向量层的定义代码  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">embedding = tf.get_variable(<span class="string">"embedding"</span>, [VOCAB_SIZE, EMB_SIZE])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出的矩阵比输入矩阵多一个维度，新增维度的大小为EMB_SIZE</span></span><br><span class="line"><span class="comment"># 在语言模型中，一般input_data的维度为batch_size * num_steps</span></span><br><span class="line"><span class="comment"># 在经过词向量层之后, 维度为batch_size * num_steps * EMB_SIZE</span></span><br><span class="line">input_embedding = tf.nn.embedding_lookup(embedding, input_data)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Softmax层</p>
<ul>
<li>作用：将循环神经网络的输出转化为一个单词表中每个单词的输出概率</li>
<li>步骤<ol>
<li>使用一个线性映射将循环神经网络的输出映射为一个维度与词汇表大小相同的向量，这一步输出叫做logits</li>
<li>调用softmax方法将logits转化为加和为1的概率<ul>
<li>事实上，语言模型的每一步输出都可以看作一个分类问题，在VOCAB_SIZE个可能的类别中决定这一步最可能输出的单词</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>通过共享参数减少参数数量</p>
<ul>
<li>Softmax层和词向量层的参数数量都与词汇表大小VOCAB_SIZE成正比</li>
<li>由于VOCAB_SIZE的数值通常较大，而HIDDEN_SIZE相对较小，导致softmax和embdding在整个网络的参数数量中占有很大比例</li>
<li>有研究指出，如果共享词向量层和Softmax层的参数，不仅能大幅度减少参数数量，还能提高最终模型结果</li>
</ul>
</li>
<li><p>完整代码</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">TRAIN_DATA = <span class="string">'/PycharmProjects/TFDemo/data/PTB/generate/ptb.train'</span>  <span class="comment"># 训练数据路径</span></span><br><span class="line">EVAL_DATA = <span class="string">'/PycharmProjects/TFDemo/data/PTB/generate/ptb.valid'</span>   <span class="comment"># 验证数据路径</span></span><br><span class="line">TEST_DATA = <span class="string">'/PycharmProjects/TFDemo/data/PTB/generate/ptb.test'</span>    <span class="comment"># 测试数据路径</span></span><br><span class="line"></span><br><span class="line">HIDDEN_SIZE = <span class="number">300</span>                                                   <span class="comment"># 隐藏层规模</span></span><br><span class="line">NUM_LAYERS = <span class="number">2</span>                                                      <span class="comment"># 深层循环网络中LSTM结构的层数</span></span><br><span class="line">VOCAB_SIZE = <span class="number">10000</span>                                                  <span class="comment"># 词典规模</span></span><br><span class="line">TRAIN_BATCH_SIZE = <span class="number">20</span>                                               <span class="comment"># 训练数据batch的大小</span></span><br><span class="line">TRAIN_NUM_STEP = <span class="number">35</span>                                                 <span class="comment"># 训练数据截断长度,可以理解为单词序列的长度</span></span><br><span class="line">EVAL_BATCH_SIZE = <span class="number">1</span>                                                 <span class="comment"># 测试数据batch的大小</span></span><br><span class="line">EVAL_NUM_STEP = <span class="number">1</span>                                                   <span class="comment"># 测试数据截断长度</span></span><br><span class="line"></span><br><span class="line">NUM_EPOCH = <span class="number">5</span>                                                       <span class="comment"># 使用训练数据的轮数</span></span><br><span class="line">LSTM_KEEP_PROB = <span class="number">0.9</span>                                                <span class="comment"># LSTM节点不被dropout的概率</span></span><br><span class="line">EMBEDDING_KEEP_PROB = <span class="number">0.9</span>                                           <span class="comment"># 词向量不被dropout的概率</span></span><br><span class="line">MAX_GRAD_NORM = <span class="number">5</span>                                                   <span class="comment"># 用于控制梯度膨胀的梯度大小上限</span></span><br><span class="line">SHARE_EMB_AND_SOFTMAX = <span class="keyword">True</span>                                        <span class="comment"># 在Softmax层和词向量层之间共享参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用PTBModel类定义整个模型的结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PTBModel</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, is_training, batch_size, num_steps)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录使用的batch大小和截断长度</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义每一步的输入和预期输出</span></span><br><span class="line">        <span class="comment"># 两者的维度都为[batch_size, num_steps]</span></span><br><span class="line">        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])</span><br><span class="line">        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义使用LSTM结构为循环体结构且使用dropout的深层循环神经网络</span></span><br><span class="line">        drop_keep_prob = LSTM_KEEP_PROB <span class="keyword">if</span> is_training <span class="keyword">else</span> <span class="number">1.0</span></span><br><span class="line">        lstm_cells = [</span><br><span class="line">            tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">                tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),</span><br><span class="line">                output_keep_prob=drop_keep_prob</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)</span><br><span class="line">        ]</span><br><span class="line">        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化最初的状态，即全0的向量</span></span><br><span class="line">        <span class="comment"># 这个量只在每个epoch初始化第一个batch时使用</span></span><br><span class="line">        self.initial_state = cell.zero_state(batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义词向量矩阵</span></span><br><span class="line">        embedding = tf.get_variable(<span class="string">"embedding"</span>, [VOCAB_SIZE, HIDDEN_SIZE])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输入单词转化为词向量</span></span><br><span class="line">        inputs = tf.nn.embedding_lookup(embedding, self.input_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只在训练中使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义输出列表，这里先将不同时刻LSTM结构的输出收集起来，再一起提供给sotfmax层</span></span><br><span class="line">        outputs = []</span><br><span class="line">        state = self.initial_state</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"RNN"</span>):</span><br><span class="line">            <span class="keyword">for</span> time_step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">                <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">                    tf.get_variable_scope().reuse_variables()</span><br><span class="line">                cell_output, state = cell(inputs[:, time_step, :], state)</span><br><span class="line">                outputs.append(cell_output)</span><br><span class="line">        output = tf.reshape(tf.concat(outputs, <span class="number">1</span>), [<span class="number">-1</span>, HIDDEN_SIZE])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Softmax层:将RNN在每个位置上的输入转化为各个单词的logits</span></span><br><span class="line">        <span class="keyword">if</span> SHARE_EMB_AND_SOFTMAX:</span><br><span class="line">            weight = tf.transpose(embedding)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            weight = tf.get_variable(<span class="string">"weight"</span>, [HIDDEN_SIZE, VOCAB_SIZE])</span><br><span class="line">        bias = tf.get_variable(<span class="string">"bias"</span>, [VOCAB_SIZE])</span><br><span class="line">        logits = tf.matmul(output, weight) + bias</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义交叉熵损失函数和平均损失</span></span><br><span class="line">        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(self.targets, [<span class="number">-1</span>]), logits=logits)</span><br><span class="line">        self.cost = tf.reduce_sum(loss) / batch_size</span><br><span class="line">        self.final_state = state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只在训练模型时定义反向传播操作</span></span><br><span class="line">        trainable_variables = tf.trainable_variables()</span><br><span class="line">        <span class="comment"># 控制梯度大小不能超过MAX_GRAD_NORM</span></span><br><span class="line">        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)</span><br><span class="line">        <span class="comment"># 定义优化方法</span></span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义训练步骤</span></span><br><span class="line">        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用给定的模型model在数据data上运行train_op并返回在全部数据上的perplexity值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(sess, model, batches, train_op, output_log, step)</span>:</span></span><br><span class="line">    <span class="comment"># 计算平均perplexity的辅助变量</span></span><br><span class="line">    total_costs = <span class="number">0.0</span></span><br><span class="line">    iters = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 将state初始化为全0</span></span><br><span class="line">    state = sess.run(model.initial_state)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练一个epoch</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> batches:</span><br><span class="line">        <span class="comment"># 在当前batch上运行train_op并计算损失值</span></span><br><span class="line">        <span class="comment"># 交叉熵损失函数计算的就是下一个单词为给定单词的概率</span></span><br><span class="line">        cost, state, _ = sess.run(</span><br><span class="line">            [model.cost, model.final_state, train_op],</span><br><span class="line">            &#123;model.input_data: x, model.targets: y, model.initial_state: state&#125;</span><br><span class="line">        )</span><br><span class="line">        total_costs += cost</span><br><span class="line">        iters += model.num_steps</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_log <span class="keyword">and</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %d steps, perplexity is %.3f"</span> % (step, np.exp(cost / model.num_steps)))</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> step, np.exp(total_costs / iters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从文件中读取数据，并返回包含单词编号的数组</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        <span class="comment"># 将整个文档读入一个长字符串</span></span><br><span class="line">        id_string = <span class="string">' '</span>.join([line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> fin.readlines()])</span><br><span class="line">    id_list = [int(w) <span class="keyword">for</span> w <span class="keyword">in</span> id_string.split()]</span><br><span class="line">    <span class="keyword">return</span> id_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将包含单词编号的数组进行batching</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_batches</span><span class="params">(id_list, batch_size, num_step)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算总的batch数量，每个batch包含的单词数量是batch_size * num_steps</span></span><br><span class="line">    num_batches = (len(id_list) <span class="number">-1</span>) // (batch_size * num_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据整理为一个维度为[batch_size, num_batches * num_step]的二维数组</span></span><br><span class="line">    data = np.array(id_list[:num_batches * batch_size * num_step])</span><br><span class="line">    data = np.reshape(data, [batch_size, num_batches * num_step])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 沿着第二个维度将数据切分为num_batches个batch，存入一个数组中</span></span><br><span class="line">    data_batches = np.split(data, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重复上述操作，但是每个位置向右移动一位</span></span><br><span class="line">    <span class="comment"># 作为输入数据的标记值，也就是RNN每一步输入所需要预测的下一个单词</span></span><br><span class="line">    label = np.array(id_list[<span class="number">1</span>: num_batches * batch_size * num_step + <span class="number">1</span>])</span><br><span class="line">    label = np.reshape(label, [batch_size, num_batches * num_step])</span><br><span class="line">    label_batches = np.split(label, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵</span></span><br><span class="line">    <span class="keyword">return</span> list(zip(data_batches, label_batches))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义初始化函数</span></span><br><span class="line">    initializer = tf.random_uniform_initializer(<span class="number">-0.05</span>, <span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义训练用的循环神经网络模型</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"language_model"</span>, reuse=<span class="keyword">None</span>, initializer=initializer):</span><br><span class="line">        train_model = PTBModel(<span class="keyword">True</span>, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义测试用的循环神经网络模型，它与train_model共用参数，但是没有dropout</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"language_model"</span>, reuse=<span class="keyword">True</span>, initializer=initializer):</span><br><span class="line">        eval_model = PTBModel(<span class="keyword">False</span>, EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 初始化变量</span></span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="comment"># 初始化训练集、验证集和测试集数据</span></span><br><span class="line">        train_batches = make_batches(read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line">        eval_batches = make_batches(read_data(EVAL_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line">        test_batches = make_batches(read_data(TEST_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line"></span><br><span class="line">        step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 一共要用所有训练数据NUM_EPOCH次，以提高训练精度</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_EPOCH):</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"In iteration: %d"</span> % (i + <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># 先运行训练过程</span></span><br><span class="line">            step, train_pplx = run_epoch(sess, train_model, train_batches, train_model.train_op, <span class="keyword">True</span>, step)</span><br><span class="line">            print(<span class="string">"Epoch: %d Train Perplexity: %.3f"</span> % (i + <span class="number">1</span>, train_pplx))</span><br><span class="line">            <span class="comment"># 运行验证过程</span></span><br><span class="line">            _, eval_pplx = run_epoch(sess, eval_model, eval_batches, tf.no_op(), <span class="keyword">False</span>, <span class="number">0</span>)</span><br><span class="line">            print(<span class="string">"Epoch: %d Eval Perplexity: %.3f"</span> % (i + <span class="number">1</span>, eval_pplx))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化结束后，运行测试过程</span></span><br><span class="line">        _, test_pplx = run_epoch(sess, eval_model, test_batches, tf.no_op(), <span class="keyword">False</span>, <span class="number">0</span>)</span><br><span class="line">        print(<span class="string">"Test Perplexity: %.3f"</span> % test_pplx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">In iteration: 1</span></span><br><span class="line"><span class="string">After 0 steps, perplexity is 9971.553</span></span><br><span class="line"><span class="string">After 100 steps, perplexity is 781.005</span></span><br><span class="line"><span class="string">After 200 steps, perplexity is 590.538</span></span><br><span class="line"><span class="string">After 300 steps, perplexity is 348.273</span></span><br><span class="line"><span class="string">After 400 steps, perplexity is 400.334</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">After 6400 steps, perplexity is 66.869</span></span><br><span class="line"><span class="string">After 6500 steps, perplexity is 62.698</span></span><br><span class="line"><span class="string">After 6600 steps, perplexity is 72.749</span></span><br><span class="line"><span class="string">Epoch: 5 Train Perplexity: 72.026</span></span><br><span class="line"><span class="string">Epoch: 5 Eval Perplexity: 106.827</span></span><br><span class="line"><span class="string">Test Perplexity: 103.619</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/24/C-3-处理数据/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/24/C-3-处理数据/" itemprop="url">[C++][3][处理数据]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-24T19:22:34+08:00">
                2019-03-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/C/" itemprop="url" rel="index">
                    <span itemprop="name">C++</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/24/C-3-处理数据/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/24/C-3-处理数据/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第-3-章-处理数据"><a href="#第-3-章-处理数据" class="headerlink" title="第 3 章 处理数据"></a>第 3 章 处理数据</h2><p>内置的 C++ 类型分两组: 基本类型和复合类型</p>
<p>但是没有任何一种整型和浮点型能够满足所有编程要求，所以对于这两种数据有很多变体</p>
<h3 id="3-1-简单变量"><a href="#3-1-简单变量" class="headerlink" title="3.1 简单变量"></a>3.1 简单变量</h3><p>程序必须记录信息的三个基本属性</p>
<ul>
<li><p>信息将存储在哪里</p>
</li>
<li><p>要存储什么值</p>
</li>
<li><p>存储何种类型的信息</p>
</li>
</ul>
<h4 id="3-1-1-变量名"><a href="#3-1-1-变量名" class="headerlink" title="3.1.1 变量名"></a>3.1.1 变量名</h4><p>C++ 提倡使用一定含义的变量名</p>
<p>必须遵循几种简单的C++ 命名规范</p>
<ol>
<li>在名称中只能使用字母字符、数字和下划线(_)</li>
<li>名称的第一个字符不能是数字</li>
<li>区分大写字符和小写字符</li>
<li>不能将 C++ 关键字用作名称</li>
<li>以两个下划线或下划线和大写字母开头的名称被保留给实现使用，以一个下划线开头的名称被保留给实现，用作全局标识符(使用这种命名变量不会导致编译器错误，但是会导致程序的不确定性)</li>
<li>C++ 对于名称的长度没有限制，名称中所有的字符都有意义，但有些平台有长度限制(C只保证前63个字符有意义)</li>
</ol>
<p>下面是一些有效和无效的 C++ 名称</p>
<p><img src="/2019/03/24/C-3-处理数据/0324_0.png" alt=""></p>
<p>如果想用两个或者更多的单词组成一个变量名称，有两种做法</p>
<ol>
<li><p>使用下划线将单词分开(C、python等语言如是)</p>
</li>
<li><p>从第二个单词开始将每个单词的第一个字母大写(Pascal, java等语言如是)</p>
</li>
</ol>
<h4 id="3-1-2-整型"><a href="#3-1-2-整型" class="headerlink" title="3.1.2 整型"></a>3.1.2 整型</h4><p>不同的 C++ 整型使用不同的内存量来存储整数，使用的内存量越大，可以表示的整数值范围也越大。</p>
<p>有的类型有正负值之分，有的类型不能表示负值</p>
<p>C++ 的基本整型分别是<code>char</code>、<code>short</code>、<code>int</code>、<code>long</code>和<code>long long</code></p>
<h4 id="3-1-3-short、int、long和long-long"><a href="#3-1-3-short、int、long和long-long" class="headerlink" title="3.1.3 short、int、long和long long"></a>3.1.3 short、int、long和long long</h4><p>C++ 的<code>short</code>、<code>int</code>、<code>long</code>和<code>long long</code>类型通过使用不同数目的位来存储值</p>
<p>但在实际生活中，这些类型长度因计算机而异，但是通常遵循一定的标准</p>
<ul>
<li><code>short</code>至少16位</li>
<li><code>int</code>至少与<code>short</code>一样长</li>
<li><code>long</code>至少32位，且至少与<code>int</code>一样长</li>
<li><code>long long</code>至少64位，且至少与<code>long</code>一样长</li>
</ul>
<p>要知道系统中整数的最大长度，可以使用C++工具来检查类型的长度</p>
<ul>
<li>使用<code>sizeof</code>运算符返回类型和变量的长度，单位为字节</li>
<li>头文件<code>climit</code>中包含了关于整型限制的信息，例如: <code>INT_MAX</code> 为 <code>int</code> 的最大取值，<code>CHAR_BIT</code>为字节的位数</li>
</ul>
<blockquote>
<p><strong>符号常量–预处理方式</strong><br>在C++编译过程中，首先将源代码传递给预处理器。<code>#define</code>与<code>#include</code>一样，也是一个预处理编译指令。该指令告诉预处理器：在程序中查找<code>INT_MAX</code>，将所有的<code>INT_MAX</code>都替换成<code>32767</code></p>
</blockquote>
<h4 id="3-1-4-变量初始化"><a href="#3-1-4-变量初始化" class="headerlink" title="3.1.4 变量初始化"></a>3.1.4 变量初始化</h4><p>C++ 的变量初始化基本与 C 相同，但是有几种方式是C ++ 独有的<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">wrens</span><span class="params">(<span class="number">432</span>)</span></span>;<span class="comment">//将 432 赋值给 wrens</span></span><br><span class="line"><span class="keyword">int</span> wrens = &#123;<span class="number">432</span>&#125;;<span class="comment">//将 432 赋值给 wrens</span></span><br><span class="line"><span class="keyword">int</span> wrens&#123;<span class="number">432</span>&#125;;<span class="comment">//将 432 赋值给 wrens</span></span><br><span class="line"><span class="keyword">int</span> wrens = &#123;&#125;;<span class="comment">//wrens被初始化为0</span></span><br><span class="line"><span class="keyword">int</span> wrens&#123;&#125;;<span class="comment">//wrens被初始化为0</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>如果不对函数内部定义的变量进行初始化，该变量的值将是不确定的。这意味着该变量的值将是它被创建之前，相应内存单元保存的值</p>
</blockquote>
<h4 id="3-1-5-C-的无符号类型和整型的溢出"><a href="#3-1-5-C-的无符号类型和整型的溢出" class="headerlink" title="3.1.5 C++的无符号类型和整型的溢出"></a>3.1.5 C++的无符号类型和整型的溢出</h4><p>无符号类型可以增大变量能够储存的最大值(因为省略了一位存储符号)</p>
<p>C++ 的整型溢出时，行为很像里程表。如果超越了界限，其值将为范围另一端的取值</p>
<p><img src="/2019/03/24/C-3-处理数据/0324_1.png" alt=""></p>
<h4 id="3-1-6-选择整型类型"><a href="#3-1-6-选择整型类型" class="headerlink" title="3.1.6 选择整型类型"></a>3.1.6 选择整型类型</h4><p>通常，<code>int</code>被设置为对目标计算机而言最为自然的长度。自然长度指的是计算机处理起来效率最高的长度</p>
<h4 id="3-1-7-整形字面量"><a href="#3-1-7-整形字面量" class="headerlink" title="3.1.7 整形字面量"></a>3.1.7 整形字面量</h4><p>通常 <code>C++</code> 可以以三种不同的计数方式来书写整数: 基数为10、基数为8和基数为16。</p>
<ul>
<li><p>使用<code>0</code>来标识8位数</p>
</li>
<li><p>使用<code>0x</code>来标识16位数</p>
</li>
</ul>
<p>这些表示方式仅仅是为了表达上的方便。不管以何种方式书写字面量，都将被存储为二进制</p>
<p><code>iostream</code>提供了<code>dec</code>、<code>hex</code>、<code>oct</code>分别用于指示<code>cout</code>以十进制、十六进制和八进制表示整数</p>
<h4 id="3-1-8-char类型-字符和小整数"><a href="#3-1-8-char类型-字符和小整数" class="headerlink" title="3.1.8 char类型: 字符和小整数"></a>3.1.8 char类型: 字符和小整数</h4><p>编程语言通过使用字母的数值编码解决了储存字母的问题，<code>char</code>类型是另一种整型。它足够长，可以表示目标计算机系统中的所有基本符号(所有字母、数字、标点符号等)</p>
<p><code>C++</code> 实现使用的是其主机系统的编码。在美国，最常用的符号集是 <code>ASCII</code> 字符集。但是 <code>ASCII</code> 字符集很难满足国际化的需求。为了满足国际化需求，<code>C++</code> 支持的宽字符类型可以存储更多的值，如 <code>Unicode</code> 字符集使用的值</p>
<p>值的类型将引导cout选择如何显示值，例如将<code>char</code>显示为字符，将<code>int</code>显示为数字</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/20/C-2-开始学习C/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/20/C-2-开始学习C/" itemprop="url">[C++][2][开始学习C++]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-20T19:57:05+08:00">
                2019-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/C/" itemprop="url" rel="index">
                    <span itemprop="name">C++</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/20/C-2-开始学习C/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/20/C-2-开始学习C/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第-2-章-开始学习-C"><a href="#第-2-章-开始学习-C" class="headerlink" title="第 2 章 开始学习 C++"></a>第 2 章 开始学习 C++</h2><p>C++对大小写敏感，区分大写字符和小写字符</p>
<h3 id="2-1-进入-C"><a href="#2-1-进入-C" class="headerlink" title="2.1 进入 C++"></a>2.1 进入 C++</h3><p>HelloWorld程序<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Come up and C++ me some time."</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"You won't regret it!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="2-1-1-main函数"><a href="#2-1-1-main函数" class="headerlink" title="2.1.1 main函数"></a>2.1.1 main函数</h4><p>通常 C++ 程序必须包含一个 <code>main()</code> 函数，作为程序的入口</p>
<p>存在一些例外条件，如 Windows 中的动态链接库(Dynamic-link library, DLL)，这是其他 Windows 程序可以使用的代码，不是独立的程序，所以不需要 <code>main()</code></p>
<h4 id="2-1-2-C-注释"><a href="#2-1-2-C-注释" class="headerlink" title="2.1.2 C++注释"></a>2.1.2 C++注释</h4><p>编译器忽略一切注释</p>
<p>两种注释的写法</p>
<ul>
<li><p>C++ 的单行注释以双斜杠(<code>\\</code>)打头</p>
</li>
<li><p>C++ 的多行注释采用C风格的注释<code>/*leave your comments here*/</code></p>
</li>
</ul>
<h4 id="2-1-3-C-预处理器与iostream文件"><a href="#2-1-3-C-预处理器与iostream文件" class="headerlink" title="2.1.3 C++预处理器与iostream文件"></a>2.1.3 C++预处理器与iostream文件</h4><p>C++ 预处理器</p>
<ul>
<li><p>预处理器在进行主编译之前对源文件进行处理，这点与 C 一样</p>
</li>
<li><p>它处理以 <code>#</code> 开头的编译指令</p>
</li>
<li><p>不必执行特殊操作来调用该预处理器，它会在编译程序时自动运行</p>
</li>
<li><p>例如: <code>#include &lt;iostream&gt;</code> 指令，导致 iostream 文件中的内容随源代码文件的内容一起被发送给编译器。也就是说 iostream 文件的内容将取代程序中的代码行 <code>#include &lt;iostream&gt;</code>。</p>
</li>
</ul>
<p>头文件</p>
<ul>
<li><p>头文件又叫做包含文件</p>
</li>
<li><p>C++ 编译器自带了很多头文件，每个头文件都支持一组特定的工具</p>
</li>
<li><p>通常，C++ 头文件没有扩展名</p>
</li>
<li><p>头文件命名约定如下图</p>
</li>
</ul>
<p><img src="/2019/03/20/C-2-开始学习C/0320_0.png" alt=""></p>
<h4 id="2-1-4-名称空间"><a href="#2-1-4-名称空间" class="headerlink" title="2.1.4 名称空间"></a>2.1.4 名称空间</h4><p>名称空间，旨在让您在编写大型程序以及多个厂商现有的代码组合起来的程序时更容易，它还有助于组织程序。</p>
<p>名称空间让厂商能够将其产品封装在一个叫做名称空间的单元中，这样就可以用名称空间的名称来指出想使用哪个厂商的产品</p>
<p>C++ 的标准组件都被放置到 <code>std</code> 名称空间中</p>
<h4 id="2-1-5-C-源代码风格"><a href="#2-1-5-C-源代码风格" class="headerlink" title="2.1.5 C++源代码风格"></a>2.1.5 C++源代码风格</h4><ol>
<li><p>每个语句占一行</p>
</li>
<li><p>每个函数都有一个开始花括号和一个结束花括号，这两个花括号各占一行</p>
</li>
<li><p>函数中的语句都相对于花括号进行缩进</p>
</li>
<li><p>与函数名称相关的圆括号周围没有空白</p>
</li>
</ol>
<h3 id="2-2-C-语句"><a href="#2-2-C-语句" class="headerlink" title="2.2 C++ 语句"></a>2.2 C++ 语句</h3><ol>
<li><p>声明语句: 定义函数中使用的变量的名称和类型</p>
</li>
<li><p>赋值语句: 使用赋值运算符( <code>=</code> )给变量进行赋值</p>
</li>
<li><p>消息语句: 将消息发送给对象，激发某种活动</p>
</li>
<li><p>函数调用: 执行函数。被调用的函数执行完毕后，程序返回到函数调用语句后面的语句</p>
</li>
<li><p>函数原型: 声明函数的返回类型、函数接受的参数数量和类型</p>
</li>
<li><p>返回语句: 将一个值从被调用的函数那里返回到调用函数中</p>
</li>
</ol>
<h3 id="2-3-函数"><a href="#2-3-函数" class="headerlink" title="2.3 函数"></a>2.3 函数</h3><h4 id="2-3-1-函数的组成"><a href="#2-3-1-函数的组成" class="headerlink" title="2.3.1 函数的组成"></a>2.3.1 函数的组成</h4><p>一个函数由函数原型和函数定义两部分组成</p>
<ul>
<li><p>函数原型</p>
<ul>
<li>函数原型描述了函数接口，即函数如何与程序的其他部分交互</li>
<li>在使用函数前，C++编译器必须知道函数的参数类型和返回值类型，函数原型负责提供这些信息</li>
<li>原型以分号结尾，省略函数体</li>
</ul>
</li>
<li><p>函数定义: 负责定义函数，包含函数的代码</p>
</li>
</ul>
<p>C 和 C++ 将库函数的这两项特性(原型和定义)分开了，库文件中包含了函数的编译代码，而头文件中包含了原型</p>
<h4 id="2-3-2-函数定义的格式"><a href="#2-3-2-函数定义的格式" class="headerlink" title="2.3.2 函数定义的格式"></a>2.3.2 函数定义的格式</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">type <span class="title">function_name</span><span class="params">(argument_list)</span></span>&#123;</span><br><span class="line">    statements</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意: C++不允许将函数定义嵌套到另一个函数中。所有函数都是平等的。java也能在方法中定义子方法，而 python 则可以。</p>
</blockquote>
<h4 id="2-3-3-函数的特性"><a href="#2-3-3-函数的特性" class="headerlink" title="2.3.3 函数的特性"></a>2.3.3 函数的特性</h4><ul>
<li><p>有函数头或函数体</p>
</li>
<li><p>接受0~n个参数</p>
</li>
<li><p>返回0~1个值</p>
</li>
<li><p>需要一个原型</p>
</li>
</ul>
<h3 id="2-4-关键字"><a href="#2-4-关键字" class="headerlink" title="2.4 关键字"></a>2.4 关键字</h3><p>关键字在C++中构成特殊的语言成分，如 <code>return</code>、<code>int</code>、<code>while</code>等都是关键字</p>
<p>关键字不能用作它用</p>
<blockquote>
<p>注意: <code>main</code> 不是关键字，因为它不是语言的组成部分。然而它是一个必不可少函数的名称。因此最好不要随意使用 <code>main</code></p>
</blockquote>
<h3 id="2-5-类与对象"><a href="#2-5-类与对象" class="headerlink" title="2.5 类与对象"></a>2.5 类与对象</h3><p>类</p>
<ul>
<li><p>类是用户定义的一种数据类型</p>
</li>
<li><p>要定义类，需要描述它能够表示什么信息和可对数据执行哪些操作</p>
</li>
</ul>
<p>对象</p>
<ul>
<li><p>对象是根据类描述创建的实体</p>
</li>
<li><p>例如: <code>cout</code> 是 <code>ostream</code> 类的实体</p>
</li>
</ul>
<p>C++向对象发送消息的两种方法</p>
<ol>
<li><p>使用类方法(本质上是函数调用)</p>
</li>
<li><p>重新定义运算符</p>
</li>
</ol>
<blockquote>
<p>在 java 中，无法重新定义运算符，所以我们经常写出，<code>objectA.equal(objectB)</code> 这类的代码，James Gosling当时这样做好像是为了降低 java 语言的编程难度。而在 python 中，我们可以通过在类中实现一些”双下”方法，例如 <code>__eq()__</code> 等，使这个类支持运算符。</p>
</blockquote>
<h3 id="2-6-一个简单的包含函数调用的例子"><a href="#2-6-一个简单的包含函数调用的例子" class="headerlink" title="2.6 一个简单的包含函数调用的例子"></a>2.6 一个简单的包含函数调用的例子</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">stonetolb</span><span class="params">(<span class="keyword">int</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line">    <span class="keyword">int</span> stone;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Enter the weight in stone: "</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; stone;</span><br><span class="line">    <span class="keyword">int</span> pounds = stonetolb(stone);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; stone &lt;&lt; <span class="string">" stone = "</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; pounds &lt;&lt; <span class="string">" pounds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">stonetolb</span><span class="params">(<span class="keyword">int</span> sts)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">14</span> * sts;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/C-1-预备知识/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/19/C-1-预备知识/" itemprop="url">[C++][1][预备知识]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-19T19:03:31+08:00">
                2019-03-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/C/" itemprop="url" rel="index">
                    <span itemprop="name">C++</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/19/C-1-预备知识/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/19/C-1-预备知识/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第-1-章-预备知识"><a href="#第-1-章-预备知识" class="headerlink" title="第 1 章 预备知识"></a>第 1 章 预备知识</h2><p>它在 C 语言的基础上添加了对<strong>面向对象编程</strong>和<strong>泛型编程</strong>的支持</p>
<p>C++ 继承了 C 语言高效、简洁、快速和可移植性的传统</p>
<p>C++ 面向对象的特性带來了全新的编程方法，这种方法是为了应付复杂程度不断提商的现代编程任务而设计的</p>
<p>C++ 的模板特性提供了一种全新的编程方法—–泛型编程。</p>
<h3 id="1-1-C-简介"><a href="#1-1-C-简介" class="headerlink" title="1.1 C++ 简介"></a>1.1 C++ 简介</h3><p>C++ 融合了 3 种不同的编程方式</p>
<ul>
<li>C 语言代表的过程性语言</li>
<li>C++ 在 C 语言基础上添加的类代表的面向对象语言</li>
<li>C++ 模板支持的泛型编程</li>
</ul>
<h3 id="1-2-C-简史"><a href="#1-2-C-简史" class="headerlink" title="1.2 C++ 简史"></a>1.2 C++ 简史</h3><p>体积更大、功能更强的计算机引出了更大、更复杂的程序，而这些程序在程序管理和维护方面带来了新的问题</p>
<p>C 和 Pascal 引领人们进入了结构化编程时代，这种机制把秩序和规程带进了迫切需要这种性质的领域中。</p>
<p>人们也见证了一种新编程模式的成长：面向对象编程（OOP)。SmallTalk 和 C++ 语言具备这种功能。</p>
<h4 id="1-2-1-C-语言"><a href="#1-2-1-C-语言" class="headerlink" title="1.2.1 C 语言"></a>1.2.1 C 语言</h4><p>1970年代，贝尔实验室的 Dennis Ritchie 致力于开发 UNIX 操作系统。为完成这项工作，Ritchie 需要一种语言，它必须简洁，能够生成简洁、快速的程序，并能有效地控制硬件。</p>
<p>一种被称为编译器的特殊程序将高级语言翻译成特定计算机的内部语言。这样就可以通过每个平台使用不同的编译器来在不同的平台上使用同一个高级语言程序了。</p>
<p>Ritchie希望有一种语言能将低级语言的效率、硬件访问能力和高级语言的通性、可移植性融合在一起，于是他在旧语言的基础上开发了C语言。</p>
<h4 id="1-2-2-C-语言编程原理"><a href="#1-2-2-C-语言编程原理" class="headerlink" title="1.2.2 C 语言编程原理"></a>1.2.2 C 语言编程原理</h4><p>一般来说，计算机语言要处理两个概念—–<strong>数据和算法</strong>。数据是程序使用和处理的信息，算法是程序使用的方法</p>
<p>过程化编程首先要确定计算机应采取的操作，然后使用编程语言来实现这些操作。程序命令计算机按照一系列流程生成特定的结果，就像菜谱指定了厨师做蛋糕时应遵循的一系列步骤一样。</p>
<p>C语言具有<strong>结构化编程</strong>的特性，结构化编程将分支限制为一小组行为良好的结构。C语言的词汇表中就包含了这些结构(<code>for</code> 循环、<code>while</code>循环、<code>do while</code> 循环和 <code>if else</code>语句)</p>
<p>另一个新原则是<strong>自顶向下</strong>（top-down) 的设计。在 C 语言中，其理念是将大型程序分解为小型、便于管理的任务</p>
<h4 id="1-2-3-面向对象编程"><a href="#1-2-3-面向对象编程" class="headerlink" title="1.2.3 面向对象编程"></a>1.2.3 面向对象编程</h4><p>与强调算法的过程性编程不同的是，OOP<strong>强调</strong>的是<strong>数椐</strong>。OOP不像过程性编程那样，试图使问题满足语言的过程性方法，而是试图让语言满足问题的要求。</p>
<p>在 C++ 中，类是一种规范，它描述了这种新型数据格式，对象是根据这种规范构造的特定数据结构。</p>
<p>通常，类规定了可使用哪些数据来表示对象以及可以对这些数据执行哪些操作。</p>
<h4 id="1-2-4-C-与泛型编程"><a href="#1-2-4-C-与泛型编程" class="headerlink" title="1.2.4 C++ 与泛型编程"></a>1.2.4 C++ 与泛型编程</h4><p>术语泛型是指创建独立于类型的代码</p>
<p>泛型编程提供了执行常见任务(如数据排序或合并链表)的工具</p>
<h4 id="1-2-5-C-的起源"><a href="#1-2-5-C-的起源" class="headerlink" title="1.2.5 C++ 的起源"></a>1.2.5 C++ 的起源</h4><p>C++ 也是在贝尔实验室诞生的， Bjarne Stroustrup于 20 世纪 80 年代在这里开发<br>出了这种语言。</p>
<blockquote>
<p>C++ 主要是为了我的朋友和我不必再使用汇编语言、C 语言或其他现代高级语言来编程而设计的。它的主要功能是可以更方便地编写出好程序，让每个程序员更加快乐。(Bjarne Stroustrup)</p>
</blockquote>
<p>名称 C++ 来自 C 语言中的递增运算符，该运算符将变量加1。名称 C++ 表明，它是 C 的扩充版本。</p>
<p>有些 OOP 正统派把为 C 添加 OOP 特性看作是<strong>为猪插上翅膀</strong>，虽然这是头瘦骨嶙峋、非常能干的猪</p>
<p><img src="/2019/03/19/C-1-预备知识/0319_0.png" alt=""></p>
<h3 id="1-3-可移植性和标准"><a href="#1-3-可移植性和标准" class="headerlink" title="1.3 可移植性和标准"></a>1.3 可移植性和标准</h3><p>可移植性的定义：如果在不修改代码的情况下，重新编译程序后，程序将运行良好，那么该程序是可移植的。</p>
<p>可移植性的两大障碍</p>
<ol>
<li><p>硬件。硬件特定的程序是不可移植的</p>
</li>
<li><p>语言上的差异，虽然多数实现都希望其 C++ 版本与其他版本兼容，但如果没有准确描述语言工作方式的公开标准，这将很难做到</p>
</li>
</ol>
<p>C++的第一个国际标准 : C++ 98</p>
<h3 id="1-4-程序创建的技巧"><a href="#1-4-程序创建的技巧" class="headerlink" title="1.4 程序创建的技巧"></a>1.4 程序创建的技巧</h3><p>程序创建的步骤</p>
<ol>
<li><p>使用文本编辑器<strong>编辑</strong>程序。并将其保存到文件中，这个文件就是程序的<strong>源代码</strong></p>
</li>
<li><p><strong>编译</strong>源代码。将源代码翻译为主机使用的机器语言(<strong>目标代码</strong>)</p>
</li>
<li><p>将目标代码与其他代码<strong>链接</strong>起来，例如一些库函数，最终生成<strong>可执行代码</strong></p>
</li>
</ol>
<p><img src="/2019/03/19/C-1-预备知识/0319_1.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/16/流畅的Python-2-序列构成的数组/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/16/流畅的Python-2-序列构成的数组/" itemprop="url">[流畅的Python][2][序列构成的数组]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-16T20:45:52+08:00">
                2019-03-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/流畅的Python/" itemprop="url" rel="index">
                    <span itemprop="name">流畅的Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/16/流畅的Python-2-序列构成的数组/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/16/流畅的Python-2-序列构成的数组/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第2章-序列构成的数组"><a href="#第2章-序列构成的数组" class="headerlink" title="第2章 序列构成的数组"></a>第2章 序列构成的数组</h2><blockquote>
<p>你可能注意到了，之前提到的几个操作可以无差别地应用于文本、列表和表格上，我们把文本、列表和表格叫做<strong>数据火车</strong>，for命令通常能作用于数据火车上。—Geurts、Meertens和Pemberton, ABC Programmer’s Handbook</p>
</blockquote>
<ul>
<li><p>ABC语言很多点子在现在看起来都很有Python风格</p>
<ul>
<li>序列的泛型操作</li>
<li>内置的元组和映射类型</li>
<li>用缩进来架构的源码</li>
<li>无需变量声明的强类型</li>
<li>用同一的风格处理序列数据</li>
</ul>
</li>
<li><p>Python从ABC继承了用统一的风格处理序列数据这一特点</p>
<ul>
<li>无论哪种数据结构，字符串、列表、字节序列、数组、XML元素，亦或是数据库查询结果</li>
<li>它们都共用一套丰富的操作：迭代、切片、排序，还有拼接</li>
</ul>
</li>
</ul>
<h3 id="2-1-内置序列类型概览"><a href="#2-1-内置序列类型概览" class="headerlink" title="2.1 内置序列类型概览"></a>2.1 内置序列类型概览</h3><ul>
<li><p>第一种分类方法</p>
<ul>
<li>容器序列<ul>
<li>简介<ul>
<li>存放的是不同类型的数据</li>
<li>即存放它们所包含的任意类型的对象的引用</li>
</ul>
</li>
<li>例如：list、tuple和collections.deque等</li>
</ul>
</li>
<li>扁平序列<ul>
<li>简介<ul>
<li>存放的是相同类型的数据</li>
<li>扁平序列其实是一段连续的内存空间，更加紧凑</li>
<li>但是只能存放诸如字符、字节和数值这种基础类型</li>
</ul>
</li>
<li>例如：str、bytes、bytearray、memoryview和array.array</li>
</ul>
</li>
</ul>
</li>
<li><p>第二种分类方法</p>
<ul>
<li>可变序列<ul>
<li>例如：list、bytearray、array.array、collections.deque和memoryview</li>
</ul>
</li>
<li>不可变序列<ul>
<li>例如：tuple、str和bytes</li>
</ul>
</li>
<li>可变序列与不可变序列的差别和联系<ul>
<li>如图<br><img src="/2019/03/16/流畅的Python-2-序列构成的数组/0316_6.png" alt=""></li>
<li>可变序列从不可变序列中继承了一些方法</li>
<li>同时可变序列又自己实现了一些方法</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-2-列表推导和生成器表达式"><a href="#2-2-列表推导和生成器表达式" class="headerlink" title="2.2 列表推导和生成器表达式"></a>2.2 列表推导和生成器表达式</h3><ul>
<li><p>列表推导：是构建列表的快捷方式，它异常强大，但相关语法比较晦涩</p>
</li>
<li><p>生成器表达式：具有生成各种类型的元素并用它们来填充序列的功能</p>
</li>
</ul>
<h4 id="2-2-1-列表推导与和可读性"><a href="#2-2-1-列表推导与和可读性" class="headerlink" title="2.2.1 列表推导与和可读性"></a>2.2.1 列表推导与和可读性</h4><ul>
<li><p>列表推导：列表推导可以帮助我们把一个序列或是其他可迭代类型中的元素过滤或是加工，然后新建一个列表</p>
</li>
<li><p>比较使用列表推导和不使用列表推导代码的可读性</p>
<ul>
<li><p>把一个字符串变成Unicode码位的列表</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">symbols = <span class="string">'αλνοηπ'</span></span><br><span class="line">codes = []</span><br><span class="line"><span class="keyword">for</span> symbol <span class="keyword">in</span> symbols:</span><br><span class="line">    codes.append(ord(symbol))</span><br><span class="line">print(codes)</span><br><span class="line"><span class="comment"># [945, 955, 957, 959, 951, 960]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>把字符串变成Unicode码位的另外一种写法</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">symbols = <span class="string">'αλνοηπ'</span></span><br><span class="line">codes = [ord(symbol) <span class="keyword">for</span> symbol <span class="keyword">in</span> symbols]</span><br><span class="line">print(codes)</span><br><span class="line"><span class="comment"># [945, 955, 957, 959, 951, 960]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>使用列表推导的原则</p>
<ul>
<li>只用列表推导来创建新的列表，并且尽量保持简短</li>
<li>如果列表推导过于复杂，要考虑用for循环重写</li>
</ul>
</li>
<li><p>Python3中列表推导不存在变量泄漏的问题</p>
<ul>
<li>解释<ul>
<li>列表推导、生成器表达式、结合推导和字典推导在Python3中都有了自己的局部作用域</li>
<li>表达式内部的变量和赋值只在局部起作用</li>
<li>表达式的上下文里的同名变量还可以正常引用，局部变量并不会影响它们</li>
</ul>
</li>
<li>举例  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="string">'ABC'</span></span><br><span class="line">dummy = [ord(x) <span class="keyword">for</span> x <span class="keyword">in</span> x]</span><br><span class="line">print(x)        <span class="comment"># ABC</span></span><br><span class="line">print(dummy)    <span class="comment"># [65, 66, 67]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="2-2-2-列表推导同filter和map的比较"><a href="#2-2-2-列表推导同filter和map的比较" class="headerlink" title="2.2.2 列表推导同filter和map的比较"></a>2.2.2 列表推导同filter和map的比较</h4><ul>
<li><p>举例：用列表推导和map/filter组合来创建同样的表单</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">symbols = <span class="string">'αλνοηπ'</span></span><br><span class="line">codes = [ord(symbol) <span class="keyword">for</span> symbol <span class="keyword">in</span> symbols <span class="keyword">if</span> ord(symbol) &gt; <span class="number">127</span>]</span><br><span class="line">print(codes)</span><br></pre></td></tr></table></figure>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">symbols = <span class="string">'αλνοηπ'</span></span><br><span class="line">codes = list(filter(<span class="keyword">lambda</span> c: c &gt; <span class="number">127</span>, map(ord, symbols)))</span><br><span class="line">print(codes)</span><br></pre></td></tr></table></figure>
</li>
<li><p>总结</p>
<ul>
<li>filter和map合起来能做的事，列表推导也能做，而且还不需要借助难以理解和阅读的lambda表达式</li>
<li>而且map/filter组合起来的效果不一定比列表推导快</li>
</ul>
</li>
</ul>
<h4 id="2-2-3-笛卡尔积"><a href="#2-2-3-笛卡尔积" class="headerlink" title="2.2.3 笛卡尔积"></a>2.2.3 笛卡尔积</h4><ul>
<li><p>笛卡尔积</p>
<ul>
<li>简介：两个或以上的列表中的元素对构成的<strong>元组</strong>，这些元组构成的<strong>列表</strong>就是笛卡尔积</li>
<li>举例如图<br><img src="/2019/03/16/流畅的Python-2-序列构成的数组/0316_3.jpg" alt=""></li>
</ul>
</li>
<li><p>使用列表推导计算笛卡尔积</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">colors = [<span class="string">'black'</span>, <span class="string">'white'</span>]</span><br><span class="line">sizes = [<span class="string">'S'</span>, <span class="string">'M'</span>, <span class="string">'L'</span>]</span><br><span class="line"></span><br><span class="line">tshirts = [(color, size) <span class="keyword">for</span> color <span class="keyword">in</span> colors <span class="keyword">for</span> size <span class="keyword">in</span> sizes]</span><br><span class="line">print(tshirts)</span><br><span class="line"><span class="comment"># 此处先按照颜色排列，再按照尺码排列</span></span><br><span class="line"><span class="comment"># [('black', 'S'), ('black', 'M'), ('black', 'L'), ('white', 'S'), ('white', 'M'), ('white', 'L')]</span></span><br><span class="line"></span><br><span class="line">tshirts = [(color, size) <span class="keyword">for</span> size <span class="keyword">in</span> sizes <span class="keyword">for</span> color <span class="keyword">in</span> colors]</span><br><span class="line">print(tshirts)</span><br><span class="line"><span class="comment"># 此处先按照尺码排列，再按照颜色排列</span></span><br><span class="line"><span class="comment"># [('black', 'S'), ('white', 'S'), ('black', 'M'), ('white', 'M'), ('black', 'L'), ('white', 'L')]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-2-4-生成器表达式"><a href="#2-2-4-生成器表达式" class="headerlink" title="2.2.4 生成器表达式"></a>2.2.4 生成器表达式</h4><ul>
<li><p>生成器表达式</p>
<ul>
<li>具有生成各种类型的元素并用它们来填充序列的功能</li>
<li>生成器表达式背后遵守了迭代器协议，可以逐个地产出元素，而不是先建立一个完整的列表，然后再把这个列表传递到某个构造函数里</li>
<li>这样可以大幅节省内存</li>
</ul>
</li>
<li><p>生成器表达式语法与列表推导差不多，只不过把方括号换成了圆括号</p>
</li>
<li><p>如何用生成器表达式建立元组和数组</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> array</span><br><span class="line"></span><br><span class="line">symbols = <span class="string">'black'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果生成器表达式是要给函数调用过程中唯一的参数，那么不需要额外再用括号把它围起来</span></span><br><span class="line">symboltuple = tuple(ord(symbol) <span class="keyword">for</span> symbol <span class="keyword">in</span> symbols)</span><br><span class="line">print(symboltuple)</span><br><span class="line"><span class="comment"># (98, 108, 97, 99, 107)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># array的构造方法需要两个参数，因此括号是必须的</span></span><br><span class="line">symbolarray = array.array(<span class="string">'I'</span>, (ord(symbol) <span class="keyword">for</span> symbol <span class="keyword">in</span> symbols))</span><br><span class="line">print(symbolarray)</span><br><span class="line"><span class="comment"># array('I', [98, 108, 97, 99, 107])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>使用生成器表达式计算笛卡尔积</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">colors = [<span class="string">'black'</span>, <span class="string">'white'</span>]</span><br><span class="line">sizes = [<span class="string">'S'</span>, <span class="string">'M'</span>, <span class="string">'L'</span>]</span><br><span class="line"><span class="keyword">for</span> tshirt <span class="keyword">in</span> (<span class="string">'%s %s'</span> % (c, s) <span class="keyword">for</span> c <span class="keyword">in</span> colors <span class="keyword">for</span> s <span class="keyword">in</span> sizes):</span><br><span class="line">    print(tshirt)</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成器表达式最大的优点：逐个产出元素，可以节省内存</p>
</li>
</ul>
<h3 id="2-3-元组不仅仅是不可变的列表"><a href="#2-3-元组不仅仅是不可变的列表" class="headerlink" title="2.3 元组不仅仅是不可变的列表"></a>2.3 元组不仅仅是不可变的列表</h3><h4 id="2-3-1-元组和记录"><a href="#2-3-1-元组和记录" class="headerlink" title="2.3.1 元组和记录"></a>2.3.1 元组和记录</h4><ul>
<li><p>元组其实是对数据的记录：元组中的每个元素都存放了记录中一个字段的数据，外加这个字段的位置，正是这个位置信息给数据赋予了意义</p>
</li>
<li><p>如果在任何的表达式里我们在元组内对元素排序，这些元素所携带的信息就会丢失，因为这些信息是跟它们的位置有关的</p>
</li>
<li><p>把元组用作记录</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 洛杉矶国际机场的经纬度信息</span></span><br><span class="line">lax_coordinates = (<span class="number">33.9425</span>, <span class="number">-118.408056</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 东京市的一些信息：市名、年份、人口、人口变化率和面积</span></span><br><span class="line">city, year, pop, chg, area = (<span class="string">'Tokyo'</span>, <span class="number">2003</span>, <span class="number">32450</span>, <span class="number">0.66</span>, <span class="number">8014</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个元组列表，元组的形式为(country_code, passport_number)</span></span><br><span class="line">traveler_ids = [(<span class="string">'USA'</span>, <span class="string">'31195855'</span>), (<span class="string">'BRA'</span>, <span class="string">'CE342567'</span>), (<span class="string">'ESP'</span>, <span class="string">'XDA205856'</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在迭代过程中，passport变量被绑定到每个元组上</span></span><br><span class="line"><span class="keyword">for</span> passport <span class="keyword">in</span> sorted(traveler_ids):</span><br><span class="line">    print(<span class="string">'%s/%s'</span> % passport)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for循环可以分别提取元组中的元素，也叫做拆包</span></span><br><span class="line"><span class="comment"># 因为元组中的第二个元素对当前情景没有什么用，所以它赋值给"_"占位符</span></span><br><span class="line"><span class="keyword">for</span> country, _ <span class="keyword">in</span> traveler_ids:</span><br><span class="line">    print(country)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">BRA/CE342567</span></span><br><span class="line"><span class="string">ESP/XDA205856</span></span><br><span class="line"><span class="string">USA/31195855</span></span><br><span class="line"><span class="string">USA</span></span><br><span class="line"><span class="string">BRA</span></span><br><span class="line"><span class="string">ESP</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-3-2-元组拆包"><a href="#2-3-2-元组拆包" class="headerlink" title="2.3.2 元组拆包"></a>2.3.2 元组拆包</h4><ul>
<li><p>元组拆包举例</p>
<ol>
<li><p>平行赋值：把一个可迭代对象中的元素，一并赋值到由对应的变量组成的元组中</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 平行赋值demo</span></span><br><span class="line">lax_coordinates = (<span class="number">33.9425</span>, <span class="number">-118.408056</span>)</span><br><span class="line"></span><br><span class="line">latitude, longitude = lax_coordinates</span><br><span class="line"></span><br><span class="line">print(latitude)</span><br><span class="line">print(longitude)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">33.9425</span></span><br><span class="line"><span class="string">-118.408056</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>不使用中间变量交换两个变量的值</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b, a = a, b</span><br></pre></td></tr></table></figure>
</li>
<li><p>用<code>*</code>运算符把一个可迭代对象拆开作为函数的参数</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">print(divmod(<span class="number">20</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">t = (<span class="number">20</span>, <span class="number">8</span>)</span><br><span class="line"><span class="comment"># 使用*运算符将元组拆开作为函数的参数</span></span><br><span class="line">print(divmod(*t))</span><br><span class="line"></span><br><span class="line">quotient, remainder = divmod(*t)</span><br><span class="line">print(quotient)</span><br><span class="line">print(remainder)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">(2, 4)</span></span><br><span class="line"><span class="string">(2, 4)</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">4</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>使用元组拆包接收函数以元组形式返回的多个值</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用元组拆包接收函数以元组形式返回的多个值</span></span><br><span class="line">_, filename = os.path.split(<span class="string">'C:/Users/admin/Desktop/NoTe/FluentPython-note/ch2-序列构成的数组/3-元组不仅仅是不可变列表.md'</span>)</span><br><span class="line">print(filename)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">3-元组不仅仅是不可变列表.md</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在拆包时，我们不总是对元组里所有的数据都感兴趣，_占位符可以帮助处理这种情况</li>
</ul>
</li>
</ol>
</li>
<li><p>元组拆包注意</p>
<ul>
<li>元组拆包可以应用到任何可迭代对象上</li>
<li>唯一的硬性要求是：被迭代对象中的元素个数必须要跟接受这些元素的元组的空挡数一致</li>
<li>除非我们用<code>*</code>来表示忽略多余的元素</li>
</ul>
</li>
<li><p>用<code>*</code>处理剩下的元素</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a, b, *rest = range(<span class="number">5</span>)</span><br><span class="line">print(a, b, rest)</span><br><span class="line"></span><br><span class="line">a, b, *rest = range(<span class="number">3</span>)</span><br><span class="line">print(a, b, rest)</span><br><span class="line"></span><br><span class="line">a, b, *rest = range(<span class="number">2</span>)</span><br><span class="line">print(a, b, rest)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">0 1 [2, 3, 4]</span></span><br><span class="line"><span class="string">0 1 [2]</span></span><br><span class="line"><span class="string">0 1 []</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在平行赋值时，*前缀最多只能用于一个变量，但是这个变量可以出现在赋值表达式的任意位置</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a, b, *body, d = range(<span class="number">5</span>)</span><br><span class="line">print(a, b, body, d)</span><br><span class="line"></span><br><span class="line">*head, b, c, d = range(<span class="number">5</span>)</span><br><span class="line">print(head, b, c, d)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">0 1 [2, 3] 4</span></span><br><span class="line"><span class="string">[0, 1] 2 3 4</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-3-3-嵌套元组拆包"><a href="#2-3-3-嵌套元组拆包" class="headerlink" title="2.3.3 嵌套元组拆包"></a>2.3.3 嵌套元组拆包</h4><ul>
<li><p>接受表达式的元组可以是嵌套式的，例如(a, b, (c, d))</p>
</li>
<li><p>只要这个接受元组的嵌套结构符合表达式本身的嵌套结构，Python就可以做出正确的对应</p>
</li>
<li><p>嵌套元组Demo</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个元组内有4个元素，其中最后一个元素是一对坐标</span></span><br><span class="line">metro_areas = [</span><br><span class="line">    (<span class="string">'Tokyo'</span>, <span class="string">'JP'</span>, <span class="number">36.933</span>, (<span class="number">35.689722</span>, <span class="number">139.691667</span>)),</span><br><span class="line">    (<span class="string">'Delhi NCR'</span>, <span class="string">'IN'</span>, <span class="number">21.935</span>, (<span class="number">28.613889</span>, <span class="number">77.208889</span>)),</span><br><span class="line">    (<span class="string">'Mexico City'</span>, <span class="string">'MX'</span>, <span class="number">20.142</span>, (<span class="number">19.433333</span>, <span class="number">-99.133333</span>)),</span><br><span class="line">    (<span class="string">'New York-Newark'</span>, <span class="string">'US'</span>, <span class="number">20.104</span>, (<span class="number">40.808611</span>, <span class="number">-74.020386</span>)),</span><br><span class="line">    (<span class="string">'Sao Paulo'</span>, <span class="string">'BR'</span>, <span class="number">19.649</span>, (<span class="number">-23.547778</span>, <span class="number">-46.635833</span>))</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'&#123;:15&#125;|&#123;:^9&#125;|&#123;:^9&#125;'</span>.format(<span class="string">''</span>,<span class="string">'lat.'</span>,<span class="string">'long.'</span>))</span><br><span class="line">fmt = <span class="string">'&#123;:15&#125;|&#123;:9.4f&#125;|&#123;:9.4f&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把输入元组的最后一个元素拆包到由变量构成的元组里，这样就获得了坐标</span></span><br><span class="line"><span class="keyword">for</span> name, cc, pop, (latitude, longitude) <span class="keyword">in</span> metro_areas:</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这个条件用来把输出限定在西半球的城市</span></span><br><span class="line">    <span class="keyword">if</span> longitude &lt;= <span class="number">0</span>:</span><br><span class="line">        print(fmt.format(name, latitude, longitude))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">               |  lat.   |  long.  </span></span><br><span class="line"><span class="string">Mexico City    |  19.4333| -99.1333</span></span><br><span class="line"><span class="string">New York-Newark|  40.8086| -74.0204</span></span><br><span class="line"><span class="string">Sao Paulo      | -23.5478| -46.6358</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>注意</p>
<ul>
<li>在Python3之前，元组可以作为形参放到函数声明中，例如<code>def fn(a, (b, c), d):</code></li>
<li>然而Python3不再支持这种格式</li>
</ul>
</li>
</ul>
<h4 id="2-3-4-具名元组"><a href="#2-3-4-具名元组" class="headerlink" title="2.3.4 具名元组"></a>2.3.4 具名元组</h4><ul>
<li><p><code>collections.namedtuple</code>是一个工厂函数，它可以用来构建一个带字段名的元组和一个有名字的类</p>
</li>
<li><p><code>namedtuple</code>创建的类的大小</p>
<ul>
<li>用namedtuple创建的类的实例所消耗的内存跟元组是一样的，因为字段名都被存在对应的类中</li>
<li>但是这个实例跟普通的对象实例比起来要小一些，因为Python不会用<code>__dict__</code>来存放这些实例的属性</li>
</ul>
</li>
<li><p>用具名元组来记录一个城市的信息</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个具名元组需要两个参数，一个是类名，另一个是类的各个字段的名字</span></span><br><span class="line"><span class="comment"># 后者可以是由多个字符串组成的可迭代对象，或者是由空格分隔开的字段名组成的字符串</span></span><br><span class="line">City = namedtuple(<span class="string">'City'</span>, <span class="string">'name country population coordinates'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存放在对应字段中的数据要以一串参数的形式传入构造函数中</span></span><br><span class="line">tokyo = City(<span class="string">'Tokyo'</span>, <span class="string">'JP'</span>, <span class="number">36.933</span>, (<span class="number">35.689722</span>, <span class="number">139.691667</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以通过字段名或者位置来获得一个字段的信息</span></span><br><span class="line">print(tokyo)</span><br><span class="line">print(tokyo.population)</span><br><span class="line">print(tokyo.country)</span><br><span class="line">print(tokyo[<span class="number">1</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">City(name='Tokyo', country='JP', population=36.933, coordinates=(35.689722, 139.691667))</span></span><br><span class="line"><span class="string">36.933</span></span><br><span class="line"><span class="string">JP</span></span><br><span class="line"><span class="string">JP</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>具名元组还有一些自己专有的属性</p>
<ul>
<li>例如：<code>_fields</code>类属性、类方法<code>_make(iterable)</code>、实例方法<code>_asdict()</code><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个具名元组需要两个参数，一个是类名，另一个是类的各个字段的名字</span></span><br><span class="line"><span class="comment"># 后者可以是由多个字符串组成的可迭代对象，或者是由空格分隔开的字段名组成的字符串</span></span><br><span class="line">City = namedtuple(<span class="string">'City'</span>, <span class="string">'name country population coordinates'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fields属性是一个包含这个类所有字段名称的元组</span></span><br><span class="line">print(City._fields)</span><br><span class="line"></span><br><span class="line">LatLong = namedtuple(<span class="string">"LatLong"</span>, <span class="string">"lat long"</span>)</span><br><span class="line">delhi_data = (<span class="string">'Delhi NCR'</span>, <span class="string">'IN'</span>, <span class="number">21.935</span>, LatLong(<span class="number">28.613889</span>, <span class="number">77.208889</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用_make()通过接受一个可迭代对象来生成这个类的一个实例，它的作用与City(*delhi_data)是一样的</span></span><br><span class="line">delhi = City._make(delhi_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># _asdict()可以把剧名元组以collections.OrderedDict的形式返回，我们可以利用它来把元组里的信息友好地呈现出来</span></span><br><span class="line">print(delhi._asdict())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> delhi._asdict().items():</span><br><span class="line">    print(key + <span class="string">": "</span>, value)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="2-3-5-作为不可变列表的元组"><a href="#2-3-5-作为不可变列表的元组" class="headerlink" title="2.3.5 作为不可变列表的元组"></a>2.3.5 作为不可变列表的元组</h4><ul>
<li>除了跟增减元素相关的方法之外，元组支持列表的其他所有方法<br><img src="/2019/03/16/流畅的Python-2-序列构成的数组/0316_4.jpg" alt=""><br><img src="/2019/03/16/流畅的Python-2-序列构成的数组/0316_5.jpg" alt=""></li>
</ul>
<h3 id="2-4-切片"><a href="#2-4-切片" class="headerlink" title="2.4 切片"></a>2.4 切片</h3><ul>
<li>在Python中，像列表(list)、元组(tuple)、字符串(str)这类序列类型都支持切片操作</li>
</ul>
<h4 id="2-4-1-为什么切片和区间会忽略最后一个元素"><a href="#2-4-1-为什么切片和区间会忽略最后一个元素" class="headerlink" title="2.4.1 为什么切片和区间会忽略最后一个元素"></a>2.4.1 为什么切片和区间会忽略最后一个元素</h4><ul>
<li><p>在切片和区间操作里不包含区间范围的最后一个元素是Python的风格</p>
</li>
<li><p>切片和区间忽略最后一个元素的好处</p>
<ol>
<li>当只有最后一个位置消息时，我们可以快速看出切片和区间里有几个元素<ul>
<li>比如：range(3)和my_list[:3]都返回3个元素</li>
</ul>
</li>
<li>当起止位置信息都可见时，我们可以快速计算出切片和区间的长度，用后一个数减去第一个下标(stop - start)即可</li>
<li>这样做也可以让我们可以利用任意一个下标来把序列分割成不重叠的两部分<ul>
<li>只要写成my_list[:x]和my_list[x:]即可</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="2-4-2-切片对象"><a href="#2-4-2-切片对象" class="headerlink" title="2.4.2 切片对象"></a>2.4.2 切片对象</h4><ul>
<li><p>一个众所周知的秘密是，我们还可以用s[a:b:c]的形式对s在a和b之间以c为间隔取值</p>
<ul>
<li>例如  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">'bicycle'</span></span><br><span class="line">print(s[::<span class="number">3</span>])</span><br><span class="line">print(s[::<span class="number">-1</span>])</span><br><span class="line">print(s[::<span class="number">-2</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">bye</span></span><br><span class="line"><span class="string">elcycib</span></span><br><span class="line"><span class="string">eccb</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>可以使用<code>slice()</code>数据类型声明切片变量，这样可以增加代码可读性</p>
<ul>
<li>代码略</li>
</ul>
</li>
</ul>
<h4 id="2-4-3-多维切片和省略"><a href="#2-4-3-多维切片和省略" class="headerlink" title="2.4.3 多维切片和省略"></a>2.4.3 多维切片和省略</h4><ul>
<li><p>[]运算符还可以使用以逗号分开的多个索引或者是切片</p>
<ul>
<li>例如,<code>numpy.ndarray</code>就可以用<code>a[i, j]</code>这种形式来获取二维矩阵中的元素</li>
<li>也可以使用<code>a[m:n, k:l]</code>的方式来得到二维切片</li>
</ul>
</li>
<li><p>Python内置的序列类型都是一维的，因此它们只支持单一的索引，成对出现的索引大部分在外部库中使用，例如Numpy等</p>
</li>
</ul>
<ul>
<li>另外，在Numpy中, … 用作多维数组切片的快捷方式<ul>
<li>比如，如果x是四维数组，那么x[i,…]就是x[i, :, :, :]的缩写</li>
</ul>
</li>
</ul>
<h4 id="2-4-4-给切片赋值"><a href="#2-4-4-给切片赋值" class="headerlink" title="2.4.4 给切片赋值"></a>2.4.4 给切片赋值</h4><ul>
<li><p>如果把切片放在赋值语句的左边，或者把它作为del操作的对象，我们就可以对序列进行嫁接、切除或就地修改操作</p>
</li>
<li><p>注意：如果赋值的对象是一个切片的话，那么赋值语句的右侧必须是一个可迭代对象，即便只有单独一个值，也要转换成可迭代序列</p>
</li>
<li><p>例如</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">l = list(range(<span class="number">10</span>))</span><br><span class="line">print(l)</span><br><span class="line"><span class="comment">#  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line">l[<span class="number">2</span>: <span class="number">5</span>] = [<span class="number">20</span>, <span class="number">30</span>]</span><br><span class="line">print(l)</span><br><span class="line"><span class="comment"># [0, 1, 20, 30, 5, 6, 7, 8, 9]</span></span><br><span class="line"><span class="keyword">del</span> l[<span class="number">5</span>: <span class="number">7</span>]</span><br><span class="line">print(l)</span><br><span class="line"><span class="comment"># [0, 1, 20, 30, 5, 8, 9]</span></span><br><span class="line">l[<span class="number">3</span>::<span class="number">2</span>] = [<span class="number">11</span>, <span class="number">22</span>]</span><br><span class="line">print(l)</span><br><span class="line"><span class="comment"># [0, 1, 20, 11, 5, 22, 9]</span></span><br><span class="line">l[<span class="number">2</span>:<span class="number">5</span>] = [<span class="number">100</span>]</span><br><span class="line">print(l)</span><br><span class="line"><span class="comment"># [0, 1, 100, 22, 9]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-5-对序列使用-和"><a href="#2-5-对序列使用-和" class="headerlink" title="2.5 对序列使用+和*"></a>2.5 对序列使用+和*</h3><hr>
<ul>
<li><p>Python程序员会默认序列是支持<code>+</code>和<code>*</code>操作的</p>
</li>
<li><p>通常<code>+</code>号两侧的序列由相同类型的数据所构成</p>
</li>
<li><p>在拼接过程中，两个被操作的序列都不会被修改，Python会<strong>新建</strong>一个包含同样类型数据的序列来作为拼接的结果</p>
</li>
<li><p>如果想把一个序列复制几份然后再拼接起来，更快捷的做法是把这个序列乘以一个整数，同样这个操作也会产生一个<strong>新序列</strong></p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">print(l * <span class="number">5</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h4 id="2-5-1-建立由列表组成的列表"><a href="#2-5-1-建立由列表组成的列表" class="headerlink" title="2.5.1 建立由列表组成的列表"></a>2.5.1 建立由列表组成的列表</h4><ul>
<li>有时我们需要初始化一个嵌套着几个列表的列表</li>
</ul>
<ol>
<li><p>方案一：使用列表推导</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">board = [[<span class="string">'_'</span>] * <span class="number">3</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">print(board)</span><br><span class="line">board[<span class="number">1</span>][<span class="number">2</span>] = <span class="string">'X'</span></span><br><span class="line">print(board)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]</span></span><br><span class="line"><span class="string">[['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>方案二：一种错的方法，此时列表中嵌套的三个子列表全部指向同一对象</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">board = [[<span class="string">'_'</span>] * <span class="number">3</span>] * <span class="number">3</span></span><br><span class="line">print(board)</span><br><span class="line">board[<span class="number">1</span>][<span class="number">2</span>] = <span class="string">'X'</span></span><br><span class="line">print(board)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]</span></span><br><span class="line"><span class="string">[['_', '_', 'X'], ['_', '_', 'X'], ['_', '_', 'X']]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>方案三：等价于方案一，只是不够简洁</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">board = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    <span class="comment"># 每次迭代都新建一个列表(row)，作为新的一行追加到游戏板(board)</span></span><br><span class="line">    row = [<span class="string">'_'</span>] * <span class="number">3</span></span><br><span class="line">    board.append(row)</span><br><span class="line">print(board)</span><br><span class="line">board[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">'X'</span></span><br><span class="line">print(board)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]</span></span><br><span class="line"><span class="string">[['_', '_', '_'], ['_', '_', '_'], ['X', '_', '_']]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>方案四：等价于方案二，所以也是错误的</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">row = [<span class="string">'_'</span>] * <span class="number">3</span></span><br><span class="line">board = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    <span class="comment"># 追加同一个行对象(row)3次到游戏板(board)</span></span><br><span class="line">    board.append(row)</span><br><span class="line">print(board)</span><br><span class="line">board[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">'X'</span></span><br><span class="line">print(board)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]</span></span><br><span class="line"><span class="string">[['X', '_', '_'], ['X', '_', '_'], ['X', '_', '_']]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="2-6-序列的增量赋值"><a href="#2-6-序列的增量赋值" class="headerlink" title="2.6 序列的增量赋值"></a>2.6 序列的增量赋值</h3><ul>
<li><p>增量赋值运算符 <code>+=</code> 和 <code>*=</code> 的表现取决于它们的第一个操作对象</p>
</li>
<li><p>+=背后的特殊方法是<code>__iadd__</code>(用于’就地加法’)，工作原理如下</p>
<ul>
<li>例如<code>a += b</code></li>
<li>如果a实现了<code>__iadd__</code>方法，就调用这个方法</li>
<li>同时对于可变序列(list等)，a会就地改变，就像调用<code>a.extend(b)</code>一样</li>
<li>如果a没有实现<code>__iadd__</code>方法，<code>a += b</code>这个表达式的效果就变得与<code>a = a + b</code>一样了<ul>
<li>首先计算 <code>a + b</code>，得到一个新的对象</li>
<li>然后赋值给<code>a</code></li>
</ul>
</li>
</ul>
</li>
<li><p>总体来说，可变序列一般都实现了<code>__iadd__</code>方法，而不可变序列根本不支持这个操作，对这个方法的实现也无从谈起</p>
</li>
<li><p>但是不可变序列是支持 <code>+=</code> 运算符的，它只是把这个运算当作<code>a = a + b</code>处理</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">row = tuple(i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>))</span><br><span class="line">print(row)</span><br><span class="line">row += (<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">print(row)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">(0, 1, 2)</span></span><br><span class="line"><span class="string">(0, 1, 2, 4, 5)</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-7-list-sort方法和内置函数sorted"><a href="#2-7-list-sort方法和内置函数sorted" class="headerlink" title="2.7 list.sort方法和内置函数sorted"></a>2.7 list.sort方法和内置函数sorted</h3><ul>
<li><p><code>list.sort</code></p>
<ul>
<li><code>list.sort</code>方法会就地排序列表，也就是说不会把原列表复制一份</li>
<li>所以这个方法的返回值为<code>None</code>，提醒你本方法不会新建一个列表</li>
<li>在这个情况下返回<code>None</code>其实是Python的一个惯例<ul>
<li>如果一个函数或则好方法对对象进行的是就地改动，那它就应该返回<code>None</code></li>
<li>这样调用者就知道传入的参数发生了改动，而不是产生新的对象</li>
</ul>
</li>
<li>用返回None来表示就地改动有一个缺点<ul>
<li>调用者无法将其串联起来</li>
</ul>
</li>
</ul>
</li>
<li><p><code>sorted</code></p>
<ul>
<li><code>sorted</code>方法会新建一个列表作为返回值</li>
<li>这个方法接受<strong>任何形式的可迭代对象</strong>作为参数，甚至包括不可变序列或者生成器</li>
<li>不管<code>sorted</code>接受的是什么参数，都将返回一个列表</li>
</ul>
</li>
<li><p><code>list.sort</code>和<code>sorted</code>方法都有两个可选的关键字参数</p>
<ul>
<li><code>reserve</code><ul>
<li>如果被设置为<code>True</code>，被排序的序列里的元素都会以降序输出</li>
</ul>
</li>
<li><code>key</code><ul>
<li>这个参数需要传入一个只有一个参数的函数</li>
<li>这个函数会被用到序列里的每一个元素，所产生的结果将是排序算法依赖的对比关键字</li>
<li>比如<ul>
<li>key=str.lower可以实现忽略大小写的排序</li>
<li>key=len进行基于字符串长度的排序</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>sortDemo</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">fruits = [<span class="string">'grape'</span>, <span class="string">'raspberry'</span>, <span class="string">'apple'</span>, <span class="string">'banana'</span>]</span><br><span class="line">print(sorted(fruits))</span><br><span class="line"><span class="comment"># ['apple', 'banana', 'grape', 'raspberry']</span></span><br><span class="line">print(fruits)</span><br><span class="line"><span class="comment"># ['grape', 'raspberry', 'apple', 'banana']</span></span><br><span class="line"></span><br><span class="line">print(sorted(fruits, reverse=<span class="keyword">True</span>))</span><br><span class="line"><span class="comment"># ['raspberry', 'grape', 'banana', 'apple']</span></span><br><span class="line">print(sorted(fruits, key=len, reverse=<span class="keyword">True</span>))</span><br><span class="line"><span class="comment"># ['raspberry', 'banana', 'grape', 'apple']</span></span><br><span class="line"></span><br><span class="line">print(fruits)</span><br><span class="line"><span class="comment"># ['grape', 'raspberry', 'apple', 'banana']</span></span><br><span class="line">fruits.sort()</span><br><span class="line">print(fruits)</span><br><span class="line"><span class="comment"># ['apple', 'banana', 'grape', 'raspberry']</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-8-用bisect来管理已排序的序列"><a href="#2-8-用bisect来管理已排序的序列" class="headerlink" title="2.8 用bisect来管理已排序的序列"></a>2.8 用bisect来管理已排序的序列</h3><ul>
<li><p>标准库的bisect模块给我们提供了二分查找算法</p>
</li>
<li><p>bisect模块包含两个主要函数,<code>bisect</code>和<code>insort</code>，两个函数都利用二分查找算法来在有序序列中查找或插入元素</p>
</li>
</ul>
<h4 id="2-8-1-用bisect来搜索"><a href="#2-8-1-用bisect来搜索" class="headerlink" title="2.8.1 用bisect来搜索"></a>2.8.1 用bisect来搜索</h4><ul>
<li><p>简介</p>
<ul>
<li><code>bisect(haystack, needle)</code>在haystack(干草垛)里搜索needle(针)的位置</li>
<li>该位置满足的条件是，把needle插入这个位置之后，haystack还能保持升序</li>
<li>也就是说这个函数返回的位置前面的值，都小于或等于needle的值</li>
</ul>
</li>
<li><p><code>bisect</code>的可选参数</p>
<ul>
<li><code>lo</code>：表示搜索范围的下标，<strong>包括</strong>这个元素(老规矩)，默认为<code>0</code></li>
<li><code>hi</code>：表示搜索范围的上标，<strong>不包括</strong>这个元素(老规矩)，默认为<code>len()</code></li>
</ul>
</li>
<li><p><code>bisect_left</code>和<code>bisect_right</code></p>
<ul>
<li><code>bisect</code>函数其实就是<code>bisect_right</code>函数</li>
<li><code>bisect_left</code>返回的插入位置是跟它相等的元素之前的位置</li>
<li><code>bisect_right</code>返回的插入位置是跟它相等的元素之后的位置</li>
</ul>
</li>
<li><p>demo：根据一个分数，找到它所对应的成绩</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bisect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grade</span><span class="params">(score, breakpoints=[<span class="number">60</span>, <span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>], grades=<span class="string">'FDCBA'</span>)</span>:</span></span><br><span class="line">    i = bisect.bisect(breakpoints, score)</span><br><span class="line">    <span class="keyword">return</span>  grades[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print([grade(score) <span class="keyword">for</span> score <span class="keyword">in</span> [<span class="number">33</span>, <span class="number">99</span>, <span class="number">77</span>, <span class="number">70</span>, <span class="number">89</span>, <span class="number">90</span>, <span class="number">100</span>]])</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">['F', 'A', 'C', 'C', 'B', 'A', 'A']</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-8-2-用bisect-insort插入新元素"><a href="#2-8-2-用bisect-insort插入新元素" class="headerlink" title="2.8.2 用bisect.insort插入新元素"></a>2.8.2 用bisect.insort插入新元素</h4><ul>
<li><p><code>insort(seq, item)</code>把变量item插入到序列seq中，并能保持seq的升序排序</p>
</li>
<li><p><code>insort</code>和<code>bisect</code>一样，有<code>lo</code>和<code>hi</code>两个可选参数用来控制查找的范围</p>
</li>
<li><p>它也有个变体叫<code>insort_left</code></p>
</li>
</ul>
<h3 id="2-9-当列表不是首选时"><a href="#2-9-当列表不是首选时" class="headerlink" title="2.9 当列表不是首选时"></a>2.9 当列表不是首选时</h3><ul>
<li><p>比如，要存放1000万个浮点数的话，数组(array)要高效得多，因为数组在背后存放的并不是float对象，而是数字的机器翻译，也就是字节描述</p>
</li>
<li><p>再比如，如果需要频繁对序列做先进先出的操作，deque(双端队列)的速度应该会更快</p>
</li>
<li><p>如果在你的代码里，<strong>包含操作</strong>的频率很高，用set(集合)会更合适，因为set是无序的</p>
</li>
</ul>
<h4 id="2-9-1-数组"><a href="#2-9-1-数组" class="headerlink" title="2.9.1 数组"></a>2.9.1 数组</h4><ul>
<li><p>简介</p>
<ul>
<li>如果我们需要一个只包含数字的列表，那么<code>array.array</code>比<code>list</code>是更好的选择</li>
<li>数组支持所有跟可变序列有关的操作，包括<code>.pop</code>、<code>.insert</code>、<code>.extend</code></li>
<li>数组还提供了从文件读取和存入文件的更快的方法，如<code>.frombytes</code>和<code>.tofile</code></li>
</ul>
</li>
<li><p>创建数组需要一个类型码，这个类型码用来表示在底层的C语言应该存放怎样的数据类型</p>
</li>
<li><p>Python不会允许你在数组里存放除指定数据类型之外的数据</p>
</li>
<li><p>从Python3.4开始，数组(array)类型不再支持诸如<code>list.sort()</code>这种就地排序方法</p>
</li>
<li><p>arrayDemo</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> array <span class="keyword">import</span> array</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用一个可迭代对象生成双精度浮点数组，这里使用的可迭代对象是一个生成器表达式</span></span><br><span class="line">floats = array(<span class="string">'d'</span>, (random() <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span> ** <span class="number">5</span>)))</span><br><span class="line">print(floats[<span class="number">-1</span>])</span><br><span class="line">fp = open(<span class="string">'floats.bin'</span>, <span class="string">'wb'</span>)</span><br><span class="line"><span class="comment"># 把所有元素以机器值的形式存入一个文件</span></span><br><span class="line">floats.tofile(fp)</span><br><span class="line">fp.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建一个双精度浮点空数组</span></span><br><span class="line">floats2 = array(<span class="string">'d'</span>)</span><br><span class="line">fp = open(<span class="string">'floats.bin'</span>, <span class="string">'rb'</span>)</span><br><span class="line"><span class="comment"># 把1000万个浮点数从二进制文件里读取出来</span></span><br><span class="line">floats2.fromfile(fp, <span class="number">10</span>**<span class="number">5</span>)</span><br><span class="line">fp.close()</span><br><span class="line">print(floats2[<span class="number">-1</span>])</span><br><span class="line"><span class="comment"># 对比两个数组的内容是否完全一致</span></span><br><span class="line">print(floats2 == floats)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-9-2-内存视图"><a href="#2-9-2-内存视图" class="headerlink" title="2.9.2 内存视图"></a>2.9.2 内存视图</h4><ul>
<li><p><code>memoryview</code>是一个内置类，它能让用户在不需要复制内容的前提下，在数据结构之间共享内存</p>
</li>
<li><p>其中<code>memoryview.cast</code>可以用不同的方式读写同一块内存数据，而且内容字节不会随意移动</p>
</li>
<li><p>memoryviewDemo</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> array <span class="keyword">import</span> array</span><br><span class="line"></span><br><span class="line">numbers = array(<span class="string">'h'</span>, [<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">memv = memoryview(numbers)</span><br><span class="line">print(len(memv))</span><br><span class="line">print(memv[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用cast将memv里的内容转换为无符合类型('B')</span></span><br><span class="line">memv_oct = memv.cast(<span class="string">'B'</span>)</span><br><span class="line">print(memv_oct.tolist())</span><br><span class="line"><span class="comment"># 修改位于位置5的字节赋值为4</span></span><br><span class="line">memv_oct[<span class="number">5</span>] = <span class="number">4</span></span><br><span class="line"><span class="comment"># 因为我们把占了两个字节的整数的高位字节改成了4，所以这个有符号整数的值就变成了1024</span></span><br><span class="line">print(numbers)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">5</span></span><br><span class="line"><span class="string">-2</span></span><br><span class="line"><span class="string">[254, 255, 255, 255, 0, 0, 1, 0, 2, 0]</span></span><br><span class="line"><span class="string">array('h', [-2, -1, 1024, 1, 2])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-9-3-NumPy和SciPy"><a href="#2-9-3-NumPy和SciPy" class="headerlink" title="2.9.3 NumPy和SciPy"></a>2.9.3 NumPy和SciPy</h4><ul>
<li><p>Numpy实现了多维同质数组(homogeneous array)和矩阵</p>
</li>
<li><p>Scipy是基于Numpy的另一个库，提供了很多跟科学计算有关的算法，专为线性代数、数值分析和统计学而设计</p>
</li>
<li><p>Scipy把基于C和Fortran的工业级数学计算功能用交互式且高度抽象的Python包装起来，让科学家如鱼得水</p>
</li>
<li><p>NumpyDemo</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建一个0-11的整数的numpy.ndarray，然后把它打印出来</span></span><br><span class="line">a = np.arange(<span class="number">12</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(type(a))</span><br><span class="line"><span class="comment"># 查看数组的维度</span></span><br><span class="line">print(a.shape)</span><br><span class="line"><span class="comment"># 改变数组的维度</span></span><br><span class="line">a.shape = <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># 打印第二行</span></span><br><span class="line">print(a[<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 打印第二行第一个元素</span></span><br><span class="line">print(a[<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 打印第二行第一列元素</span></span><br><span class="line">print(a[:<span class="number">-1</span>])</span><br><span class="line"><span class="comment"># 将数组行列交换</span></span><br><span class="line">print(a.transpose())</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[ 0  1  2  3  4  5  6  7  8  9 10 11]</span></span><br><span class="line"><span class="string">&lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line"><span class="string">(12,)</span></span><br><span class="line"><span class="string">[[ 0  1  2  3]</span></span><br><span class="line"><span class="string">[ 4  5  6  7]</span></span><br><span class="line"><span class="string">[ 8  9 10 11]]</span></span><br><span class="line"><span class="string">[ 8  9 10 11]</span></span><br><span class="line"><span class="string">9</span></span><br><span class="line"><span class="string">[[0 1 2 3]</span></span><br><span class="line"><span class="string">[4 5 6 7]]</span></span><br><span class="line"><span class="string">[[ 0  4  8]</span></span><br><span class="line"><span class="string">[ 1  5  9]</span></span><br><span class="line"><span class="string">[ 2  6 10]</span></span><br><span class="line"><span class="string">[ 3  7 11]]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="2-9-4-双向队列和其他形式的队列"><a href="#2-9-4-双向队列和其他形式的队列" class="headerlink" title="2.9.4 双向队列和其他形式的队列"></a>2.9.4 双向队列和其他形式的队列</h4><ul>
<li><p>利用.append和.pop方法，我们可以把列表当作栈或者队列使用，但是删除列表的第一个元素之类的操作是很费时的</p>
</li>
<li><p><code>collections.deque</code>类(双向队列)是个线程安全、可以快速从两端添加或者删除元素的数据类型</p>
</li>
<li><p><code>append</code>和<code>popleft</code>都是原子操作，也就是deque可以在多线程程序中安全地当作先进先出的队列使用，而使用者不必担心资源锁问题</p>
</li>
<li><p>dequeDemo</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"><span class="comment"># maxlen是可选参数，代表这个队列可以容纳的元素的个数，而且一旦设定，这个属性无法改变</span></span><br><span class="line">dq = deque(range(<span class="number">10</span>), maxlen=<span class="number">10</span>)</span><br><span class="line">print(dq)</span><br><span class="line"><span class="comment"># 队列的旋转操作接受一个参数n,当n&gt;0时，队列中最右边的n个元素被移动到队列的左边，当n&lt;0时，队列中最左边的n个元素被移动到队列的右边</span></span><br><span class="line">dq.rotate(<span class="number">3</span>)</span><br><span class="line">print(dq)</span><br><span class="line">dq.rotate(<span class="number">-4</span>)</span><br><span class="line">print(dq)</span><br><span class="line"><span class="comment"># 当试图对一个已满的队列头部做添加操作，它尾部的元素会被山除掉</span></span><br><span class="line">dq.appendleft(<span class="number">-1</span>)</span><br><span class="line">print(dq)</span><br><span class="line"><span class="comment"># 在尾部添加3个元素会挤掉-1, 1和2</span></span><br><span class="line">dq.extend([<span class="number">11</span>, <span class="number">22</span>, <span class="number">33</span>])</span><br><span class="line">print(dq)</span><br><span class="line"><span class="comment"># extendleft(iter)会把可迭代里的元素逐个添加到双向队列的左边，因此列表中的元素会逆序出现在队列中</span></span><br><span class="line">dq.extendleft([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>])</span><br><span class="line">print(dq)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)</span></span><br><span class="line"><span class="string">deque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)</span></span><br><span class="line"><span class="string">deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)</span></span><br><span class="line"><span class="string">deque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)</span></span><br><span class="line"><span class="string">deque([3, 4, 5, 6, 7, 8, 9, 11, 22, 33], maxlen=10)</span></span><br><span class="line"><span class="string">deque([40, 30, 20, 10, 3, 4, 5, 6, 7, 8], maxlen=10)</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-10-本章小结"><a href="#2-10-本章小结" class="headerlink" title="2.10 本章小结"></a>2.10 本章小结</h3><ol>
<li><p>Python序列类型最常见的分类为可变和不可变序列，另外一种分类方式也很有用，那就是把它们分成扁平序列和容器序列</p>
</li>
<li><p>列表推导和生成器表达式则提供了灵活构建和初始化序列的方式</p>
</li>
<li><p>元组在Python中扮演两种角色</p>
<ul>
<li>用作无名称的字段的记录</li>
<li>看作不可变的列表</li>
</ul>
</li>
<li><p>序列切片</p>
</li>
<li><p>重复拼接<code>seq * n</code>在正确使用的前提下，能让我们方便地初始化含有不可变元素的多维列表</p>
</li>
<li><p>增量赋值 <code>+=</code> 和 <code>*=</code> 会区别对待可变和不可变序列</p>
</li>
<li><p>序列的sort方法和内置的sorted函数可以实现就地排序和新建序列排序</p>
</li>
<li><p>bisect.bisect实现了二分查找</p>
</li>
<li><p>array.array、deque、Numpy等提供了除了列表之外的其他选择</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/16/流畅的Python-1-Python数据模式/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/16/流畅的Python-1-Python数据模式/" itemprop="url">[流畅的Python][1][Python数据模式]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-16T20:24:58+08:00">
                2019-03-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/流畅的Python/" itemprop="url" rel="index">
                    <span itemprop="name">流畅的Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/16/流畅的Python-1-Python数据模式/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/16/流畅的Python-1-Python数据模式/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第1章-Python数据模式"><a href="#第1章-Python数据模式" class="headerlink" title="第1章 Python数据模式"></a>第1章 Python数据模式</h2><blockquote>
<p>Guido对语言设计美学的深入理解让人震惊。我认识不少很不错的编程语言设计者，它们设计出来的东西确实很精彩，但是从来都不会有用户。Guido知道如何在理论上做出一定妥协，设计出来的语言让使用者觉得如沐春风，这真是不可多得  —–Jim Hugunin</p>
</blockquote>
<ul>
<li><p>Python最好的品质之一是一致性</p>
</li>
<li><p>数据模型</p>
<ul>
<li>数据模型所描述的API，为使用最地道的预言特性来构建你自己的对象提供了工具</li>
<li>数据模型其实是对Python框架的描述，它规范了这门语言自身构建模块的接口，这些接口包括但不限于序列、迭代器、函数、类和上下文管理器</li>
</ul>
</li>
<li><p>特殊方法</p>
<ul>
<li>Python解释器碰到特殊的句法时，会使用特殊方法去激活一些基本的对象操作</li>
<li>这些特殊方法的名字以两个下划线开头，以两个下划线结尾</li>
<li>举例：obj[key]的背后是<strong>getitem</strong>方法，为了能求得my_collection[key]的值，解释器实际上会调用my_collection.getitem(key)</li>
<li>这些特殊方法名能让你自己的对象实现和支持一下的语言构架，并与之交互：<ul>
<li>迭代</li>
<li>集合类</li>
<li>属性访问</li>
<li>运算符重载</li>
<li>函数和方法的调用</li>
<li>对象的创建和销毁</li>
<li>字符串表示形式和格式化</li>
<li>管理上下文(即with块)</li>
</ul>
</li>
<li>特殊方法也称为魔术方法(magic method)或者双下方法(dunder method)</li>
</ul>
</li>
</ul>
<h3 id="1-1-一摞Python风格的纸牌"><a href="#1-1-一摞Python风格的纸牌" class="headerlink" title="1.1 一摞Python风格的纸牌"></a>1.1 一摞Python风格的纸牌</h3><ul>
<li><p>纸牌类代码实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> choice</span><br><span class="line">Card = collections.namedtuple(<span class="string">'Card'</span>, [<span class="string">'rank'</span>, <span class="string">'suit'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FrenchDeck</span>:</span></span><br><span class="line">    ranks = [str(n) <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">11</span>)] + list(<span class="string">'JQKA'</span>)</span><br><span class="line">    suits = <span class="string">'spades diamonds clubs hearts'</span>.split()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._cards = [Card(rank, suit) <span class="keyword">for</span> suit <span class="keyword">in</span> self.suits <span class="keyword">for</span> rank <span class="keyword">in</span> self.ranks]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._cards)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, position)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._cards[position]</span><br></pre></td></tr></table></figure>
</li>
<li><p>namedtuple(下面的内容跑题了)</p>
<ul>
<li>用来构建只有少数属性但是没有方法的对象</li>
<li>例如：数据库条目</li>
</ul>
</li>
<li><p>短小但强悍的FrenchDeck都可以做什么</p>
<ol>
<li><p>可以跟任何标准Python集合类型一样，用<code>len()</code>函数查看一叠牌有多少张</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">deck = FrecnDeck()</span><br><span class="line">len(deck) <span class="comment"># 52</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>从一叠牌中抽取特定一张卡牌是容易的</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deck[<span class="number">0</span>] <span class="comment"># Card(rank='2', suit='spades')</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>我们并不需要单独写一个方法来抽取一张纸牌，只需调用内置函数<code>random.choice()</code></p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">choice(deck) <span class="comment"># Card(rank='K', suit='hearts')</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>我们的deck类已经可以支持切片(slicing)操作了，因为我们实现了<code>__getitem__</code>方法</p>
 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deck[:<span class="number">3</span>] <span class="comment"># [Card(rank='2', suit='spades'), Card(rank='3', suit='spades'), Card(rank='4', suit='spades')]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>另外，仅仅实现了<code>__getitem__</code>方法,这摞牌就可以迭代了</p>
<ul>
<li><p>input:</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> card <span class="keyword">in</span> deck:</span><br><span class="line">    print(card)</span><br></pre></td></tr></table></figure>
</li>
<li><p>output:</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">        ...</span><br><span class="line">        Card(rank=&apos;8&apos;, suit=&apos;hearts&apos;)</span><br><span class="line">        Card(rank=&apos;9&apos;, suit=&apos;hearts&apos;)</span><br><span class="line">        Card(rank=&apos;10&apos;, suit=&apos;hearts&apos;)</span><br><span class="line">        Card(rank=&apos;J&apos;, suit=&apos;hearts&apos;)   </span><br><span class="line">        Card(rank=&apos;Q&apos;, suit=&apos;hearts&apos;)</span><br><span class="line">        Card(rank=&apos;K&apos;, suit=&apos;hearts&apos;)</span><br><span class="line">        Card(rank=&apos;A&apos;, suit=&apos;hearts&apos;)</span><br><span class="line">        ...</span><br><span class="line">        ````  </span><br><span class="line">6. `in`运算符也可以用在我们的FrenchDeck类上了</span><br><span class="line">    - 代码</span><br><span class="line">        ````py</span><br><span class="line">        Card(&apos;Q&apos;, &apos;hearts&apos;) in deck # True</span><br></pre></td></tr></table></figure>
</li>
<li><p>原因</p>
<ul>
<li>一个类型如果没有实现<code>__contains__</code>方法，那么<code>in</code>运算符就会按顺序做一次迭代搜索</li>
<li>因为我们的<code>FrenchDeck</code>实现了<code>__getitem__</code>方法，所以可以迭代，所以支持<code>in</code>运算符</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><p>通过实现特殊方法来利用Python数据模型的好处</p>
<ol>
<li>作为你的类的用户，它们不必去记住操作的格式名称<ul>
<li>例如：得到元素个数是<code>size()</code>还是<code>len()</code>？</li>
</ul>
</li>
<li>可以更加方便地使用Python标准库，如<code>random.choice()</code>函数，而不用重新发明轮子</li>
</ol>
</li>
<li><p>写在最后</p>
<ul>
<li>虽然<code>FrenchDeck</code>隐式地继承了<code>object</code>类，但是功能却不是继承而来的</li>
<li>我们通过数据模型和一些合成来实现了这些功能</li>
<li>通过实现<code>__len__</code>和<code>__getitem__</code>两个特殊方法,<code>FrenchDeck</code>就跟一个Python自有的序列数据类型一样，可以体现出Python语言的核心特性(例如迭代和切片)</li>
</ul>
</li>
</ul>
<h3 id="1-2-如何使用特殊方法"><a href="#1-2-如何使用特殊方法" class="headerlink" title="1.2 如何使用特殊方法"></a>1.2 如何使用特殊方法</h3><ol>
<li><p>首先要明确，特殊方法的存在是为了被<strong>Python解释器</strong>调用的，你自己并不需要调用它们</p>
<ul>
<li>举例：在执行<code>len(my_obj)</code>的时候，如果<code>my_obj</code>是一个自定义类的对象，那么Python会自己去调用其中由你实现的<code>__len__</code>方法</li>
</ul>
</li>
<li><p>如果是Python内置的类型，比如列表、字符串、字节序列等，那么CPython会抄近路以获得更快的执行速度</p>
<ul>
<li>举例：在内置类型中，<code>__len__</code>实际上会直接返回<code>PyVarObject</code>中的<code>ob_size</code>属性</li>
</ul>
</li>
<li><p>很多时候，特殊方法的调用是隐式的</p>
<ul>
<li>举例：<code>for i in x:</code>背后实际用的是<code>iter(x)</code>，而这个函数的背后则是<code>__iter()__</code>方法</li>
</ul>
</li>
<li><p>通常你的代码无需直接使用特殊方法</p>
</li>
<li><p>通过内置的函数(例如len、iter、str,等等)来使用特殊方法是最好的选择</p>
</li>
<li><p>不要自己想当然的随意添加特殊方法，比如<code>__foo__</code>之类的，因为虽然现在这个名字没有在Python内部被使用，以后就不一定了</p>
</li>
</ol>
<h4 id="1-2-1-模拟数值类型"><a href="#1-2-1-模拟数值类型" class="headerlink" title="1.2.1 模拟数值类型"></a>1.2.1 模拟数值类型</h4><p>此处我们实现一个二维向量类，作为例子<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> hypot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vector</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x=<span class="number">0</span>, y=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Vector(%r,%r)'</span> % (self.x, self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__abs__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> hypot(self.x, self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__bool__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> bool(abs(self))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        x = self.x + other.x</span><br><span class="line">        y = self.y + other.y</span><br><span class="line">        <span class="keyword">return</span> Vector(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__mul__</span><span class="params">(self, scalar)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Vector(self.x * scalar, self.y * scalar)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为实现的__add()__方法，所以我们可以使用加号运算符来完成向量相加</span></span><br><span class="line">v1 = Vector(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">v2 = Vector(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">print(v1 + v2)  <span class="comment"># Vector(4, 5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为实现了__abs()__方法，所以我们可以调用abs()函数来完成求绝对值操作</span></span><br><span class="line">v = Vector(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(abs(v))   <span class="comment"># 5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为实现了__mul()__方法，所以我们可以使用乘法运算符完成向量与数字相乘</span></span><br><span class="line">print(v * <span class="number">3</span>)    <span class="comment"># Vector(9, 12)</span></span><br></pre></td></tr></table></figure></p>
<p>注意</p>
<ul>
<li>这些特殊方法并不会在这个类自身的代码中使用</li>
<li>即便其他程序要使用这个类的这些方法，也不会直接调用它们</li>
</ul>
<h4 id="1-2-2-字符串表示形式"><a href="#1-2-2-字符串表示形式" class="headerlink" title="1.2.2 字符串表示形式"></a>1.2.2 字符串表示形式</h4><p><code>repr</code>函数</p>
<ul>
<li><p>Python的内置函数</p>
</li>
<li><p>可以把一个对象用字符串的形式表达出来以便辨认</p>
</li>
<li><p>这就是“字符串表示形式”</p>
</li>
</ul>
<p><code>__repr__</code>所返回的字符串应该准确、无歧义并且尽可能表达出如何用代码创建出这个被打印的对象</p>
<p>如果<code>__str__</code>函数没有被实现，而Python又需要调用它时，会用<code>__repr__</code>代替</p>
<h4 id="1-2-3-算术运算符"><a href="#1-2-3-算术运算符" class="headerlink" title="1.2.3 算术运算符"></a>1.2.3 算术运算符</h4><p>通过<code>__add__</code>和<code>__mul__</code>，前面的向量类可以使用<code>+</code>和<code>*</code>这两个算术运算符</p>
<p>这两个方法的返回值都是新创建的向量对象，被操作的两个向量还是原封不动，代码里只是读取它们的值而已</p>
<p>中缀表达式的基本原则就是不改变操作对象，而是产出一个新的值</p>
<h4 id="1-2-4-自定义的布尔值"><a href="#1-2-4-自定义的布尔值" class="headerlink" title="1.2.4 自定义的布尔值"></a>1.2.4 自定义的布尔值</h4><p>任何对象都可以用于需要布尔值的上下文中，Python调用bool(x)，判断x是真还是假</p>
<p>判断boolean值的流程</p>
<ul>
<li><p>默认情况下，我们自己定义的类的实例总被认为是真的，除非这个类对<code>__bool__</code>或<code>__len__</code>函数有自己的实现</p>
</li>
<li><p><code>bool(x)</code>的背后是调用<code>x.__bool__()</code>的结果</p>
</li>
<li><p>如果不存在<code>x.__bool__()</code>，那么<code>bool(x)</code>会尝试调用<code>x.__len()__</code></p>
<ul>
<li>若返回0,则bool会返回False</li>
<li>否则返回True</li>
</ul>
</li>
</ul>
<h3 id="1-3-特殊方法一览"><a href="#1-3-特殊方法一览" class="headerlink" title="1.3 特殊方法一览"></a>1.3 特殊方法一览</h3><p>跟运算符无关的特殊方法</p>
<p><img src="/2019/03/16/流畅的Python-1-Python数据模式/0316_0.png" alt=""></p>
<p>跟运算符相关的特殊方法</p>
<p><img src="/2019/03/16/流畅的Python-1-Python数据模式/0316_1.png" alt=""><br><img src="/2019/03/16/流畅的Python-1-Python数据模式/0316_2.png" alt=""></p>
<h3 id="1-4-为什么len不是普通方法"><a href="#1-4-为什么len不是普通方法" class="headerlink" title="1.4 为什么len不是普通方法"></a>1.4 为什么len不是普通方法</h3><p>为什么len不是普通方法</p>
<ul>
<li><p>Raymond Hetting的回答:“实用胜于纯粹”</p>
</li>
<li><p>如果x是一个内置类型的实例，那么len(x)的速度会非常快,也就是在str、list、memoryview等类型上，这个操作非常高效</p>
</li>
<li><p>len之所以是特殊方法，是为了让Python自带的数据结构可以走后门</p>
</li>
<li><p>这种处理方式在保持内置类型的效率和保证语言的一致性之间找到了一个平衡点</p>
</li>
</ul>
<h3 id="1-5-本章小结"><a href="#1-5-本章小结" class="headerlink" title="1.5 本章小结"></a>1.5 本章小结</h3><ol>
<li><p>通过实现特殊方法，自定义数据类型可以表现得像内置类型一样，从而让我们写出更具表达力的代码(更具Python风格的代码)</p>
</li>
<li><p>Python对象的一个基本要求就是它得有合理的字符串表达形式</p>
<ul>
<li>我们可以通过<code>__repr__</code>和<code>__str__</code>来满足这个要求</li>
<li>前者便于我们调试和记录日志</li>
<li>后者则是给终端用户看的</li>
</ul>
</li>
<li><p>Python通过运算符重载这一模式提供了丰富的数值类型，除了内置的那些，还有<code>decimal.Decimal</code>和<code>fractions.Fraction</code>。这些数据类型都支持中缀算术运算符</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/16/流畅的Python-0-Python之禅/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/16/流畅的Python-0-Python之禅/" itemprop="url">[流畅的Python][0][Python之禅]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-16T20:23:00+08:00">
                2019-03-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/流畅的Python/" itemprop="url" rel="index">
                    <span itemprop="name">流畅的Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/16/流畅的Python-0-Python之禅/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/16/流畅的Python-0-Python之禅/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第0章-Python之禅-The-Zen-of-Python-by-Tim-Peters"><a href="#第0章-Python之禅-The-Zen-of-Python-by-Tim-Peters" class="headerlink" title="第0章 Python之禅(The Zen of Python, by Tim Peters)"></a>第0章 Python之禅(The Zen of Python, by Tim Peters)</h2><ul>
<li><p>Beautiful is better than ugly.<br>优美胜于丑陋（Python程序员笃信代码可以编写得漂亮而优雅）</p>
</li>
<li><p>Explicit is better than implicit.<br>明了胜于晦涩（优美的代码应当是明了的，命名规范，风格相似）</p>
</li>
<li><p>Simple is better than complex.<br>简洁胜于复杂（如果有两个解决方案，一个简单，一个复杂，且都行之有效，就选择简单的解决方法）</p>
</li>
<li><p>Complex is better than complicated.<br>复杂胜于凌乱（如果复杂不可避免，那代码间也不能有难懂的关系，要保持接口简洁）</p>
</li>
<li><p>Flat is better than nested.<br>扁平胜于嵌套（优美的代码应当是扁平的，不能有太多的嵌套）</p>
</li>
<li><p>Sparse is better than dense.<br>间隔胜于紧凑（优美的代码有适当的间隔，不要奢望一行代码解决问题）</p>
</li>
<li><p>Readability counts.<br>可读性很重要（优美的代码是可读的）</p>
</li>
<li><p>Special cases aren’t special enough to break the rules.<br>Although practicality beats purity.<br>即便假借特例的实用性之名，也不可违背这些规则（这些规则至高无上）</p>
</li>
<li><p>Errors should never pass silently. Unless explicitly silenced.<br>不要包容所有错误，除非你确定需要这样做（精准地捕获异常，不写 except:pass 风格的代码）</p>
</li>
<li><p>In the face of ambiguity, refuse the temptation to guess.<br>当存在多种可能，不要尝试去猜测</p>
</li>
<li><p>There should be one– and preferably only one –obvious way to do it.<br>尽量找一种，最好是唯一一种明显的解决方案（如果让两个Python程序员去解决同一个问题，他们提供的解决方案应大致相同）</p>
</li>
<li><p>Although that way may not be obvious at first unless you’re Dutch.<br>虽然这并不容易，因为你不是 Python 之父（这里的 Dutch 是指 Guido ）</p>
</li>
<li><p>Now is better than never.<br>做也许好过不做.（不要企图编写完美无缺的代码；先编写行之有效的代码，再决定是对其改进还是重新编写）</p>
</li>
<li><p>Although never is often better than <em>right</em> now.<br>但不假思索就动手还不如不做（动手之前要细思量）</p>
</li>
<li><p>If the implementation is hard to explain, it’s a bad idea.<br>如果你无法向人描述你的方案，那肯定不是一个好方案</p>
</li>
<li><p>If the implementation is easy to explain, it may be a good idea.<br>如果你可以向人描述你的方案，那肯定是一个好方案</p>
</li>
<li><p>Namespaces are one honking great idea – let’s do more of those!<br>命名空间是一种绝妙的理念，我们应当多加利用（倡导与号召）</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/13/算法导论-16-贪心算法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/13/算法导论-16-贪心算法/" itemprop="url">[算法导论][16]贪心算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-13T21:04:54+08:00">
                2019-03-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/算法导论/" itemprop="url" rel="index">
                    <span itemprop="name">算法导论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/13/算法导论-16-贪心算法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/13/算法导论-16-贪心算法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第16章-贪心算法"><a href="#第16章-贪心算法" class="headerlink" title="第16章 贪心算法"></a>第16章 贪心算法</h2><p>对于许多最优化问题，使用动态规划算法来求最优解有些杀鸡用牛刀了，可以使用更简单、更高效的算法。贪心算法(greedy algorithm)就是这样的算法，它在每一步都做出当时看起来最佳的选择。它总是做出局部最优的选择，寄希望这样的选择能导致全局最优解</p>
<p>贪心算法并不保证得到最优解，但对很多问题确实可以求得最优解</p>
<p>有很多利用贪心策略设计的算法，包括最小生成树(minimum-spanning-tree)算法、单源最短路径算法的Dijkstra算法，以及集合覆盖问题的Chvatal贪心启发式算法</p>
<h3 id="16-1-活动选择问题"><a href="#16-1-活动选择问题" class="headerlink" title="16.1 活动选择问题"></a>16.1 活动选择问题</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>假定有一个 n 个活动(activity)的集合 S = {a<sub>1</sub>, a<sub>2</sub>, …, a<sub>n</sub>}，这些活动使用同一个资源，而这个资源在某个时刻只能供一个活动使用。每个活动 a<sub>i</sub> 都有一个开始时间 s<sub>i</sub> 和一个结束时间 f<sub>i</sub>。</p>
<p>如果两个活动 a<sub>i</sub> 和 a<sub>j</sub> 满足 [s<sub>i</sub>, f<sub>i</sub>) 和 [s<sub>j</sub>, f<sub>j</sub>) 不重叠，则称它们是兼容的。</p>
<p>在活动选择问题中，我们希望选出一个最大兼容活动集。 </p>
<p>假定活动已经按照时间的单调递增顺序排序</p>
<p>f<sub>1</sub> &lt;= f<sub>2</sub> &lt;= … &lt;= f<sub>n-1</sub> &lt;= f<sub>n</sub></p>
<h4 id="活动选择问题的最优子结构"><a href="#活动选择问题的最优子结构" class="headerlink" title="活动选择问题的最优子结构"></a>活动选择问题的最优子结构</h4><p>令 S<sub>ij</sub> 表示在 a<sub>i</sub> 结束之后开始，在 a<sub>j</sub> 开始之前结束的那些活动的集合。假定 A<sub>ij</sub> 是 S<sub>ij</sub> 的一个最大的相互兼容的活动子集，且包含活动 a<sub>k</sub>。由于最优解包含活动 a<sub>k</sub> ，我们得到了两个子问题: 寻找 S<sub>ik</sub> 中的最大兼容活动子集(在 a<sub>i</sub> 结束之后开始，在 a<sub>k</sub> 开始之前结束的活动)以及寻找 S<sub>kj</sub> 中的最大兼容活动子集(在 a<sub>k</sub> 结束之后开始，在 a<sub>j</sub> 开始之前结束的活动)。可得 |A<sub>ij</sub>| = |A<sub>ik</sub>| + |A<sub>kj</sub>| + 1 个活动</p>
<p>我们容易用剪切-粘贴法证明最优解A<sub>ij</sub>必然包含两个子问题S<sub>ik</sub>和S<sub>kj</sub>的最优解</p>
<p>可以得到如下递归式</p>
<p><img src="/2019/03/13/算法导论-16-贪心算法/0312_0.png" alt=""></p>
<p>于是接下来可以设计一种带备忘机制的递归算法或者使用自底向上法填写表项</p>
<h4 id="贪心选择"><a href="#贪心选择" class="headerlink" title="贪心选择"></a>贪心选择</h4><p>对于活动选择问题，什么是贪心选择？ 直觉告诉我们，应该选择S中最早结束的活动，因为它剩下的资源可供它之后尽可能多的活动使用</p>
<p>当做出贪心选择之后，只剩下一个子问题需要我们求解: 寻找在 a<sub>1</sub> 结束后开始的活动</p>
<p>令 S<sub>k</sub> = { a<sub>i</sub> ∈ S: s<sub>i</sub> &gt;= f<sub>k</sub>}为在 a<sub>k</sub>结束后开始的任务集合。当我们做出贪心选择，选择了 a<sub>1</sub> 之后，剩下的 S<sub>1</sub> 是唯一需要求解的子问题</p>
<p>可以证明我们的直觉是正确的，证明过程略</p>
<p>贪心算法通常都是自顶而下的设计: 做出一个选择，然后求解剩下的那个子问题，而不是自底向上地求解出很多子问题，然后再做出选择</p>
<h4 id="递归贪心算法"><a href="#递归贪心算法" class="headerlink" title="递归贪心算法"></a>递归贪心算法</h4><p>过程 <code>RECURSIVE-ACTIVITY-SELECTOR</code> 的输入为两个数组 s 和 f，表示活动的开始和结束时间，下标 k 指出要求解的子问题 S<sub>k</sub>，以及问题规模 n 。它返回 S<sub>k</sub> 的一个最大兼容活动集。我们假定输入的 n 个活动已经按照结束时间的单调递增顺序排序好。为了方便算法初始化，我们添加了一个虚拟活动 a<sub>0</sub>，其结束时间为 f<sub>0</sub> = 0，这样子问题 S<sub>0</sub> 就是完整的活动集S。求解原问题即可调用 <code>RECURSIVE-ACTIVITY-SELECTOR(s, f, 0, n)</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">RECURSIVE-ACTIVITY-SELECTOR(s, f, k, n)</span><br><span class="line">    m = k + 1</span><br><span class="line">    while m &lt;= n and s[m] &lt; f[k]</span><br><span class="line">        m += 1</span><br><span class="line">    if m &lt;= n</span><br><span class="line">        return &#123; am &#125; ∪ RECURSIVE-ACTIVITY-SELECTOR(s, f, m, n)</span><br><span class="line">    else </span><br><span class="line">        return ∅</span><br></pre></td></tr></table></figure>
<p><code>RECURSIVE-ACTIVITY-SELECTOR</code> 算法运行时间 <code>Θ(n)</code></p>
<h4 id="迭代贪心算法"><a href="#迭代贪心算法" class="headerlink" title="迭代贪心算法"></a>迭代贪心算法</h4><p>我们可以很容易地将算法转换为迭代形式。过程 <code>RECURSIVE-ACTIVITY-SELECTOR</code> 几乎就是”尾递归”: 它以一个对自身地递归调用再接一次并集操作结尾。将尾递归过程改为迭代过程是很直接的</p>
<p>过程 <code>GREEDY-ACTIVITY-SELECTOR</code> 是过程 <code>RECURSIVE-ACTIVITY-SELECTOR</code> 的一个迭代版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">GREEDY-ACTIVITY-SELECTOR(s, f)</span><br><span class="line">    n = s.length</span><br><span class="line">    A = &#123;a1&#125;</span><br><span class="line">    k = 1</span><br><span class="line">    for m = 2 to n</span><br><span class="line">        if s[m] &gt;= f[k]</span><br><span class="line">            A = A U &#123;am&#125;</span><br><span class="line">            k = m</span><br><span class="line">    return A</span><br></pre></td></tr></table></figure>
<p><code>GREEDY-ACTIVITY-SELECTOR</code> 算法运行时间 <code>Θ(n)</code></p>
<h3 id="16-2-贪心算法原理"><a href="#16-2-贪心算法原理" class="headerlink" title="16.2 贪心算法原理"></a>16.2 贪心算法原理</h3><p>贪心算法通过做出一系列选择来求出问题的最优解。在每个决策点，它做出在当时看起来最佳的选择</p>
<p>我们可以按如下步骤设计贪心算法</p>
<ol>
<li>将最优化问题转化为这样的形式: 对其做出一次选择后，只剩下一个子问题需要求解</li>
<li>证明做出贪心选择后，原问题总是存在最优解，即贪心选择总是安全的</li>
<li>证明做出贪心选择后，剩余的子问题满足性质，其最优解与贪心选择组合即可得到原问题的最优解</li>
</ol>
<p>在每个贪心算法之下，几乎总有一个更繁琐的动态规划算法</p>
<h4 id="贪心选择性质"><a href="#贪心选择性质" class="headerlink" title="贪心选择性质"></a>贪心选择性质</h4><p>贪心算法的一个关键要素是，贪心选择性质。即，我们可以通过做出局部最优选择来构建全局最优解。当进行选择时，我们直接选择做出在当前问题中看起来最优的选择，而不必考虑子问题的解。</p>
<p>贪心算法与动态规划</p>
<ul>
<li><p>在动态规划和贪心算法中，每个步骤都要进行一次选择</p>
</li>
<li><p>但动态规划中，选择通常依赖于子问题的选择。因此，我们通常以一种自底向上的方式求解动态规划问题，先求解较小的子问题，然后是较大的子问题</p>
</li>
<li><p>在贪心算法中，我们总是做出当时看来最佳的选择，然后求解剩下的唯一的子问题。贪心算法进行选择时可能依赖之间做出的选择，但不依赖任何将来的选择或是子问题的解</p>
</li>
<li><p>贪心算法在进行第一次选择之前不求解任何子问题</p>
</li>
</ul>
<p>但是我们必须证明每个步骤做出贪心算法能生成全局最优解</p>
<h4 id="最优子结构"><a href="#最优子结构" class="headerlink" title="最优子结构"></a>最优子结构</h4><p>如果一个问题的最优解包含其子问题的最优解，则称此问题具有最优子结构性质</p>
<p>当应用贪心算法时，我们通常使用更为直接的最优子结构。如前所述，我们可以假定，通过对原问题应用贪心选择即可得到子问题。我们真正要做的全部工作就是论证: 将子问题的最优解与贪心选择组合在一起就能生成原问题的最优解。这种方法隐含地对子问题使用了数学归纳法，证明了在每个步骤进行贪心选择会生成原问题的最优解</p>
<h4 id="贪心对动态规划"><a href="#贪心对动态规划" class="headerlink" title="贪心对动态规划"></a>贪心对动态规划</h4><p>为了说明贪心算法和动态规划之间的细微差别，我们研究一个经典最优化问题的两个变形</p>
<p>0-1 背包问题的描述如下</p>
<p><img src="/2019/03/13/算法导论-16-贪心算法/0314_0.png" alt=""></p>
<p>分数背包问题的描述如下</p>
<p><img src="/2019/03/13/算法导论-16-贪心算法/0314_1.png" alt=""></p>
<p>我们用贪心策略可以求解分数背包问题，而不能求解 0-1 背包问题。为了求解分数背包问题，我们首先计算每个商品的每磅价值 v/w。遵循贪心策略，小偷首先尽量多地拿走每磅价值最高的商品。如果该商品已全部拿走而背包尚未满，他继续尽量多地拿走每磅价值第二高的产品，依次类推，直至达到重量上限W</p>
<p>这种贪心算法的运行时间为 <code>O(nlgn)</code></p>
<p>拿走商品1的策略对 0-1 背包问题无效是因为小偷无法装满背包，空闲空间降低了方案的有效每磅价值</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/11/算法导论-15-动态规划/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/11/算法导论-15-动态规划/" itemprop="url">算法导论[15][动态规划]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-11T15:54:50+08:00">
                2019-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/算法导论/" itemprop="url" rel="index">
                    <span itemprop="name">算法导论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/11/算法导论-15-动态规划/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/11/算法导论-15-动态规划/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第15章-动态规划"><a href="#第15章-动态规划" class="headerlink" title="第15章 动态规划"></a>第15章 动态规划</h2><p><strong>动态规划</strong>(dynamic programming)与分治方法相似，都是通过组合子问题的解来求解原问题。分治方法是将问题划分为互不相交的子问题，递归地求解子问题，再将它们的解结合起来，求出原问题的解。与之相反，动态规划用于子问题重叠的情况，即不同的子问题具有公共的子子问题。动态规划算法对每个子问题只求解一次，将其解保存到一个表格中，从而无需每次求解一个子子问题时都重新计算，避免了这种不必要的计算工作</p>
<p>动态规划方法通常用来求解<strong>最优化问题</strong>。这类问题可以有很多可行解，每个解都有一个值，我们希望寻找具有最优值的解。我们称这个解为问题的一个最优解</p>
<p>我们按照如下4个步骤来设计一个动态规划算法</p>
<ol>
<li>刻画一个最优解的结构特征</li>
<li>递归地定义最优解的值</li>
<li>计算最优解的值，通常采用自底向上的方法</li>
<li>利用计算出来的信息构建最优解</li>
</ol>
<h3 id="15-1-钢条切割"><a href="#15-1-钢条切割" class="headerlink" title="15.1 钢条切割"></a>15.1 钢条切割</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>给定一段长度为n英寸的钢条和一个价格表p<sub>i</sub>(i = 1, 2, …, n)，求切割钢条方案，使得销售收益r<sub>n</sub>最大</p>
<h4 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h4><p>假设钢条的价格表如下</p>
<p><img src="/2019/03/11/算法导论-15-动态规划/钢条价格表.PNG" alt=""></p>
<p>考虑n=4的情况，下图给出了n=4的所有可能的切割情况</p>
<p><img src="/2019/03/11/算法导论-15-动态规划/0310_1.PNG" alt=""></p>
<p>我们用普通的加号来代表切割方案，例如: 将<code>7=2+2+3</code>表示为将长度7英寸的钢条切割为三段。</p>
<p>对于上述价格表样例，我们可以观察所有最优收益值r<sub>i</sub> (i = 1, 2, …, 10)及对应的最初切割方案</p>
<p><img src="/2019/03/11/算法导论-15-动态规划/0310_2.PNG" alt=""></p>
<p>第一个参数p<sub>n</sub>表示不切割，直接出售长度为 <code>n</code> 英尺的钢条的方案。其他 <code>n-1</code> 个参数对应另外 <code>n - 1</code>种方案: 对每个 <code>i = 1, 2, ..., n-1</code>，首先把钢条切割为长度为<code>i</code>和<code>n-i</code>的两段，接着求解这两段的最优切割收益r<sub>n</sub>和r<sub>n-1</sub>。由于无法预知哪种方案会获得最优收益，我们必须考察所有可能的<code>i</code>，选取其中收益最大者。如果直接出售原钢条会获得最大收益，我们当然可以选择不做任何切割。</p>
<p>注意到，为了求解规模为<code>n</code>的原问题，我们先求解形式完全相同，但规模更小的子问题。即当完成首次切割后，我们将两端钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案中提取组合收益最大者，构成原问题的最优解。我们称钢条切割问题满足最优子结构性质: 问题的最优解由相关子问题的最优解组合而成，而这些子问题可以独立求解</p>
<h4 id="解决方案1—使用自顶向下的递归方法求解"><a href="#解决方案1—使用自顶向下的递归方法求解" class="headerlink" title="解决方案1—使用自顶向下的递归方法求解"></a>解决方案1—使用自顶向下的递归方法求解</h4><p>我们将钢条从左边切割下长度为 <code>i</code> 的一段，支队右边剩下的长度为 <code>n-i</code> 的一段继续进行切割(递归求解)，对左边的一段则不再进行切割。即问题分解的方式为：将长度为<code>n</code>的钢条分解为左边开始一段，以及剩余部分继续分解的结果。</p>
<p>这样，不做任何切割的方案就可以描述为: 第一段的长度为 <code>n</code>，收益为 p<sub>n</sub>，剩余部分长度为0，对应的收益为r<sub>0</sub> = 0。</p>
<p>公式如下<br><img src="/2019/03/11/算法导论-15-动态规划/0310_3.PNG" alt=""></p>
<p>伪代码如下,它采用的是一种直接的自顶向下的递归方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CUT-ROD(p, n)</span><br><span class="line">    if n == 0</span><br><span class="line">        return 0</span><br><span class="line">    q = -∞</span><br><span class="line">    for i = 1 to n</span><br><span class="line">        q = max(q, p[i] + CUT-ROD(p, n-i))</span><br><span class="line">    return q</span><br></pre></td></tr></table></figure></p>
<p>一旦输入规模稍微变大，程序运行时间会变得相当长。每当将 <code>n</code> 增加 <code>1</code> ，程序运行时间差不多会增加 <code>1</code> 倍。</p>
<p>为什么<code>CUT-ROD</code>的效率如此差，原因是，<code>CUT-ROD</code>反复地用相同的参数值对自身进行递归调用，即它反复求解相同的子问题。如下图所示<br><img src="/2019/03/11/算法导论-15-动态规划/0310_4.PNG" alt=""></p>
<p>此算法的时间复杂度为<br><img src="/2019/03/11/算法导论-15-动态规划/0310_5.PNG" alt=""></p>
<h4 id="解决方案2—使用动态规划方法求解最优钢条切割问题"><a href="#解决方案2—使用动态规划方法求解最优钢条切割问题" class="headerlink" title="解决方案2—使用动态规划方法求解最优钢条切割问题"></a>解决方案2—使用动态规划方法求解最优钢条切割问题</h4><p>动态规划的思想如下所述。我们已经看到，朴素递归算法之所以效率很低，是因为它反复求解同样的子问题。因此，动态规划方法仔细安排求解顺序，对每个问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果，而不必重复计算。因此，动态规划方法是付出额外的内存空间来节省计算时间，是典型的时空权衡(time-memory-trade-off)的例子。而时间上的节省可能是非常巨大的:可能将指数时间的解转化为一个多项式时间的解。</p>
<p>动态规划有两种等价的实现方法</p>
<ol>
<li><p>第一种方法称为带备忘的自顶向下法(top-down with memorization)。此方法仍按照自然的递归形式编写过程，但过程会保存每个子问题的解(通常保存在一个数组或散列表中)。当需要一个子问题的解时，过程首先检查是否已经保存过此解。如果是，则直接返回保存的值，从而节省了计算时间；否则，按通常方式计算这个子问题。我们称这个递归过程是带备忘的(memoized)，因为它记住了之前已经计算出的结果。</p>
</li>
<li><p>第二种方法称为自底向上法(bottom-up method)。这种方法一般需要恰当定义子问题”规模”的概念，使得任何子问题的求解都只依赖于”更小的”子问题的求解。因此我们可以将子问题按规模排序，按照由小至大的顺序进行求解。当求解某个子问题时，它所依赖的那些更小的子问题都已求解完毕，结果已经保存。每个子问题只需求解一次，当我们求解它时，它的所有前提子问题都已求解完成</p>
</li>
</ol>
<p>下面给出的是带备忘的自顶向下法的伪代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">MEMOIZED-CUT-ROD(p,n)</span><br><span class="line">    let r[0..n] be a new array</span><br><span class="line">    for i = 0 to n</span><br><span class="line">        r[i] = -∞</span><br><span class="line">MEMORIZED-CUT-ROD-AUX(p,n,r)</span><br><span class="line">    if r[n] &gt;= 0</span><br><span class="line">        return r[n]</span><br><span class="line">    if n == 0</span><br><span class="line">        q = 0</span><br><span class="line">    else q = -∞</span><br><span class="line">        for i = 1 to n</span><br><span class="line">            q = max(q, p[i] + MEMOIZED-CUT-ROD-AUX(p, n-i, r))</span><br><span class="line">    r[n] = q</span><br><span class="line">    return q</span><br></pre></td></tr></table></figure></p>
<p>自底向上版本更为简单<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">BOTTOM-UP-CUT-ROD(p, n)</span><br><span class="line">    let r[0..n] be a new array</span><br><span class="line">    r[0] = 0</span><br><span class="line">    for j = 1 to n</span><br><span class="line">        q = -∞</span><br><span class="line">        for i = 1 to j</span><br><span class="line">            q = max(q, p[i] + r[j - i])</span><br><span class="line">        r[j] = q</span><br><span class="line">    return r[n]</span><br></pre></td></tr></table></figure></p>
<p>自底向上版本采用子问题的自然顺序；若 <code>i&lt;j</code>，则规模为 <code>i</code> 的子问题比规模为 <code>j</code> 的子问题”更小”。因此，过程依次求解规模为 <code>j = 0, 1, ..., n</code> 的子问题</p>
<p>此算法的时间复杂度为 Θ(n<sup>2</sup>)</p>
<h4 id="子问题图"><a href="#子问题图" class="headerlink" title="子问题图"></a>子问题图</h4><p>当思考一个动态规划问题时，我们应该弄清所涉及的子问题及子问题之间的依赖关系</p>
<p>问题的子问题图准确地表达了这些信息，下图显示了 <code>n=4</code> 时钢条切割问题地子问题图。<br>它是一个有向图，每个顶点唯一地对应一个子问题。若求子问题x地最优解时需要直接用到子问题y地最优解，那么在子问题图中就会有一条从子问题x的定点到子问题y的定点的有向边。</p>
<p><img src="/2019/03/11/算法导论-15-动态规划/0310_6.PNG" alt=""></p>
<p>自底向上的动态规划方法处理子问题图中顶点的顺序为: 对于一个给定的子问题<code>x</code>，在求解它之前求解邻接它的子问题<code>y</code>。对于任何子问题，直至它依赖的所有子问题均已求解完成，才会求解它</p>
<p>子问题图<code>G=(V, E)</code>的规模可以帮我们确定动态规划算法的运行时间。由于每个子问题只求解一次，因此算法运行时间等于每个子问题求解时间之和。通常，一个子问题的求解时间与子问题图中对应顶点的度(出射边的数目)成正比，而子问题的数目等于子问题图的顶点数。因此，通常情况下，动态规划算法的运行使劲按与顶点与边的数量呈线性关系</p>
<h3 id="15-2-矩阵链乘法"><a href="#15-2-矩阵链乘法" class="headerlink" title="15.2 矩阵链乘法"></a>15.2 矩阵链乘法</h3><h4 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h4><p>给定一个 <code>n</code> 个矩阵的序列(矩阵链)(A<sub>1</sub>),我们希望计算它们的乘积如下</p>
<p>A<sub>1</sub>A<sub>2</sub>…A<sub>n</sub></p>
<p>由于矩阵乘法满足结合律，因此任何加括号的方法都会得到相同的计算结果。我们称有如下性质的矩阵乘积链为完全括号化的(fully parenthesized): 它是单一矩阵，或者是两个完全括号化的矩阵乘积链的积，且已外加括号。</p>
<p>例如，如果矩阵链为(A1, A2, A3, A4)，则共有5种完全括号化的矩阵乘积链</p>
<p>(A1(A2(A3A4)))<br>(A1((A2A3)A4))<br>((A1A2)(A3A4))<br>((A1(A2A3))A4)<br>(((A1A2)A3)A4)  </p>
<p>对矩阵链加乘法的方式会对矩阵运算的代价产生重大影响</p>
<p>两个矩阵 <code>A</code> 和 <code>B</code> 只有相容(compatible)，即 <code>A</code> 的列数等于 <code>B</code> 的行数时，才能相乘。如果 <code>A</code> 是 <code>p * q</code> 的矩阵,<code>B</code> 是 <code>q * r</code> 的矩阵，那么乘积 <code>C</code> 是 <code>p * r</code> 的矩阵。计算 <code>C</code> 所需时间由标量乘法的次数决定，即 <code>p * q * r</code></p>
<p>以矩阵链 <code>(A1, A2, A3)</code> 相乘为例，来说明不同的加括号方式会导致不同的计算代价。假设三个矩阵的规模分别是 <code>10 * 100</code>、<code>100 * 5</code>、 <code>5 * 50</code>。如果按照 <code>((A1A2)A3)</code> 的顺序计算，为计算 <code>A1A2</code> ，需要做 <code>10 * 100 *5</code> 次标量乘法，再与A3相乘又需要做 <code>10 * 5 * 50</code> 次标量乘法，一共需要<code>7500</code>次。假如按照 <code>((A1)A2A3)</code> 的顺序，需要 <code>75000</code> 次</p>
<p>矩阵链乘法问题(matrix-chain multiplication problem): 给定n个矩阵的链 <code>(A1, A2, ..., An)</code>, 矩阵<code>Ai</code> 的规模是p<sub>i-1</sub> * p<sub>i</sub>(1&lt;=i&lt;=n)，求完全括号化方案，使得计算乘积 <code>A1A2...An</code> 所需标量乘法次数最少</p>
<h4 id="计算括号化方案的数量"><a href="#计算括号化方案的数量" class="headerlink" title="计算括号化方案的数量"></a>计算括号化方案的数量</h4><p>首先说明穷举所有可能的括号化方案不会产生一个高效的算法</p>
<p>对于一个 <code>n</code> 个矩阵的链，令 <code>P(n)</code> 表示可供选择的括号化方案的数量。当 <code>n=1</code> 时，由于只有一个矩阵，因此只有一种完全括号化方案。当 <code>n&gt;=2</code> 时，完全括号化的矩阵乘积可描述为两个完全括号化的部分积相乘的形式，而两个部分积的划分点在第 <code>k</code> 个矩阵和第 <code>k+1</code> 个矩阵之间，<code>k</code> 为 <code>1, 2, ...., n - 1</code>中的任意一个值。可以得到递归式</p>
<p><img src="/2019/03/11/算法导论-15-动态规划/0310_7.PNG" alt=""></p>
<p>经过证明，这种方法的时间复杂度为 Ω(2<sup>n</sup>)。因此通过暴力搜索穷尽所有可能的括号化方案来寻求最优方案，是一个糟糕的策略</p>
<h4 id="应用动态规划方法"><a href="#应用动态规划方法" class="headerlink" title="应用动态规划方法"></a>应用动态规划方法</h4><p>我们按照如下4个步骤来进行</p>
<ol>
<li>刻画一个最优解的结构特征(寻找最优子结构)</li>
<li>递归地定义最优解的值</li>
<li>计算最优解地值，通常采用自底向上的方法</li>
<li>利用计算出的信息构建一个最优解</li>
</ol>
<h5 id="步骤1-最优括号化方案的结构特征-寻找最优子结构"><a href="#步骤1-最优括号化方案的结构特征-寻找最优子结构" class="headerlink" title="步骤1 : 最优括号化方案的结构特征(寻找最优子结构)"></a>步骤1 : 最优括号化方案的结构特征(寻找最优子结构)</h5><p>动态规划的第一步是寻找最优子结构，然后就可以利用这种子结构从子问题的最优解构造原问题的最优解</p>
<p>下面给出本问题的最优子结构。假设<code>Ai Ai+1 ... Aj</code>的最优括号化方案的分割点在 <code>Ak</code> 和 <code>Ak+1</code> 之间。那么继续对子链<code>Ai Ai+1 ... Ak</code>和<code>Ak+1 ... Aj</code>进行括号化时，我们应该直接采用独立求解它时所得的最优方案</p>
<p>我们已经看到，一个非平凡的矩阵链乘法问题的实例的任何解都需要划分链，而任何最优解都是由子问题实例的最优解构成的。因此，为了构建一个矩阵链乘法问题实例的最优解，我们可以将问题划分为两个子问题(<code>Ai Ai+1 ... Ak</code>和<code>Ak+1 ... Aj</code>的最优括号化问题)，求出子问题实例的最优解，然后将子问题的最优解组合起来。我们必须保证在确定分割点时，已经考察了所有可能的划分点，这样就可以保证不会遗漏最优解</p>
<h5 id="步骤2-一个递归求解方案-递归地定义最优解的值"><a href="#步骤2-一个递归求解方案-递归地定义最优解的值" class="headerlink" title="步骤2 : 一个递归求解方案(递归地定义最优解的值)"></a>步骤2 : 一个递归求解方案(递归地定义最优解的值)</h5><p>下面用子问题的最优解来递归地定义原问题最优解地代价。对矩阵链乘法问题，我们可以将对所有 <code>1&lt;=i&lt;=j&lt;=n</code>确定<code>Ai Ai+1 ... Aj</code>的最小代价括号化方案作为子问题。令<code>m[i, j]</code>表示计算矩阵 A<sub>i <em> j</em></sub>所需标量乘法次数的最小值，那么。原问题的最优解—计算A<sub>1  n</sub>所需的最低代价就是<code>m[1, n]</code></p>
<p>我们可以递归地定义<code>m[i, j]</code>如下。</p>
<p>对于 i=j 时的平凡问题，矩阵链只包含唯一地矩阵A<sub>i * i</sub> = A<sub>i</sub>，不需要做任何标量乘法计算。所以，对所有 <code>i = 1, 2, ..., n</code> 来说，<code>m[i, i] = 0</code>。</p>
<p>若 <code>i&lt;j</code>，我们利用步骤1中得到的最优子结构来计算<code>m[i, j]</code>。我们假设<code>Ai Ai+1 ... Aj</code>的最优括号化方案的分割点在矩阵<code>Ak</code>和<code>Ak+1</code>之间，其中<code>i&lt;=k&lt;j</code>。那么，<code>m[i, j]</code>就等于计算<code>Ai * k</code>和<code>Ak+1 * j</code>的代价加上两者相乘的代价的最小值。由于矩阵Ai的大小为 <code>pi-1 * pi</code>，易知<code>Ai * k</code>和<code>Ak+1 * j</code>相乘的代价为<code>pi-1 * pk * pj</code>次标量乘法运算。因此我们得到<br><img src="/2019/03/11/算法导论-15-动态规划/0310_8.PNG" alt=""></p>
<p>上述递归公式假定最优分割点 <code>k</code> 是已知的，但实际上不知道。不过 <code>k</code> 只有 <code>j-i</code> 种可能的取值。我们只需检查这<code>j-i</code>种情况，找出最优者即可。一次迭代求解公式变为<br><img src="/2019/03/11/算法导论-15-动态规划/0310_9.PNG" alt=""></p>
<h5 id="步骤3-计算最优代价"><a href="#步骤3-计算最优代价" class="headerlink" title="步骤3 : 计算最优代价"></a>步骤3 : 计算最优代价</h5><p>现在，我们可以很容易地基于上面的递归公式写出一个递归算法，来计算 <code>A1A2...An</code> 相乘的最小代价 <code>m[1, n]</code> 。此递归算法是指数时间的，并不比暴力搜索算法好</p>
<p>此递归算法会在递归调用中多次遇到同一个子问题，这种子问题重叠的性质是应用动态规划的另一个标志</p>
<p>我们采用自底向上表格法来计算最优代价。假定矩阵Ai的规模为 <code>pi-1 * pi(i = 1, 2, ..., n)</code>。它的输入是一个序列<code>p=(p0, p1, ..., pn)</code>，其长度为<code>p.length = n + 1</code>。过程用一个辅助表<code>m[1..n, 1..n]</code>来保存代码<code>m[i, j]</code>，用另一个辅助表<code>s[1..n-1, 2..n]</code>记录最优值<code>m[i, j]</code>对应的分割点<code>k</code>。我们就可以利用表s构建最优解。</p>
<p>为了实现自底向上的方法，我们必须确定计算 <code>m[i, j]</code> 时需要访问哪些其他表项。<code>m[i, j]</code> 的值只依赖于那些少于 <code>j-i+1</code> 个矩阵链相乘的最优计算代价。因此算法应该按照长度递增的顺序求解矩阵链括号化问题，并按对应的顺序填写表<code>m</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MATRIX-CHAIN-ORDER(p)</span><br><span class="line">    n = p.length - 1</span><br><span class="line">    let m[1..n,1..n] and s[1..n-1, 2..n]be new tables</span><br><span class="line">    for i = 1 to n</span><br><span class="line">        m[i, i] = 0</span><br><span class="line">    for l = 2 to n</span><br><span class="line">        for i = 1 to n - l + 1</span><br><span class="line">        j = i + l - 1</span><br><span class="line">        m[i, j] = ∞</span><br><span class="line">        for k = i to j - 1</span><br><span class="line">            q = m[i, k] + m[k+1, j] + p[i - 1] * p[k] * [pj]</span><br><span class="line">        if q &lt; m[i, j]</span><br><span class="line">            m[i, j] = q</span><br><span class="line">            s[i, j] = k</span><br><span class="line">    return m and s</span><br></pre></td></tr></table></figure>
<p>下图展示了对一个长度为6的矩阵链执行此算法的过程<br><img src="/2019/03/11/算法导论-15-动态规划/0310_10.PNG" alt=""></p>
<p>简单分析<code>MATRIX-CHAIN-ORDER</code>的嵌套循环结构，可以看到算法的运算时间为O(n<sup>3</sup>)。</p>
<h5 id="步骤4-构建最优解"><a href="#步骤4-构建最优解" class="headerlink" title="步骤4: 构建最优解"></a>步骤4: 构建最优解</h5><p>我们可以根据表s记录的信息来递归地恢复最优解，算法如图<br><img src="/2019/03/11/算法导论-15-动态规划/0310_11.PNG" alt=""></p>
<h3 id="15-3-动态规划原理"><a href="#15-3-动态规划原理" class="headerlink" title="15.3 动态规划原理"></a>15.3 动态规划原理</h3><p>在本节我们将关注适合应用动态规划方法求解的最优化问题应该具备的两个要素: 最优子结构和子问题重叠</p>
<h4 id="最优子结构"><a href="#最优子结构" class="headerlink" title="最优子结构"></a>最优子结构</h4><p>用动态规划问题求解最优化问题的第一步就是刻画最优解的结构。如果一个问题的最优解包含其子问题的最优解，我们就称此问题具有最优子结构性质。因此，某个问题是否适合应用动态规划算法，它是否具有最优子结构性质是一个好结构(当然，具有最优子结构性质也可能意味着适合应用贪心策略)</p>
<p>本章前面的两个问题都具有最优子结构性质。在15.1中，我们观察到，长度为 <code>n</code> 的钢条的最优切割方案是由第一次切割后得到的两端钢条的最优切割方案组成的。在15.2中，我们看到 <code>Ai Ai+1 ... Aj</code> 的最优括号化方案首先在 <code>Ak</code> 和 <code>Ak+1</code>之间进行划分，然后对 <code>Ai Ai+1 ... Ak</code> 和 <code>Ak+1 Ak+2 ... Aj</code>继续进行最优括号化</p>
<p>发掘最优子结构性质的过程，遵循如下通用模式</p>
<ol>
<li><p>证明问题最优解的一个组成部分是做出一个选择，例如，选择钢条第一次切割位置，选择矩阵链的划分位置等，做出这次选择会产生一个或多个待解决的子问题</p>
</li>
<li><p>对于一个给定问题，在其可能的第一步选择中，你假定已经知道哪种选择才会得到最优解，你现在并不关心这种选择具体是如何得到的，只是假定已经知道了这种选择</p>
</li>
<li><p>给定可获得最优解的选择后，你确定这次选择会产生哪些子问题，以及如何最好地刻画子问题空间</p>
</li>
<li><p>利用”剪贴-粘贴”(cut-and-paste)技术证明: 作为构成子问题最优解的组成部分，每个子问题的解就是它本身的最优解。证明这一点可以用反证法: 假定子问题的解不是其自身的最优解，那么我们就可以从原问题的解中”剪切”掉哪些非最优解，将最优解”粘贴”进去，从而得到原问题一个更优的解，这与最初的解是原问题最优解的前提假设矛盾。</p>
</li>
</ol>
<p>对于不同问题领域，最优子结构的不同体现在两个方面:</p>
<ol>
<li><p>原问题的最优解中涉及多少个子问题，以及</p>
</li>
<li><p>在确定最优解使用哪些子问题时，我们需要考察多少种选择</p>
</li>
</ol>
<p>在钢条切割问题中，长度为 <code>n</code> 的钢条的最优切割方案仅仅使用一个子问题(长度为<code>n - i</code> 的钢条的最有切割)，但必须考察 <code>i</code> 的 <code>n</code> 种不同取值，来确定哪一个会产生最优解。 <code>Ai Ai+1 ... Aj</code> 的矩阵链乘法问题种，最优解使用两个子问题，我们需要考察 <code>j - i</code> 种情况。对于给定的矩阵链划分位置—-矩阵<code>Ak</code>，我们需要求解两个子问题—- <code>Ai Ai+1 ... Ak</code>和<code>Ak+1 Ak+2 .. Aj</code>的括号化方案—-而且两个子问题都必须求解最优方案。一旦确定了子问题的最优解，就可以在 <code>j - i</code> 个候选的 <code>k</code> 中选取最优者</p>
<p>我们可以用子问题的总数和每个子问题需要考察多少种选择这两个因素来粗略分析动态规划算法的运行时间。对于钢条切割问题，共有 Θ(n)个子问题，每个子问题最多需要考察 n 种选择，因此运行时间为O(n<sup>2</sup>)</p>
<h4 id="一些微妙之处—-子问题无关性"><a href="#一些微妙之处—-子问题无关性" class="headerlink" title="一些微妙之处—-子问题无关性"></a>一些微妙之处—-子问题无关性</h4><p>为什么最长简单路径问题的子结构与最短路径有这么大的差别？原因在于，虽然最长路径问题和最短路径问题的解都用到了两个子问题，但两个最长简单路径子问题是相关的，而两个最短路径子问题是无关的。这里，子问题无关的含义是，同一个原问题的一个子问题的解不影响另一个子问题的解。换个角度来开，我们所面临的困境就是：求解一个子问题时用到了某些资源，导致这些资源在求解其他子问题时不可用。</p>
<h4 id="重叠子问题"><a href="#重叠子问题" class="headerlink" title="重叠子问题"></a>重叠子问题</h4><p>适合用动态规划方法求解的最优化问题应该具备的第二个性质是子问题空间必须足够小，即问题的递归算法会反复求解相同的子问题，而不是一直产生新的子问题，一般来讲，不同子问题的总数是输入规模的多项式函数为好。如果递归算法求解相同的子问题，我们就称最优化问题具有重叠子问题性质。与之相对的，适合用分治方法求解的问题通常在递归的每一步都生成一个全新的子问题。动态规划算法通常这样利用重叠子问题性质: 对每个子问题求解一次，将解存在一张表种，当再次需要这个子问题时，直接查表，每次查表代价为常数时间</p>
<h4 id="重构最优解"><a href="#重构最优解" class="headerlink" title="重构最优解"></a>重构最优解</h4><p>从实际考虑，我们通常将每个子问题所作的选择存在一个表中，这样就不必根据代价值来重构这些信息</p>
<h4 id="备忘"><a href="#备忘" class="headerlink" title="备忘"></a>备忘</h4><p>带备忘的递归算法为每个子问题维护一个表项来保存它的解。每个表项的初值设为一个特殊值，表示尚未填入子问题的解。当递归调用过程第一次遇到子问题时，计算其解，并存入对应表项，随后每次遇到同一个子问题，只是简单地查表，返回其解</p>
<p>为求解矩阵链乘法问题，我们既可以用带备忘地自顶向下动态规划算法，也可以用自底向上的动态规划算法，时间复杂度均为 O(n<sup>3</sup>)。两个方法都利用了重叠子问题性质。不同的子问题一共只有Θ(n<sup>2</sup>)个，对每个子问题，两种方法都只计算一次。而没有备忘机制的自然递归算法的运行时间是指数阶，因为它会反复求解同样的子问题</p>
<p>通常情况下，如果每个子问题必须至少求解一次，自底向上动态规划算法会比自顶向下备忘算法快(都是O(n<sup>3</sup>)时间，相差一个常量系数)，因为自底向下算法没有递归调用的开销，表的维护开销也更小。</p>
<p>相反，如果子问题空间中的某些子问题完全不必求解，备忘方法就体现其优势了，因为它只会求解那些绝对必要的子问题</p>
<h3 id="15-4-最长公共子序列"><a href="#15-4-最长公共子序列" class="headerlink" title="15.4 最长公共子序列"></a>15.4 最长公共子序列</h3><h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><p>一种衡量DNA串S<sub>1</sub>和S<sub>2</sub>的相似度的方式是：寻找第三个串S<sub>3</sub>，它的所有碱基也都出现在S<sub>1</sub>和S<sub>2</sub>中，且在三个串中出现的顺序都相同，但在S<sub>1</sub>和S<sub>2</sub>中不要求连续出现，可以找到的S<sub>3</sub>越长，就可以认为S<sub>1</sub>和S<sub>2</sub>的相似度越高, 这种相似度概念就是最大公共子序列问题。</p>
<p>一个给定序列的子序列，就是将给定序列中零个或多个元素去掉之后得到的结果。其形式化定义如下: 给定一个序列 X = (x<sub>1</sub>, x<sub>2</sub>, …, x<sub>m</sub>),另一个序列 Z = (z<sub>1</sub>, z<sub>2</sub>, …, z<sub>k</sub>)满足如下条件时称为X的子序列(subsequence)，即存在一个严格递增的X的下标序列(i<sub>1</sub>, i<sub>2</sub>, …, i<sub>k</sub>),对所有j = 1, 2, …k,满足 x<sub>i<sub>j</sub></sub> = z<sub>j</sub>。</p>
<p>最大公共子序列问题(longest-common-subsequence problem)给定两个序列 X = (x<sub>1</sub>, x<sub>2</sub>, …, x<sub>m</sub>) 和 Y = (y<sub>1</sub>, y<sub>2</sub>, …, y<sub>n</sub>),求 X 和 Y 长度最长的公共子序列。</p>
<h4 id="步骤1-刻画最大公共子序列的特征"><a href="#步骤1-刻画最大公共子序列的特征" class="headerlink" title="步骤1: 刻画最大公共子序列的特征"></a>步骤1: 刻画最大公共子序列的特征</h4><p>如下面的定理所示，LCS问题具有最优子结构性质。子问题的自然分类对应两个输入序列的”前缀”对。</p>
<p>定理 15.1 (LCS的最优子结构) 令 <code>X=&lt;x1, x2, ..., xm&gt;</code> 和 <code>Y=&lt;y1, y2, ..., yn&gt;</code> 为两个序列，<code>Z=&lt;z1, z2, ..., zk&gt;</code> 为 <code>X</code> 和 <code>Y</code> 的任意LCS</p>
<ol>
<li>如果 x<sub>m</sub> = y<sub>n</sub>，则z<sub>k</sub>=x<sub>m</sub>=y<sub>n</sub>,且Z<sub>k-1</sub>是X<sub>m-1</sub>和Y<sub>n-1</sub>的一个LCS</li>
<li>如果 x<sub>m</sub> != y<sub>n</sub>，那么z<sub>k</sub> != x<sub>m</sub>,意味着Z是X<sub>m-1</sub>和Y的一个LCS</li>
<li>如果 x<sub>m</sub> != y<sub>n</sub>，那么z<sub>k</sub> != y<sub>n</sub>,意味着Z是X和Y<sub>n-1</sub>的一个LCS</li>
</ol>
<p>定理15.1告诉我们，两个序列的LCS包含两个序列前缀的LCS。因此，LCS问题具有最优子结构性质</p>
<h4 id="步骤2-一个递归的求解方案"><a href="#步骤2-一个递归的求解方案" class="headerlink" title="步骤2: 一个递归的求解方案"></a>步骤2: 一个递归的求解方案</h4><p>定理 15.1 意味着，在求 <code>X=&lt;x1, x2, ..., xm&gt;</code> 和 <code>Y=&lt;y1, y2, ..., yn&gt;</code> 的一个LCS时，我们需要求解一个或两个子问题。如果 x<sub>m</sub> = y<sub>n</sub>, 我们应该求解X<sub>m-1</sub>和Y<sub>n-1</sub>的一个LCS。将 x<sub>m</sub> = y<sub>n</sub> 追加到一个LCS的末尾，就得到 X 和 Y 的一个LCS。如果 x<sub>m</sub> != y<sub>n</sub>，我们必须求解两个子问题，求 X<sub>m-1</sub> 和 Y 的一个LCS与 X 和 Y<sub>n-1</sub> 的一个LCS。两个LCS较长者即为 X 和 Y 的一个LCS。</p>
<p>可以看出LCS问题的重叠子问题性质。为了求  X 和 Y 的LCS，要求 X<sub>m-1</sub> 和 Y 的一个LCS与 X 和 Y<sub>n-1</sub> 的LCS。但者几个子问题都包含求X<sub>m-1</sub>和Y<sub>n-1</sub>的LCS。</p>
<p>设计LCS问题首先要建立最优解的递归式，如下</p>
<p><img src="/2019/03/11/算法导论-15-动态规划/0311_1.PNG" alt=""></p>
<p>LCS问题是一个根据条件排除子问题的动态规划算法</p>
<h4 id="步骤3-计算LCS的长度"><a href="#步骤3-计算LCS的长度" class="headerlink" title="步骤3: 计算LCS的长度"></a>步骤3: 计算LCS的长度</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">LCS-LENGTH(X, Y)</span><br><span class="line"></span><br><span class="line">m = X.length</span><br><span class="line">n = Y.length</span><br><span class="line">let b[1..m, 1..n] and c[0..m, 0..n] be new table</span><br><span class="line">for i = 1 to m</span><br><span class="line">    c[i, 0] = 0</span><br><span class="line">for j = 0 to n</span><br><span class="line">    c[0, j] = 0</span><br><span class="line">for i = 1 to m</span><br><span class="line">    for j = 1 to n</span><br><span class="line">        if xi == yi</span><br><span class="line">            c[i, i] = c[i - 1, j - 1] + 1</span><br><span class="line">            b[i, j] = &quot;↖&quot;</span><br><span class="line">        elseif c[i - 1, j] &gt;= c[i, j - 1]</span><br><span class="line">            c[i, j] = c[i - 1, j]</span><br><span class="line">            b[i, j] = &quot;↑&quot;</span><br><span class="line">        else </span><br><span class="line">            c[i, j] = c[i, j - 1]</span><br><span class="line">            b[i, j] = &quot;←&quot;</span><br></pre></td></tr></table></figure>
<p>过程LCS-LENGTH接受两个序列  <code>X=&lt;x1, x2, ..., xm&gt;</code> 和 <code>Y=&lt;y1, y2, ..., yn&gt;</code> 作为输入。它将 <code>c[i, j]</code> 的值保存在表 <code>c[0..m, 0..n]</code>中，并按行主次序(row-major order)计算表项(即首先由左至右计算c的第一行，然后计算第二行，依次类推)。过程还维护一个表<code>b[0..m, 0..n]</code>，帮助构建最优解。<code>b[i, j]</code> 指向的表项对应计算 <code>c[i, j]</code> 时所选择的子问题最优解。过程返回表b和表c, <code>c[m, n]</code> 保存了 <code>X</code> 和 <code>Y</code> 的LCS的长度</p>
<h4 id="步骤4-构造LCS"><a href="#步骤4-构造LCS" class="headerlink" title="步骤4: 构造LCS"></a>步骤4: 构造LCS</h4><p>略</p>
<h4 id="算法改进"><a href="#算法改进" class="headerlink" title="算法改进"></a>算法改进</h4><p>略</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/10/Tensorflow-8-循环神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="T0UGH(GuiPing Wang)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="打怪升级日常">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/Tensorflow-8-循环神经网络/" itemprop="url">[Tensorflow][8]循环神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-10T14:54:13+08:00">
                2019-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/10/Tensorflow-8-循环神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/10/Tensorflow-8-循环神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第8章-循环神经网络"><a href="#第8章-循环神经网络" class="headerlink" title="第8章 循环神经网络"></a>第8章 循环神经网络</h2><ul>
<li>本章将介绍<ol>
<li>循环神经网络(recurrent neural network, RNN)</li>
<li>长短时记忆网络(long short-term memory, LSTM)—–循环神经网络中的一个重要结构</li>
</ol>
</li>
</ul>
<hr>
<h3 id="8-1-循环神经网络简介"><a href="#8-1-循环神经网络简介" class="headerlink" title="8.1 循环神经网络简介"></a>8.1 循环神经网络简介</h3><ul>
<li><p>循环神经网络的历史、传统机器学习和全连接的瓶颈</p>
<ul>
<li>循环神经网络(RNN)源自1982年由Saratha Sathasivam提出的霍普菲尔德网络</li>
<li>传统的机器学习算法非常依赖于人工提取的特征</li>
<li>基于全连接神经网络的方法也存在参数太多，无法利用数据中时间序列信息等问题</li>
<li>循环神经网络挖掘数据中时序信息以及语义信息的深度表达能力被充分利用，并在语音识别、语言模型、机器翻译以及时序分析等方面实现了突破</li>
</ul>
</li>
<li><p>循环神经网络简介</p>
<ul>
<li>循环神经网络的主要用途是处理和预测<strong>序列</strong>数据</li>
<li>循环神经网络的来源就是为了刻画一个序列<strong>当前的输入</strong>与<strong>之前信息</strong>的关系</li>
<li>从网络结构上，循环神经网络会记忆之前的信息，并利用之前的信息影响后面节点的输出</li>
<li>循环神经网络的隐藏层之间的节点是有连接的，<strong>隐藏层的输入</strong>不仅包括<strong>输入层的输入</strong>，还包括<strong>上一时刻隐藏层的输入</strong></li>
</ul>
</li>
<li><p>循环神经网络的经典结构</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/循环神经网络经典结构.jpg" alt=""></li>
<li>解释<ul>
<li>在每一时刻t,循环神经网络会针对该时刻的输入结合当前模型的状态给出一个输出，并更新模型状态</li>
<li>循环神经网络的主体结构A的输入除了来自输入层xt，还有一个循环的边来提供上一时刻的隐藏状态(hidden state)ht-1</li>
<li>在每一个时刻，循环神经网络的模块A在读取了xt和ht-1之后会生成新的隐藏状态ht,并产生本时刻的输出ot</li>
<li>由于模块A中的运算和变量在不同时刻是相同的，因此循环神经网络理论上可以被看作是同一神经网络结构无限复制的结果</li>
<li>循环神经网络是在不同时间位置共享参数，从而能够使用有限的参数处理任意长度的序列</li>
</ul>
</li>
</ul>
</li>
<li><p>循环神经网络经典结构展开</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/循环神经网络经典结构展开.jpg" alt=""></li>
<li>循环神经网络对长度为N的序列展开之后，可以视为一个有N个中间层的前馈神经网络</li>
<li>这个前馈神经网络可以直接使用反向传播算法进行训练，这样的训练方法称为”沿时间反向传播”(Back-Propagation Through Time)</li>
</ul>
</li>
<li><p>循环神经网络与时间序列处理</p>
<ul>
<li>从循环神经网络的结构特征可以很容易看出它擅长解决与时间序列相关的问题</li>
<li>对于一个序列数据，可以将这个序列上不同时刻的数据依次传入循环神经网络的输入层</li>
<li>输出可以是对序列中下一个时刻的预测，也可以是对当前时刻信息的处理结果</li>
<li>循环神经网络要求每个时刻都有一个输入，但不一定每个时刻都需要有输出</li>
</ul>
</li>
<li><p>以机器翻译为例介绍循环神经网络如何解决实际问题</p>
<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/循环神经网络实现机器翻译示意图.jpg" alt=""></li>
<li>循环神经网络中每一个时刻的输入为需要翻译的句子中的单词</li>
<li>需要翻译的句子是ABCD，那么循环神经网络第一段每一个时刻的输入就分别为A、B、C、D，然后用“_”作为待翻译句子的结束符</li>
<li>在第一段，循环神经网络没有输出</li>
<li>从结束符”_”开始，循环神经网络进入翻译阶段</li>
<li>该阶段中每个时刻的输入是上一个时刻的输出，而最终得到的输出XYZ就是句子ABCD翻译的结果</li>
</ul>
</li>
<li><p>循环体的概念</p>
<ul>
<li>循环神经网络可以看作是同一神经网络结构在时间序列上被复制多次的结果，这个被复制多次的结构被称为循环体</li>
<li>一个简单的循环体<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/简单的循环体.jpg" alt=""></li>
</ul>
</li>
<li><p>循环神经网络的前向传播简介</p>
<ul>
<li>循环体中的神经网络的输入有两部分<ol>
<li>上一时刻的状态:ht-1</li>
<li>当前时刻的输入样本:xt</li>
</ol>
</li>
<li>将上一时刻的状态与当前时刻的输入拼接为一个大的向量作为循环体中神经网络的输入<ul>
<li>例如：假设输入向量的维度为x，隐藏状态的维度为n，那么循环体的全连接网络的输入大小为n+x</li>
</ul>
</li>
<li>输出为当前时刻的状态ht，一个长度为n的向量</li>
<li>然而有时RNN输出不仅需要提供给下一时刻作为状态，也需要提供给当前时刻作为当前时刻的输出</li>
<li>此时需要一个全连接层，输入为前面计算出的状态向量，输出为当前时刻的输出，而不同时刻用于产生当前时刻输出的全连接层的参数也是共享的</li>
<li>举例：一个循环神经网络前向传播的具体计算过程<ul>
<li>如图<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/循环神经网络的前向传播计算过程.jpg" alt=""></li>
<li>计算过程详解<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/RNN前向传播计算过程详解1.jpg" alt=""><br><img src="/2019/03/10/Tensorflow-8-循环神经网络/RNN前向传播计算过程详解2.jpg" alt=""></li>
</ul>
</li>
</ul>
</li>
<li><p>循环神经网络的损失函数和优化过程</p>
<ul>
<li>损失函数：因为RNN在每个时刻都有一个输出，所以循环神经网络的总损失为所有时刻上的损失函数的综合</li>
<li>优化过程：在定义完损失函数之后，套用第四章介绍的优化框架即可完成模型训练的过程</li>
<li>在实际训练过程中，如果序列太长，会导致优化时出现梯度消散和梯度爆炸的问题</li>
</ul>
</li>
<li><p>循环神经网络前向传播样例代码</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">state = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义循环体A中产生当前状态的全连接层的参数，包括权重和偏置</span></span><br><span class="line">w_cell_state = np.asarray([[<span class="number">0.1</span>, <span class="number">0.2</span>], [<span class="number">0.3</span>, <span class="number">0.4</span>]])</span><br><span class="line">w_cell_input = np.asarray([<span class="number">0.5</span>, <span class="number">0.6</span>])</span><br><span class="line">b_cell = np.asarray([<span class="number">0.1</span>, <span class="number">-0.1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义用于输出的全连接层的参数</span></span><br><span class="line">w_output = np.asarray([[<span class="number">1.0</span>], [<span class="number">2.0</span>]])</span><br><span class="line">b_output = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照时间顺序执行循环神经网络的前向传播过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算循环体中的全连接层神经网络</span></span><br><span class="line">    before_activation = np.dot(state, w_cell_state) + X[i] * w_cell_input + b_cell</span><br><span class="line">    state = np.tanh(before_activation)</span><br><span class="line">    <span class="comment"># 根据当前时刻状态计算最终输出</span></span><br><span class="line">    final_output = np.dot(state, w_output) + b_output</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出每个时刻的信息</span></span><br><span class="line">    print(<span class="string">"before activation: "</span>, before_activation)</span><br><span class="line">    print(<span class="string">"state: "</span>, state)</span><br><span class="line">    print(<span class="string">"output: "</span>, final_output)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">运行上面程序可以得到输出</span></span><br><span class="line"><span class="string">before activation:  [0.6 0.5]</span></span><br><span class="line"><span class="string">state:  [0.53704957 0.46211716]</span></span><br><span class="line"><span class="string">output:  [1.56128388]</span></span><br><span class="line"><span class="string">before activation:  [1.2923401  1.39225678]</span></span><br><span class="line"><span class="string">state:  [0.85973818 0.88366641]</span></span><br><span class="line"><span class="string">output:  [2.72707101]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h3 id="8-2-长短时记忆网络-LSTM-结构"><a href="#8-2-长短时记忆网络-LSTM-结构" class="headerlink" title="8.2 长短时记忆网络(LSTM)结构"></a>8.2 长短时记忆网络(LSTM)结构</h3><ul>
<li><p>背景</p>
<ul>
<li>循环神经网络通过保存历史信息来帮助当前的决策</li>
<li>但是存在长期依赖(long-term dependecies)问题<ul>
<li>有时模型可能需要某些长期的信息来执行当前的任务</li>
<li>比如：当模型试图去预测段落“某地开设了大量工厂，空气污染十分严重……这里的天空都是灰色的”的最后一个单词时，仅仅根据短期依赖就无法解决这个问题。因为只根据最后一小段，最后一个词可以是“蓝色的”或者“灰色的”。但如果模型需要预测清楚具体是什么颜色，就需要考虑先前提到但离当前位置较远的上下文信息</li>
</ul>
</li>
</ul>
</li>
<li><p>LSTM简介</p>
<ul>
<li>LSTM的设计就是为了解决长期依赖问题</li>
<li>LSTM是一种拥有三个“门”结构的特殊网络结构</li>
<li>结构图如下<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/LSTM单元结构图.jpg" alt=""></li>
</ul>
</li>
<li><p>门结构</p>
<ul>
<li>LSTM靠一些“门”的结构让信息有选择性地影响循环神经网络中每个时刻的状态</li>
<li>所谓“门”的结构就是一个使用sigmoid神经网络和一个按位做乘法的操作，这两个操作合在一起就是一个“门”的结构</li>
<li>为什么被称为“门”结构<ul>
<li>因为sigmoid作为激活函数的全连接神经网络层会输出一个0到1之间的数值，描述当前输入有多少信号量可以通过这个结构</li>
<li>这个结构的功能就类似于“门”，当门打开时(sigmoid神经网络层输出为1时)，全部信息都可以通过</li>
<li>当门关上时(sigmoid神经网络层输出为0时)，任何信息都无法通过</li>
</ul>
</li>
</ul>
</li>
<li><p>遗忘门</p>
<ul>
<li>作用：让循环神经网络“忘记”之前没有用的信息</li>
<li>工作原理<ul>
<li>遗忘门会根据当前的输入xt和上一时刻输出ht-1决定哪一部分记忆需要被遗忘</li>
<li>假设状态向量c的长度为n</li>
<li>遗忘门会根据当前的输入xt和上一时刻输出ht-1计算一个维度为n的向量<code>f=sigmoid(W1x+W2h)</code>，它在每个维度的值都在<code>(0,1)</code>范围内</li>
<li>再将上一时刻的状态ct-1与f向量按位相乘，那么f取值接近0的维度上的信息就会被遗忘，而f取值接近1的维度上的信息会被保留</li>
</ul>
</li>
</ul>
</li>
<li><p>输入门</p>
<ul>
<li>作用：循环神经网络忘记了部分之前的状态之后，还需要从当前的输入补充最新的记忆</li>
<li>工作原理<ul>
<li>输入门会根据当前的输入<code>xt</code>和上一时刻输出<code>ht-1</code>决定哪些信息加入到状态<code>ct-1</code>中生成新的状态<code>ct</code></li>
</ul>
</li>
</ul>
</li>
<li><p>输出门</p>
<ul>
<li>作用：LSTM在计算得到新的状态<code>ct</code>后需要产生当前时刻的输出，这个输出由输出门完成</li>
<li>工作原理<ul>
<li>输出门根据最新的状态<code>ct</code>、上一时刻的输出<code>ht-1</code>和当前的输入<code>xt</code>来决定该时刻的输出<code>ht</code></li>
</ul>
</li>
</ul>
</li>
<li><p>使用LSTM结构的RNN的前向传播过程</p>
<ul>
<li>LSTM单元细节图如下<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/LSTM单元细节图.jpg" alt=""></li>
<li>具体LSTM每个门的公式定义如下<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/LSTM公式1.jpg" alt=""><br><img src="/2019/03/10/Tensorflow-8-循环神经网络/LSTM公式2.jpg" alt=""></li>
</ul>
</li>
<li><p>在TensorFlow中实现LSTM结构的RNN前向传播</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个LSTM结构</span></span><br><span class="line"><span class="comment"># LSTM中使用的变量也会在该函数中自动被声明</span></span><br><span class="line">lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将LSTM中的状态初始化为全0数组</span></span><br><span class="line"><span class="comment"># state中包含两个张量，其中state.c和state.h分别对应了LSTM单元细节图中的c状态和h状态</span></span><br><span class="line"><span class="comment"># batch_size给出了一个batch的大小</span></span><br><span class="line">state = lstm.zero_state(batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练中为了将循环神经网络展开成前馈神经网络，需要直到训练数据的序列长度</span></span><br><span class="line"><span class="comment"># 其中，num_steps代表了这个长度</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在第一个时刻声明LSTM结构中使用的变量，在之后的时刻都需要复用之前定义好的变量</span></span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">        tf.get_variable_scope().reuse_variables()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每一步处理时间序列中的一个时刻</span></span><br><span class="line">    <span class="comment"># 将当前输入current_input和前一时刻状态state(ht-1和ct-1)传入LSTM结构，</span></span><br><span class="line">    <span class="comment"># 可以得到当前LSTM的输出lstm_output(ht)和更新后状态state(ht和ct)</span></span><br><span class="line">    <span class="comment"># lstm_output可以用于输出给其他层，state用于输出给下一时刻</span></span><br><span class="line">    lstm_output, state = lstm(current_input, state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将当前时刻LSTM结构的输出传入一个全连接层得到最后的输出</span></span><br><span class="line">    final_output = fully_connected(lstm_output)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算当前输出的损失</span></span><br><span class="line">    loss += calc_loss(final_output, expected_output)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 然后使用类似第四章介绍的方法训练模型</span></span><br><span class="line">train()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="8-3-循环神经网络的变种"><a href="#8-3-循环神经网络的变种" class="headerlink" title="8.3 循环神经网络的变种"></a>8.3 循环神经网络的变种</h3><hr>
<h4 id="8-3-1-双向循环神经网络和深层循环神经网络"><a href="#8-3-1-双向循环神经网络和深层循环神经网络" class="headerlink" title="8.3.1 双向循环神经网络和深层循环神经网络"></a>8.3.1 双向循环神经网络和深层循环神经网络</h4><ul>
<li><p>双向循环神经网络(bidirectional RNN)</p>
<ul>
<li>简介<ul>
<li>在有些问题中，当前时刻的输出不仅和之前的状态有关，也和之后的状态有关，这时就需要双向循环神经网络</li>
<li>例如：预测一个语句中缺失的单词不仅需要根据前文判断，还需要根据后文判断，这时双向循环网络就可以发挥它的作用</li>
<li>双向循环神经网络由两个独立的循环神经网络叠加在一起组成</li>
<li>输出是由这两个循环神经网络的输出拼接而成</li>
</ul>
</li>
<li>结构图<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/双向循环神经网络结构.jpg" alt=""></li>
<li>工作原理<ul>
<li>双向循环神经网络的主体结构就是两个单向循环神经网络的结合</li>
<li>在每个时刻t，输入会同时提供给两个方向相反的循环神经网络</li>
<li>两个网络独立进行计算，各自产生该时刻的新状态和输出，而双向循环神经网络的最终输出是这两个单向网络的输出的简单拼接</li>
<li>两个循环神经网络除方向不同之外，其余结构完全对称</li>
<li>每一层网络中的循环体可以自由选用任意结构，如前面介绍过的简单RNN、LSTM均可作为循环体</li>
</ul>
</li>
</ul>
</li>
<li><p>深层循环神经网络(deep RNN)</p>
<ul>
<li>简介<ul>
<li>为了增强模型的表达能力，可以在网络中设置多个循环层，将每层循环网络的输出传给下一层进行处理</li>
</ul>
</li>
<li>结构图<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/深层循环神经网络结构.jpg" alt=""></li>
<li>工作原理<ul>
<li>如图，在一个L层的深层循环网络中，每一时刻的输入xt到输出ot之间有L个循环体</li>
<li>网络因此可以抽象更加高层的信息</li>
<li>网络中每一层的循环体中参数是一致的，而不同层中的参数可以不同</li>
</ul>
</li>
<li>代码实现  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个基本的LSTM结构作为循环体的基础结构</span></span><br><span class="line"><span class="comment"># 深层循环神经网络也支持使用其他的循环体结构</span></span><br><span class="line">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过MultiRNNCell类实现深层循环神经网络的结构</span></span><br><span class="line"><span class="comment"># 其中number_of_layers表示有多少层</span></span><br><span class="line">stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(lstm_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(number_of_layers)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 和经典的循环神经网络一样，可以通过zero_state函数来获得初始状态</span></span><br><span class="line">state = stacked_lstm.zero_state(batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一时刻的前向传播过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(num_steps)):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">0</span>:tf.get_variable_scope().reuse_variables()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每一步处理时间序列中的一个时刻</span></span><br><span class="line">    <span class="comment"># 将当前输入current_input和前一时刻状态state(ht-1和ct-1)传入LSTM结构，</span></span><br><span class="line">    <span class="comment"># 可以得到当前LSTM的输出lstm_output(ht)和更新后状态state(ht和ct)</span></span><br><span class="line">    <span class="comment"># lstm_output可以用于输出给其他层，state用于输出给下一时刻</span></span><br><span class="line">    stacked_lstm_output, state = stacked_lstm(current_input, state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 通过一个全连接层得到当前时刻的最终输出</span></span><br><span class="line">    final_output = fully_connected(stacked_lstm_output)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 累加损失函数</span></span><br><span class="line">    loss += calc_loss(final_output, expected_output)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="8-3-2-循环神经网络的dropout"><a href="#8-3-2-循环神经网络的dropout" class="headerlink" title="8.3.2 循环神经网络的dropout"></a>8.3.2 循环神经网络的dropout</h4><ul>
<li><p>通过dropout，可以让神经网络更加健壮(robust)</p>
</li>
<li><p>循环神经网络一般只在不同层循环体结构之间使用dropout，而不在同一层的循环体结构之间使用</p>
<ul>
<li>也就是说，从时刻t-1传递到时刻t时，循环神经微那里过不会进行状态的dropout</li>
<li>而在同一时刻t中，不同层循环体之间会使用dropout</li>
<li>如图<br><img src="/2019/03/10/Tensorflow-8-循环神经网络/深层循环网络的dropout.jpg" alt=""></li>
</ul>
</li>
<li><p>代码实现</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义LSTM结构</span></span><br><span class="line">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用DropoutWrapper类来实现dropout功能</span></span><br><span class="line"><span class="comment"># 这个类可以传入两个参数来控制dropout的概率</span></span><br><span class="line"><span class="comment"># input_keep_prob 用来控制输入的dropout概率</span></span><br><span class="line"><span class="comment"># output_keep_prob 用来控制输出的dropout概率</span></span><br><span class="line">stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.DropoutWrapper(lstm_cell(lstm_size)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(number_of_layers)])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h3 id="8-3-循环神经网络的变种-1"><a href="#8-3-循环神经网络的变种-1" class="headerlink" title="8.3 循环神经网络的变种"></a>8.3 循环神经网络的变种</h3><ul>
<li><p>简介：以时序预测为例，利用循环神经网络实现对函数sinx取值的预测</p>
</li>
<li><p>代码如下</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明常量</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">30</span>                    <span class="comment"># LSTM中隐藏节点的个数</span></span><br><span class="line">NUM_LAYERS = <span class="number">2</span>                      <span class="comment"># LSTM的层数</span></span><br><span class="line"></span><br><span class="line">TIMESTEPS = <span class="number">10</span>                      <span class="comment"># 循环神经网络的训练序列长度</span></span><br><span class="line">TRAINING_STEPS = <span class="number">6000</span>               <span class="comment"># 训练轮数</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span>                     <span class="comment"># batch的大小</span></span><br><span class="line"></span><br><span class="line">TRAINING_EXAMPLES = <span class="number">10000</span>           <span class="comment"># 训练数据个数</span></span><br><span class="line">TESTING_EXAMPLES = <span class="number">1000</span>             <span class="comment"># 测试数据个数</span></span><br><span class="line">SAMPLE_GAP = <span class="number">0.1</span>                    <span class="comment"># 采样间隔</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 产生训练所用数据集X和标记Y</span></span><br><span class="line"><span class="comment"># 输入为一个数组,数组中的每个元素是sin函数的x值和对应的y值</span></span><br><span class="line"><span class="comment"># 输出为生成的数据集x和标记y</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(seq)</span>:</span></span><br><span class="line">    x, y = [], []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将序列的第i项到第i+TIMESTEPS-1项，共TIMESTEPS项作为输入</span></span><br><span class="line">    <span class="comment"># 将序列的第i+TIMESTEPS项作为输出</span></span><br><span class="line">    <span class="comment"># 即，使用sin函数前TIMESTEPS个节点的信息，来预测第i + TIMESTEPS个节点的函数值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seq) - TIMESTEPS):</span><br><span class="line">        x.append([seq[i:i+TIMESTEPS]])</span><br><span class="line">        y.append([seq[i + TIMESTEPS]])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(x, dtype=np.float32), np.array(y, dtype=np.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型、前向传播过程、损失函数和优化过程</span></span><br><span class="line"><span class="comment"># 输入为数据x、标记y和是否是训练过程is_training</span></span><br><span class="line"><span class="comment"># 返回预测结果，损失函数，优化过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_model</span><span class="params">(x, y, is_training)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用多层的LSTM结构</span></span><br><span class="line">    cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将多层的LSTM结构连接成RNN网络并计算其前向传播的结果</span></span><br><span class="line">    outputs, _ = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># outputs是顶层LSTM在每一步的输出结果</span></span><br><span class="line">    <span class="comment"># 它的维度为[batch_size, time, HIDDEN_SIZE]</span></span><br><span class="line">    <span class="comment"># 本问题中只关注最后一个时刻的输出结果</span></span><br><span class="line">    output = outputs[:, <span class="number">-1</span>, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对LSTM网络的输出再加一层全连接层并计算损失</span></span><br><span class="line">    predictions = tf.contrib.layers.fully_connected(output, <span class="number">1</span>, activation_fn=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 只在训练时计算损失函数和优化步骤，测试时直接返回预测结果</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">        <span class="keyword">return</span> predictions, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失函数</span></span><br><span class="line">    loss = tf.losses.mean_squared_error(y, predictions)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型优化器并得到优化步骤</span></span><br><span class="line">    train_op = tf.contrib.layers.optimize_loss(loss, tf.train.get_global_step(), optimizer=<span class="string">"Adagrad"</span>, learning_rate=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回预测结果，损失函数，优化过程</span></span><br><span class="line">    <span class="keyword">return</span> predictions, loss, train_op</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(sess, train_x, train_y)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对数据进行处理，将训练数据以数据集的方式提供给计算图</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 切分传入Tensor的第一个维度，生成相应的dataSet</span></span><br><span class="line">    ds = tf.data.Dataset.from_tensor_slices((train_x, train_y))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ds.repeat : Repeats this dataset `count` times.</span></span><br><span class="line">    <span class="comment"># ds.shuffle : 随机打乱这个数据集的元素</span></span><br><span class="line">    <span class="comment"># ds.batch : 将此数据集的连续元素组合成batch</span></span><br><span class="line">    ds = ds.repeat().shuffle(<span class="number">1000</span>).batch(BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make_one_shot_iterator()用来生成一个迭代器来读取数据</span></span><br><span class="line">    <span class="comment"># one_shot迭代器人如其名，意思就是数据输出一次后就丢弃了</span></span><br><span class="line">    <span class="comment"># 之后每次x, y被会话调用迭代器就会将指针指向下一个元素</span></span><br><span class="line">    x, y = ds.make_one_shot_iterator().get_next()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用模型传入数据并得到预测结果、损失函数和优化步骤</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>):</span><br><span class="line">        predictions, loss, train_op = lstm_model(x, y, <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化变量</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开启训练</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        _, loss_result = sess.run([train_op, loss])</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %d training step(s), the loss result is %f"</span> % (i, loss_result))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_eval</span><span class="params">(sess, test_x, test_y)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将测试数据以数据集的方式提供给计算图</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 切分传入Tensor的第一个维度，生成相应的dataSet</span></span><br><span class="line">    data_set = tf.data.Dataset.from_tensor_slices((test_x, test_y))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 类型转换，将Dataset转换为BatchDataset</span></span><br><span class="line">    data_set = data_set.batch(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make_one_shot_iterator()用来生成一个迭代器来读取数据</span></span><br><span class="line">    <span class="comment"># one_shot迭代器人如其名，意思就是数据输出一次后就丢弃了</span></span><br><span class="line">    x, y = data_set.make_one_shot_iterator().get_next()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用模型得到预测结果</span></span><br><span class="line">        prediction, _, _ = lstm_model(x, [<span class="number">0.0</span>], <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将预测结果存入数组中</span></span><br><span class="line">        predictions, labels = [], []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(TESTING_EXAMPLES):</span><br><span class="line">            prediction_result, labels_result = sess.run([prediction, y])</span><br><span class="line">            predictions.append(prediction_result)</span><br><span class="line">            labels.append(labels_result)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算rmse作为评价标准</span></span><br><span class="line">        predictions = np.array(predictions).squeeze()</span><br><span class="line">        labels = np.array(labels).squeeze()</span><br><span class="line">        rmse = np.sqrt(((predictions - labels) ** <span class="number">2</span>).mean(axis=<span class="number">0</span>))</span><br><span class="line">        print(<span class="string">"Mean Square Error is: %f"</span> % rmse)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对预测的sin函数曲线进行绘图</span></span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(labels, label=<span class="string">"real_sin"</span>)</span><br><span class="line">        plt.plot(predictions, label=<span class="string">'predictions'</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序入口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 产生训练集数据的起始值</span></span><br><span class="line">    train_start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 产生训练集数据的终止值，也就是测试集数据的起始值</span></span><br><span class="line">    train_end = test_start = (TRAINING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP</span><br><span class="line">    <span class="comment"># 产生测试集数据的终止值</span></span><br><span class="line">    test_end = test_start + (TESTING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 产生训练集数据和测试集数据</span></span><br><span class="line">    train_x, train_y = generate_data(np.sin(np.linspace(train_start, train_end, TRAINING_EXAMPLES + TIMESTEPS, dtype=np.float32)))</span><br><span class="line">    test_x, test_y = generate_data(np.sin(np.linspace(test_start, test_end, TESTING_EXAMPLES + TIMESTEPS, dtype=np.float32)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开启会话开始计算</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        train(sess, train_x, train_y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用训练好的模型对测试数据进行预测</span></span><br><span class="line">        run_eval(sess, test_x, test_y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow提供的一个主程序入口，tf.app.run()会调用上面定义的main函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 某次训练的结果</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">After 0 training step(s), the loss result is 0.525303</span></span><br><span class="line"><span class="string">After 100 training step(s), the loss result is 0.002255</span></span><br><span class="line"><span class="string">After 200 training step(s), the loss result is 0.000643</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">After 9800 training step(s), the loss result is 0.000001</span></span><br><span class="line"><span class="string">After 9900 training step(s), the loss result is 0.000001</span></span><br><span class="line"><span class="string">Mean Square Error is: 0.001200</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://i.loli.net/2020/03/21/WAKimNUecFR64uo.jpg" alt="T0UGH(GuiPing Wang)">
            
              <p class="site-author-name" itemprop="name">T0UGH(GuiPing Wang)</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">112</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">56</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/T0UGH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="tough.neu.edu@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Friends
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://neusoftware.top/solo" title="ChengYi" target="_blank">ChengYi</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://mike4ellis.github.io/" title="Mike" target="_blank">Mike</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">T0UGH(GuiPing Wang)</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'E1MH6h0YP3yhA0PJsohNBgiT-gzGzoHsz',
        appKey: 'YOiN6zLq3XGfKmlR0b8vyHtN',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
